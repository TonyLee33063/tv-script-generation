{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you l\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]\n",
    "print (text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 20:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n",
      "Moe_Szyslak: Ah, isn't that nice. Now, there is a politician who cares.\n",
      "Barney_Gumble: If I ever vote, it'll be for him. (BELCH)\n",
      "\n",
      "\n",
      "Barney_Gumble: Hey Homer, how's your neighbor's store doing?\n",
      "Homer_Simpson: Lousy. He just sits there all day. He'd have a great job if he didn't own the place. (CHUCKLES)\n",
      "Moe_Szyslak: (STRUGGLING WITH CORKSCREW) Crummy right-handed corkscrews! What does he sell?\n",
      "Homer_Simpson: Uh, well actually, Moe...\n",
      "HOMER_(CONT'D: I dunno.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 20)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {e:i for i,e in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return {\".\":\"||Period||\",\",\":\"||Comma||\",'\"':\"||Quotation_Mark||\",\";\":\"||Semicolon||\",\"!\":\"||Exclamation_Mark||\",\"?\":\"||Question_Mark||\",\"(\":\"||Left_Parentheses||\",\")\":\"||Right_Parentheses||\",\"--\":\"||Dash||\",\"\\n\":\"||Return||\"}\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\software_sets\\Anaconda\\envs\\tflearn\\lib\\site-packages\\ipykernel\\__main__.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following the tuple `(Input, Targets, LearingRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    Input = tf.placeholder(tf.int32, shape=[None,None],name='input')\n",
    "    Targets = tf.placeholder(tf.int32,shape=[None,None], name='targets')\n",
    "    LearingRate = tf.placeholder(tf.float32,name='learningRate')\n",
    "    return (Input, Targets, LearingRate)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "     ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    Cell = tf.contrib.rnn.MultiRNNCell([lstm] * 2)\n",
    "    initial_state = Cell.zero_state(batch_size, tf.float32)\n",
    "    InitialState = tf.identity(initial_state,name=\"initial_state\")\n",
    "    return (Cell, InitialState)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    (Outputs, state)  = tf.nn.dynamic_rnn(cell,inputs,dtype=tf.float32)\n",
    "    FinalState = tf.identity(state,name=\"final_state\")\n",
    "    return (Outputs, FinalState)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = get_embed(input_data, vocab_size, embed_dim=200) \n",
    "    (Outputs, FinalState) = build_rnn(cell, inputs)\n",
    "    Logits= tf.contrib.layers.fully_connected(Outputs,num_outputs=vocab_size,activation_fn=None)\n",
    "    return (Logits, FinalState)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int_text <class 'list'>\n",
      "batches 7.8109375\n",
      "n_batches 7\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # step1: compute the batches number\n",
    "    print (\"int_text\",type(int_text))\n",
    "    int_text = int_text[:-1]\n",
    "    slice_size = batch_size * seq_length\n",
    "    batches = len(int_text)/slice_size\n",
    "    print (\"batches\",batches)\n",
    "    n_batches = int(batches)\n",
    "    print (\"n_batches\",n_batches)\n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = int_text[: n_batches*slice_size]\n",
    "    x = np.asarray(x)\n",
    "    # target is just one character away from the input data\n",
    "    y = int_text[1: n_batches*slice_size+1]\n",
    "    y = np.asarray(y)\n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, n_batches))\n",
    "    y = np.stack(np.split(y, n_batches))\n",
    "    result = []\n",
    "    for i in range(n_batches):\n",
    "        xi=x[i].reshape(batch_size,seq_length)\n",
    "        yi=y[i].reshape(batch_size,seq_length)\n",
    "        xy = np.array([xi,yi])\n",
    "        result.append(xy)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 60\n",
    "# Batch Size\n",
    "batch_size = 10\n",
    "# RNN Size\n",
    "rnn_size = 32\n",
    "# Sequence Length\n",
    "seq_length = 3\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int_text <class 'list'>\n",
      "batches 2303.3\n",
      "n_batches 2303\n",
      "Epoch   0 Batch    0/2303   train_loss = 8.823\n",
      "Epoch   0 Batch   20/2303   train_loss = 7.766\n",
      "Epoch   0 Batch   40/2303   train_loss = 6.032\n",
      "Epoch   0 Batch   60/2303   train_loss = 6.274\n",
      "Epoch   0 Batch   80/2303   train_loss = 7.233\n",
      "Epoch   0 Batch  100/2303   train_loss = 6.430\n",
      "Epoch   0 Batch  120/2303   train_loss = 5.312\n",
      "Epoch   0 Batch  140/2303   train_loss = 6.903\n",
      "Epoch   0 Batch  160/2303   train_loss = 5.518\n",
      "Epoch   0 Batch  180/2303   train_loss = 6.041\n",
      "Epoch   0 Batch  200/2303   train_loss = 6.672\n",
      "Epoch   0 Batch  220/2303   train_loss = 6.776\n",
      "Epoch   0 Batch  240/2303   train_loss = 6.374\n",
      "Epoch   0 Batch  260/2303   train_loss = 6.012\n",
      "Epoch   0 Batch  280/2303   train_loss = 5.438\n",
      "Epoch   0 Batch  300/2303   train_loss = 6.690\n",
      "Epoch   0 Batch  320/2303   train_loss = 6.000\n",
      "Epoch   0 Batch  340/2303   train_loss = 6.976\n",
      "Epoch   0 Batch  360/2303   train_loss = 4.948\n",
      "Epoch   0 Batch  380/2303   train_loss = 5.739\n",
      "Epoch   0 Batch  400/2303   train_loss = 5.368\n",
      "Epoch   0 Batch  420/2303   train_loss = 6.108\n",
      "Epoch   0 Batch  440/2303   train_loss = 4.457\n",
      "Epoch   0 Batch  460/2303   train_loss = 5.329\n",
      "Epoch   0 Batch  480/2303   train_loss = 6.151\n",
      "Epoch   0 Batch  500/2303   train_loss = 4.967\n",
      "Epoch   0 Batch  520/2303   train_loss = 5.919\n",
      "Epoch   0 Batch  540/2303   train_loss = 5.048\n",
      "Epoch   0 Batch  560/2303   train_loss = 5.418\n",
      "Epoch   0 Batch  580/2303   train_loss = 6.172\n",
      "Epoch   0 Batch  600/2303   train_loss = 6.486\n",
      "Epoch   0 Batch  620/2303   train_loss = 5.578\n",
      "Epoch   0 Batch  640/2303   train_loss = 4.877\n",
      "Epoch   0 Batch  660/2303   train_loss = 5.488\n",
      "Epoch   0 Batch  680/2303   train_loss = 5.871\n",
      "Epoch   0 Batch  700/2303   train_loss = 5.458\n",
      "Epoch   0 Batch  720/2303   train_loss = 5.666\n",
      "Epoch   0 Batch  740/2303   train_loss = 5.548\n",
      "Epoch   0 Batch  760/2303   train_loss = 5.751\n",
      "Epoch   0 Batch  780/2303   train_loss = 4.549\n",
      "Epoch   0 Batch  800/2303   train_loss = 5.842\n",
      "Epoch   0 Batch  820/2303   train_loss = 4.986\n",
      "Epoch   0 Batch  840/2303   train_loss = 4.138\n",
      "Epoch   0 Batch  860/2303   train_loss = 3.996\n",
      "Epoch   0 Batch  880/2303   train_loss = 5.247\n",
      "Epoch   0 Batch  900/2303   train_loss = 5.914\n",
      "Epoch   0 Batch  920/2303   train_loss = 5.047\n",
      "Epoch   0 Batch  940/2303   train_loss = 4.846\n",
      "Epoch   0 Batch  960/2303   train_loss = 5.678\n",
      "Epoch   0 Batch  980/2303   train_loss = 4.781\n",
      "Epoch   0 Batch 1000/2303   train_loss = 4.717\n",
      "Epoch   0 Batch 1020/2303   train_loss = 6.412\n",
      "Epoch   0 Batch 1040/2303   train_loss = 5.514\n",
      "Epoch   0 Batch 1060/2303   train_loss = 3.852\n",
      "Epoch   0 Batch 1080/2303   train_loss = 6.166\n",
      "Epoch   0 Batch 1100/2303   train_loss = 6.118\n",
      "Epoch   0 Batch 1120/2303   train_loss = 6.901\n",
      "Epoch   0 Batch 1140/2303   train_loss = 4.667\n",
      "Epoch   0 Batch 1160/2303   train_loss = 6.053\n",
      "Epoch   0 Batch 1180/2303   train_loss = 4.440\n",
      "Epoch   0 Batch 1200/2303   train_loss = 5.551\n",
      "Epoch   0 Batch 1220/2303   train_loss = 5.273\n",
      "Epoch   0 Batch 1240/2303   train_loss = 6.174\n",
      "Epoch   0 Batch 1260/2303   train_loss = 7.260\n",
      "Epoch   0 Batch 1280/2303   train_loss = 5.177\n",
      "Epoch   0 Batch 1300/2303   train_loss = 5.803\n",
      "Epoch   0 Batch 1320/2303   train_loss = 5.743\n",
      "Epoch   0 Batch 1340/2303   train_loss = 5.019\n",
      "Epoch   0 Batch 1360/2303   train_loss = 6.195\n",
      "Epoch   0 Batch 1380/2303   train_loss = 4.521\n",
      "Epoch   0 Batch 1400/2303   train_loss = 6.267\n",
      "Epoch   0 Batch 1420/2303   train_loss = 6.484\n",
      "Epoch   0 Batch 1440/2303   train_loss = 5.662\n",
      "Epoch   0 Batch 1460/2303   train_loss = 5.276\n",
      "Epoch   0 Batch 1480/2303   train_loss = 5.722\n",
      "Epoch   0 Batch 1500/2303   train_loss = 5.450\n",
      "Epoch   0 Batch 1520/2303   train_loss = 5.159\n",
      "Epoch   0 Batch 1540/2303   train_loss = 4.697\n",
      "Epoch   0 Batch 1560/2303   train_loss = 4.386\n",
      "Epoch   0 Batch 1580/2303   train_loss = 3.886\n",
      "Epoch   0 Batch 1600/2303   train_loss = 6.288\n",
      "Epoch   0 Batch 1620/2303   train_loss = 7.247\n",
      "Epoch   0 Batch 1640/2303   train_loss = 5.829\n",
      "Epoch   0 Batch 1660/2303   train_loss = 6.415\n",
      "Epoch   0 Batch 1680/2303   train_loss = 4.670\n",
      "Epoch   0 Batch 1700/2303   train_loss = 5.125\n",
      "Epoch   0 Batch 1720/2303   train_loss = 5.534\n",
      "Epoch   0 Batch 1740/2303   train_loss = 5.990\n",
      "Epoch   0 Batch 1760/2303   train_loss = 5.377\n",
      "Epoch   0 Batch 1780/2303   train_loss = 5.189\n",
      "Epoch   0 Batch 1800/2303   train_loss = 5.875\n",
      "Epoch   0 Batch 1820/2303   train_loss = 5.169\n",
      "Epoch   0 Batch 1840/2303   train_loss = 6.072\n",
      "Epoch   0 Batch 1860/2303   train_loss = 5.764\n",
      "Epoch   0 Batch 1880/2303   train_loss = 6.504\n",
      "Epoch   0 Batch 1900/2303   train_loss = 6.209\n",
      "Epoch   0 Batch 1920/2303   train_loss = 4.646\n",
      "Epoch   0 Batch 1940/2303   train_loss = 5.450\n",
      "Epoch   0 Batch 1960/2303   train_loss = 6.579\n",
      "Epoch   0 Batch 1980/2303   train_loss = 4.693\n",
      "Epoch   0 Batch 2000/2303   train_loss = 4.796\n",
      "Epoch   0 Batch 2020/2303   train_loss = 4.494\n",
      "Epoch   0 Batch 2040/2303   train_loss = 5.478\n",
      "Epoch   0 Batch 2060/2303   train_loss = 4.963\n",
      "Epoch   0 Batch 2080/2303   train_loss = 4.237\n",
      "Epoch   0 Batch 2100/2303   train_loss = 5.423\n",
      "Epoch   0 Batch 2120/2303   train_loss = 5.287\n",
      "Epoch   0 Batch 2140/2303   train_loss = 4.488\n",
      "Epoch   0 Batch 2160/2303   train_loss = 5.016\n",
      "Epoch   0 Batch 2180/2303   train_loss = 5.390\n",
      "Epoch   0 Batch 2200/2303   train_loss = 4.488\n",
      "Epoch   0 Batch 2220/2303   train_loss = 4.838\n",
      "Epoch   0 Batch 2240/2303   train_loss = 4.222\n",
      "Epoch   0 Batch 2260/2303   train_loss = 5.633\n",
      "Epoch   0 Batch 2280/2303   train_loss = 5.273\n",
      "Epoch   0 Batch 2300/2303   train_loss = 5.814\n",
      "Epoch   1 Batch   17/2303   train_loss = 4.117\n",
      "Epoch   1 Batch   37/2303   train_loss = 5.719\n",
      "Epoch   1 Batch   57/2303   train_loss = 5.272\n",
      "Epoch   1 Batch   77/2303   train_loss = 5.120\n",
      "Epoch   1 Batch   97/2303   train_loss = 5.226\n",
      "Epoch   1 Batch  117/2303   train_loss = 4.814\n",
      "Epoch   1 Batch  137/2303   train_loss = 5.036\n",
      "Epoch   1 Batch  157/2303   train_loss = 4.291\n",
      "Epoch   1 Batch  177/2303   train_loss = 5.482\n",
      "Epoch   1 Batch  197/2303   train_loss = 3.517\n",
      "Epoch   1 Batch  217/2303   train_loss = 4.979\n",
      "Epoch   1 Batch  237/2303   train_loss = 4.111\n",
      "Epoch   1 Batch  257/2303   train_loss = 4.388\n",
      "Epoch   1 Batch  277/2303   train_loss = 3.896\n",
      "Epoch   1 Batch  297/2303   train_loss = 4.988\n",
      "Epoch   1 Batch  317/2303   train_loss = 3.792\n",
      "Epoch   1 Batch  337/2303   train_loss = 4.967\n",
      "Epoch   1 Batch  357/2303   train_loss = 4.427\n",
      "Epoch   1 Batch  377/2303   train_loss = 5.472\n",
      "Epoch   1 Batch  397/2303   train_loss = 5.338\n",
      "Epoch   1 Batch  417/2303   train_loss = 4.786\n",
      "Epoch   1 Batch  437/2303   train_loss = 5.161\n",
      "Epoch   1 Batch  457/2303   train_loss = 5.486\n",
      "Epoch   1 Batch  477/2303   train_loss = 6.362\n",
      "Epoch   1 Batch  497/2303   train_loss = 5.123\n",
      "Epoch   1 Batch  517/2303   train_loss = 4.535\n",
      "Epoch   1 Batch  537/2303   train_loss = 4.888\n",
      "Epoch   1 Batch  557/2303   train_loss = 5.049\n",
      "Epoch   1 Batch  577/2303   train_loss = 4.105\n",
      "Epoch   1 Batch  597/2303   train_loss = 5.058\n",
      "Epoch   1 Batch  617/2303   train_loss = 4.184\n",
      "Epoch   1 Batch  637/2303   train_loss = 5.457\n",
      "Epoch   1 Batch  657/2303   train_loss = 5.563\n",
      "Epoch   1 Batch  677/2303   train_loss = 4.297\n",
      "Epoch   1 Batch  697/2303   train_loss = 4.655\n",
      "Epoch   1 Batch  717/2303   train_loss = 5.367\n",
      "Epoch   1 Batch  737/2303   train_loss = 3.738\n",
      "Epoch   1 Batch  757/2303   train_loss = 5.181\n",
      "Epoch   1 Batch  777/2303   train_loss = 5.286\n",
      "Epoch   1 Batch  797/2303   train_loss = 4.593\n",
      "Epoch   1 Batch  817/2303   train_loss = 5.125\n",
      "Epoch   1 Batch  837/2303   train_loss = 4.967\n",
      "Epoch   1 Batch  857/2303   train_loss = 6.112\n",
      "Epoch   1 Batch  877/2303   train_loss = 5.127\n",
      "Epoch   1 Batch  897/2303   train_loss = 5.333\n",
      "Epoch   1 Batch  917/2303   train_loss = 4.168\n",
      "Epoch   1 Batch  937/2303   train_loss = 4.445\n",
      "Epoch   1 Batch  957/2303   train_loss = 4.419\n",
      "Epoch   1 Batch  977/2303   train_loss = 4.560\n",
      "Epoch   1 Batch  997/2303   train_loss = 4.984\n",
      "Epoch   1 Batch 1017/2303   train_loss = 4.371\n",
      "Epoch   1 Batch 1037/2303   train_loss = 4.016\n",
      "Epoch   1 Batch 1057/2303   train_loss = 4.658\n",
      "Epoch   1 Batch 1077/2303   train_loss = 5.191\n",
      "Epoch   1 Batch 1097/2303   train_loss = 4.437\n",
      "Epoch   1 Batch 1117/2303   train_loss = 6.039\n",
      "Epoch   1 Batch 1137/2303   train_loss = 5.897\n",
      "Epoch   1 Batch 1157/2303   train_loss = 4.750\n",
      "Epoch   1 Batch 1177/2303   train_loss = 5.545\n",
      "Epoch   1 Batch 1197/2303   train_loss = 5.748\n",
      "Epoch   1 Batch 1217/2303   train_loss = 5.184\n",
      "Epoch   1 Batch 1237/2303   train_loss = 5.707\n",
      "Epoch   1 Batch 1257/2303   train_loss = 5.034\n",
      "Epoch   1 Batch 1277/2303   train_loss = 5.452\n",
      "Epoch   1 Batch 1297/2303   train_loss = 4.227\n",
      "Epoch   1 Batch 1317/2303   train_loss = 5.073\n",
      "Epoch   1 Batch 1337/2303   train_loss = 5.274\n",
      "Epoch   1 Batch 1357/2303   train_loss = 5.036\n",
      "Epoch   1 Batch 1377/2303   train_loss = 5.564\n",
      "Epoch   1 Batch 1397/2303   train_loss = 4.962\n",
      "Epoch   1 Batch 1417/2303   train_loss = 4.378\n",
      "Epoch   1 Batch 1437/2303   train_loss = 5.554\n",
      "Epoch   1 Batch 1457/2303   train_loss = 4.096\n",
      "Epoch   1 Batch 1477/2303   train_loss = 5.244\n",
      "Epoch   1 Batch 1497/2303   train_loss = 4.284\n",
      "Epoch   1 Batch 1517/2303   train_loss = 4.288\n",
      "Epoch   1 Batch 1537/2303   train_loss = 5.139\n",
      "Epoch   1 Batch 1557/2303   train_loss = 5.218\n",
      "Epoch   1 Batch 1577/2303   train_loss = 5.733\n",
      "Epoch   1 Batch 1597/2303   train_loss = 5.556\n",
      "Epoch   1 Batch 1617/2303   train_loss = 4.591\n",
      "Epoch   1 Batch 1637/2303   train_loss = 4.881\n",
      "Epoch   1 Batch 1657/2303   train_loss = 4.418\n",
      "Epoch   1 Batch 1677/2303   train_loss = 4.766\n",
      "Epoch   1 Batch 1697/2303   train_loss = 5.352\n",
      "Epoch   1 Batch 1717/2303   train_loss = 4.399\n",
      "Epoch   1 Batch 1737/2303   train_loss = 4.745\n",
      "Epoch   1 Batch 1757/2303   train_loss = 5.560\n",
      "Epoch   1 Batch 1777/2303   train_loss = 4.459\n",
      "Epoch   1 Batch 1797/2303   train_loss = 4.845\n",
      "Epoch   1 Batch 1817/2303   train_loss = 5.559\n",
      "Epoch   1 Batch 1837/2303   train_loss = 5.478\n",
      "Epoch   1 Batch 1857/2303   train_loss = 4.383\n",
      "Epoch   1 Batch 1877/2303   train_loss = 4.900\n",
      "Epoch   1 Batch 1897/2303   train_loss = 5.630\n",
      "Epoch   1 Batch 1917/2303   train_loss = 5.632\n",
      "Epoch   1 Batch 1937/2303   train_loss = 5.557\n",
      "Epoch   1 Batch 1957/2303   train_loss = 5.267\n",
      "Epoch   1 Batch 1977/2303   train_loss = 4.928\n",
      "Epoch   1 Batch 1997/2303   train_loss = 5.694\n",
      "Epoch   1 Batch 2017/2303   train_loss = 5.089\n",
      "Epoch   1 Batch 2037/2303   train_loss = 6.387\n",
      "Epoch   1 Batch 2057/2303   train_loss = 5.109\n",
      "Epoch   1 Batch 2077/2303   train_loss = 5.626\n",
      "Epoch   1 Batch 2097/2303   train_loss = 7.456\n",
      "Epoch   1 Batch 2117/2303   train_loss = 5.706\n",
      "Epoch   1 Batch 2137/2303   train_loss = 4.814\n",
      "Epoch   1 Batch 2157/2303   train_loss = 5.035\n",
      "Epoch   1 Batch 2177/2303   train_loss = 5.386\n",
      "Epoch   1 Batch 2197/2303   train_loss = 5.262\n",
      "Epoch   1 Batch 2217/2303   train_loss = 4.554\n",
      "Epoch   1 Batch 2237/2303   train_loss = 5.418\n",
      "Epoch   1 Batch 2257/2303   train_loss = 4.593\n",
      "Epoch   1 Batch 2277/2303   train_loss = 4.517\n",
      "Epoch   1 Batch 2297/2303   train_loss = 4.980\n",
      "Epoch   2 Batch   14/2303   train_loss = 5.533\n",
      "Epoch   2 Batch   34/2303   train_loss = 5.526\n",
      "Epoch   2 Batch   54/2303   train_loss = 4.578\n",
      "Epoch   2 Batch   74/2303   train_loss = 4.937\n",
      "Epoch   2 Batch   94/2303   train_loss = 4.723\n",
      "Epoch   2 Batch  114/2303   train_loss = 4.573\n",
      "Epoch   2 Batch  134/2303   train_loss = 4.728\n",
      "Epoch   2 Batch  154/2303   train_loss = 5.478\n",
      "Epoch   2 Batch  174/2303   train_loss = 4.999\n",
      "Epoch   2 Batch  194/2303   train_loss = 5.155\n",
      "Epoch   2 Batch  214/2303   train_loss = 5.337\n",
      "Epoch   2 Batch  234/2303   train_loss = 4.190\n",
      "Epoch   2 Batch  254/2303   train_loss = 5.236\n",
      "Epoch   2 Batch  274/2303   train_loss = 4.043\n",
      "Epoch   2 Batch  294/2303   train_loss = 4.442\n",
      "Epoch   2 Batch  314/2303   train_loss = 5.431\n",
      "Epoch   2 Batch  334/2303   train_loss = 3.899\n",
      "Epoch   2 Batch  354/2303   train_loss = 3.331\n",
      "Epoch   2 Batch  374/2303   train_loss = 4.076\n",
      "Epoch   2 Batch  394/2303   train_loss = 5.315\n",
      "Epoch   2 Batch  414/2303   train_loss = 4.913\n",
      "Epoch   2 Batch  434/2303   train_loss = 4.395\n",
      "Epoch   2 Batch  454/2303   train_loss = 4.810\n",
      "Epoch   2 Batch  474/2303   train_loss = 4.870\n",
      "Epoch   2 Batch  494/2303   train_loss = 5.152\n",
      "Epoch   2 Batch  514/2303   train_loss = 4.445\n",
      "Epoch   2 Batch  534/2303   train_loss = 5.115\n",
      "Epoch   2 Batch  554/2303   train_loss = 5.724\n",
      "Epoch   2 Batch  574/2303   train_loss = 4.936\n",
      "Epoch   2 Batch  594/2303   train_loss = 4.677\n",
      "Epoch   2 Batch  614/2303   train_loss = 3.698\n",
      "Epoch   2 Batch  634/2303   train_loss = 4.408\n",
      "Epoch   2 Batch  654/2303   train_loss = 5.199\n",
      "Epoch   2 Batch  674/2303   train_loss = 6.094\n",
      "Epoch   2 Batch  694/2303   train_loss = 4.842\n",
      "Epoch   2 Batch  714/2303   train_loss = 4.056\n",
      "Epoch   2 Batch  734/2303   train_loss = 5.253\n",
      "Epoch   2 Batch  754/2303   train_loss = 4.681\n",
      "Epoch   2 Batch  774/2303   train_loss = 4.932\n",
      "Epoch   2 Batch  794/2303   train_loss = 4.934\n",
      "Epoch   2 Batch  814/2303   train_loss = 3.569\n",
      "Epoch   2 Batch  834/2303   train_loss = 4.094\n",
      "Epoch   2 Batch  854/2303   train_loss = 4.531\n",
      "Epoch   2 Batch  874/2303   train_loss = 4.710\n",
      "Epoch   2 Batch  894/2303   train_loss = 4.959\n",
      "Epoch   2 Batch  914/2303   train_loss = 5.164\n",
      "Epoch   2 Batch  934/2303   train_loss = 4.377\n",
      "Epoch   2 Batch  954/2303   train_loss = 3.825\n",
      "Epoch   2 Batch  974/2303   train_loss = 4.472\n",
      "Epoch   2 Batch  994/2303   train_loss = 3.871\n",
      "Epoch   2 Batch 1014/2303   train_loss = 5.012\n",
      "Epoch   2 Batch 1034/2303   train_loss = 5.027\n",
      "Epoch   2 Batch 1054/2303   train_loss = 4.812\n",
      "Epoch   2 Batch 1074/2303   train_loss = 4.599\n",
      "Epoch   2 Batch 1094/2303   train_loss = 5.182\n",
      "Epoch   2 Batch 1114/2303   train_loss = 4.579\n",
      "Epoch   2 Batch 1134/2303   train_loss = 4.344\n",
      "Epoch   2 Batch 1154/2303   train_loss = 4.687\n",
      "Epoch   2 Batch 1174/2303   train_loss = 5.118\n",
      "Epoch   2 Batch 1194/2303   train_loss = 5.331\n",
      "Epoch   2 Batch 1214/2303   train_loss = 4.280\n",
      "Epoch   2 Batch 1234/2303   train_loss = 4.971\n",
      "Epoch   2 Batch 1254/2303   train_loss = 4.823\n",
      "Epoch   2 Batch 1274/2303   train_loss = 4.401\n",
      "Epoch   2 Batch 1294/2303   train_loss = 5.925\n",
      "Epoch   2 Batch 1314/2303   train_loss = 4.505\n",
      "Epoch   2 Batch 1334/2303   train_loss = 4.431\n",
      "Epoch   2 Batch 1354/2303   train_loss = 4.471\n",
      "Epoch   2 Batch 1374/2303   train_loss = 5.015\n",
      "Epoch   2 Batch 1394/2303   train_loss = 4.282\n",
      "Epoch   2 Batch 1414/2303   train_loss = 4.408\n",
      "Epoch   2 Batch 1434/2303   train_loss = 5.479\n",
      "Epoch   2 Batch 1454/2303   train_loss = 4.671\n",
      "Epoch   2 Batch 1474/2303   train_loss = 5.853\n",
      "Epoch   2 Batch 1494/2303   train_loss = 5.820\n",
      "Epoch   2 Batch 1514/2303   train_loss = 5.197\n",
      "Epoch   2 Batch 1534/2303   train_loss = 4.482\n",
      "Epoch   2 Batch 1554/2303   train_loss = 4.701\n",
      "Epoch   2 Batch 1574/2303   train_loss = 4.542\n",
      "Epoch   2 Batch 1594/2303   train_loss = 5.768\n",
      "Epoch   2 Batch 1614/2303   train_loss = 4.962\n",
      "Epoch   2 Batch 1634/2303   train_loss = 4.547\n",
      "Epoch   2 Batch 1654/2303   train_loss = 5.966\n",
      "Epoch   2 Batch 1674/2303   train_loss = 4.431\n",
      "Epoch   2 Batch 1694/2303   train_loss = 5.762\n",
      "Epoch   2 Batch 1714/2303   train_loss = 5.068\n",
      "Epoch   2 Batch 1734/2303   train_loss = 4.936\n",
      "Epoch   2 Batch 1754/2303   train_loss = 4.948\n",
      "Epoch   2 Batch 1774/2303   train_loss = 5.062\n",
      "Epoch   2 Batch 1794/2303   train_loss = 4.621\n",
      "Epoch   2 Batch 1814/2303   train_loss = 4.849\n",
      "Epoch   2 Batch 1834/2303   train_loss = 5.275\n",
      "Epoch   2 Batch 1854/2303   train_loss = 3.898\n",
      "Epoch   2 Batch 1874/2303   train_loss = 5.004\n",
      "Epoch   2 Batch 1894/2303   train_loss = 3.959\n",
      "Epoch   2 Batch 1914/2303   train_loss = 5.006\n",
      "Epoch   2 Batch 1934/2303   train_loss = 5.243\n",
      "Epoch   2 Batch 1954/2303   train_loss = 4.383\n",
      "Epoch   2 Batch 1974/2303   train_loss = 3.444\n",
      "Epoch   2 Batch 1994/2303   train_loss = 5.489\n",
      "Epoch   2 Batch 2014/2303   train_loss = 5.100\n",
      "Epoch   2 Batch 2034/2303   train_loss = 5.371\n",
      "Epoch   2 Batch 2054/2303   train_loss = 4.664\n",
      "Epoch   2 Batch 2074/2303   train_loss = 4.431\n",
      "Epoch   2 Batch 2094/2303   train_loss = 5.145\n",
      "Epoch   2 Batch 2114/2303   train_loss = 3.973\n",
      "Epoch   2 Batch 2134/2303   train_loss = 4.302\n",
      "Epoch   2 Batch 2154/2303   train_loss = 4.782\n",
      "Epoch   2 Batch 2174/2303   train_loss = 4.280\n",
      "Epoch   2 Batch 2194/2303   train_loss = 4.496\n",
      "Epoch   2 Batch 2214/2303   train_loss = 4.353\n",
      "Epoch   2 Batch 2234/2303   train_loss = 5.219\n",
      "Epoch   2 Batch 2254/2303   train_loss = 5.068\n",
      "Epoch   2 Batch 2274/2303   train_loss = 4.099\n",
      "Epoch   2 Batch 2294/2303   train_loss = 4.304\n",
      "Epoch   3 Batch   11/2303   train_loss = 4.772\n",
      "Epoch   3 Batch   31/2303   train_loss = 4.774\n",
      "Epoch   3 Batch   51/2303   train_loss = 4.996\n",
      "Epoch   3 Batch   71/2303   train_loss = 3.917\n",
      "Epoch   3 Batch   91/2303   train_loss = 6.433\n",
      "Epoch   3 Batch  111/2303   train_loss = 4.420\n",
      "Epoch   3 Batch  131/2303   train_loss = 3.684\n",
      "Epoch   3 Batch  151/2303   train_loss = 4.570\n",
      "Epoch   3 Batch  171/2303   train_loss = 4.422\n",
      "Epoch   3 Batch  191/2303   train_loss = 4.440\n",
      "Epoch   3 Batch  211/2303   train_loss = 4.785\n",
      "Epoch   3 Batch  231/2303   train_loss = 3.965\n",
      "Epoch   3 Batch  251/2303   train_loss = 4.141\n",
      "Epoch   3 Batch  271/2303   train_loss = 3.648\n",
      "Epoch   3 Batch  291/2303   train_loss = 5.014\n",
      "Epoch   3 Batch  311/2303   train_loss = 5.406\n",
      "Epoch   3 Batch  331/2303   train_loss = 4.307\n",
      "Epoch   3 Batch  351/2303   train_loss = 5.730\n",
      "Epoch   3 Batch  371/2303   train_loss = 5.085\n",
      "Epoch   3 Batch  391/2303   train_loss = 3.992\n",
      "Epoch   3 Batch  411/2303   train_loss = 4.348\n",
      "Epoch   3 Batch  431/2303   train_loss = 4.512\n",
      "Epoch   3 Batch  451/2303   train_loss = 5.138\n",
      "Epoch   3 Batch  471/2303   train_loss = 3.516\n",
      "Epoch   3 Batch  491/2303   train_loss = 5.705\n",
      "Epoch   3 Batch  511/2303   train_loss = 3.425\n",
      "Epoch   3 Batch  531/2303   train_loss = 5.831\n",
      "Epoch   3 Batch  551/2303   train_loss = 4.852\n",
      "Epoch   3 Batch  571/2303   train_loss = 4.525\n",
      "Epoch   3 Batch  591/2303   train_loss = 4.769\n",
      "Epoch   3 Batch  611/2303   train_loss = 4.023\n",
      "Epoch   3 Batch  631/2303   train_loss = 5.612\n",
      "Epoch   3 Batch  651/2303   train_loss = 5.387\n",
      "Epoch   3 Batch  671/2303   train_loss = 4.283\n",
      "Epoch   3 Batch  691/2303   train_loss = 5.066\n",
      "Epoch   3 Batch  711/2303   train_loss = 4.580\n",
      "Epoch   3 Batch  731/2303   train_loss = 3.951\n",
      "Epoch   3 Batch  751/2303   train_loss = 3.718\n",
      "Epoch   3 Batch  771/2303   train_loss = 4.671\n",
      "Epoch   3 Batch  791/2303   train_loss = 4.172\n",
      "Epoch   3 Batch  811/2303   train_loss = 3.899\n",
      "Epoch   3 Batch  831/2303   train_loss = 4.473\n",
      "Epoch   3 Batch  851/2303   train_loss = 4.244\n",
      "Epoch   3 Batch  871/2303   train_loss = 4.665\n",
      "Epoch   3 Batch  891/2303   train_loss = 4.503\n",
      "Epoch   3 Batch  911/2303   train_loss = 4.880\n",
      "Epoch   3 Batch  931/2303   train_loss = 5.230\n",
      "Epoch   3 Batch  951/2303   train_loss = 4.168\n",
      "Epoch   3 Batch  971/2303   train_loss = 5.030\n",
      "Epoch   3 Batch  991/2303   train_loss = 4.307\n",
      "Epoch   3 Batch 1011/2303   train_loss = 4.962\n",
      "Epoch   3 Batch 1031/2303   train_loss = 4.171\n",
      "Epoch   3 Batch 1051/2303   train_loss = 5.016\n",
      "Epoch   3 Batch 1071/2303   train_loss = 4.726\n",
      "Epoch   3 Batch 1091/2303   train_loss = 4.294\n",
      "Epoch   3 Batch 1111/2303   train_loss = 4.316\n",
      "Epoch   3 Batch 1131/2303   train_loss = 5.028\n",
      "Epoch   3 Batch 1151/2303   train_loss = 4.967\n",
      "Epoch   3 Batch 1171/2303   train_loss = 5.052\n",
      "Epoch   3 Batch 1191/2303   train_loss = 5.351\n",
      "Epoch   3 Batch 1211/2303   train_loss = 4.853\n",
      "Epoch   3 Batch 1231/2303   train_loss = 3.522\n",
      "Epoch   3 Batch 1251/2303   train_loss = 3.618\n",
      "Epoch   3 Batch 1271/2303   train_loss = 4.263\n",
      "Epoch   3 Batch 1291/2303   train_loss = 4.444\n",
      "Epoch   3 Batch 1311/2303   train_loss = 4.247\n",
      "Epoch   3 Batch 1331/2303   train_loss = 4.073\n",
      "Epoch   3 Batch 1351/2303   train_loss = 3.925\n",
      "Epoch   3 Batch 1371/2303   train_loss = 4.035\n",
      "Epoch   3 Batch 1391/2303   train_loss = 3.902\n",
      "Epoch   3 Batch 1411/2303   train_loss = 3.892\n",
      "Epoch   3 Batch 1431/2303   train_loss = 4.768\n",
      "Epoch   3 Batch 1451/2303   train_loss = 4.892\n",
      "Epoch   3 Batch 1471/2303   train_loss = 4.786\n",
      "Epoch   3 Batch 1491/2303   train_loss = 3.798\n",
      "Epoch   3 Batch 1511/2303   train_loss = 4.607\n",
      "Epoch   3 Batch 1531/2303   train_loss = 4.169\n",
      "Epoch   3 Batch 1551/2303   train_loss = 4.915\n",
      "Epoch   3 Batch 1571/2303   train_loss = 3.936\n",
      "Epoch   3 Batch 1591/2303   train_loss = 4.226\n",
      "Epoch   3 Batch 1611/2303   train_loss = 4.375\n",
      "Epoch   3 Batch 1631/2303   train_loss = 5.435\n",
      "Epoch   3 Batch 1651/2303   train_loss = 5.787\n",
      "Epoch   3 Batch 1671/2303   train_loss = 3.228\n",
      "Epoch   3 Batch 1691/2303   train_loss = 4.662\n",
      "Epoch   3 Batch 1711/2303   train_loss = 5.009\n",
      "Epoch   3 Batch 1731/2303   train_loss = 4.967\n",
      "Epoch   3 Batch 1751/2303   train_loss = 4.807\n",
      "Epoch   3 Batch 1771/2303   train_loss = 5.260\n",
      "Epoch   3 Batch 1791/2303   train_loss = 4.700\n",
      "Epoch   3 Batch 1811/2303   train_loss = 4.209\n",
      "Epoch   3 Batch 1831/2303   train_loss = 4.053\n",
      "Epoch   3 Batch 1851/2303   train_loss = 4.355\n",
      "Epoch   3 Batch 1871/2303   train_loss = 3.581\n",
      "Epoch   3 Batch 1891/2303   train_loss = 4.762\n",
      "Epoch   3 Batch 1911/2303   train_loss = 5.023\n",
      "Epoch   3 Batch 1931/2303   train_loss = 3.910\n",
      "Epoch   3 Batch 1951/2303   train_loss = 5.022\n",
      "Epoch   3 Batch 1971/2303   train_loss = 3.674\n",
      "Epoch   3 Batch 1991/2303   train_loss = 4.738\n",
      "Epoch   3 Batch 2011/2303   train_loss = 3.463\n",
      "Epoch   3 Batch 2031/2303   train_loss = 5.157\n",
      "Epoch   3 Batch 2051/2303   train_loss = 5.371\n",
      "Epoch   3 Batch 2071/2303   train_loss = 4.223\n",
      "Epoch   3 Batch 2091/2303   train_loss = 3.937\n",
      "Epoch   3 Batch 2111/2303   train_loss = 5.190\n",
      "Epoch   3 Batch 2131/2303   train_loss = 3.752\n",
      "Epoch   3 Batch 2151/2303   train_loss = 4.790\n",
      "Epoch   3 Batch 2171/2303   train_loss = 3.463\n",
      "Epoch   3 Batch 2191/2303   train_loss = 4.301\n",
      "Epoch   3 Batch 2211/2303   train_loss = 4.371\n",
      "Epoch   3 Batch 2231/2303   train_loss = 3.551\n",
      "Epoch   3 Batch 2251/2303   train_loss = 5.126\n",
      "Epoch   3 Batch 2271/2303   train_loss = 5.354\n",
      "Epoch   3 Batch 2291/2303   train_loss = 3.499\n",
      "Epoch   4 Batch    8/2303   train_loss = 3.543\n",
      "Epoch   4 Batch   28/2303   train_loss = 4.316\n",
      "Epoch   4 Batch   48/2303   train_loss = 3.708\n",
      "Epoch   4 Batch   68/2303   train_loss = 3.716\n",
      "Epoch   4 Batch   88/2303   train_loss = 4.173\n",
      "Epoch   4 Batch  108/2303   train_loss = 4.246\n",
      "Epoch   4 Batch  128/2303   train_loss = 4.038\n",
      "Epoch   4 Batch  148/2303   train_loss = 3.438\n",
      "Epoch   4 Batch  168/2303   train_loss = 4.707\n",
      "Epoch   4 Batch  188/2303   train_loss = 4.325\n",
      "Epoch   4 Batch  208/2303   train_loss = 3.380\n",
      "Epoch   4 Batch  228/2303   train_loss = 5.004\n",
      "Epoch   4 Batch  248/2303   train_loss = 3.936\n",
      "Epoch   4 Batch  268/2303   train_loss = 3.766\n",
      "Epoch   4 Batch  288/2303   train_loss = 4.296\n",
      "Epoch   4 Batch  308/2303   train_loss = 3.882\n",
      "Epoch   4 Batch  328/2303   train_loss = 4.108\n",
      "Epoch   4 Batch  348/2303   train_loss = 4.630\n",
      "Epoch   4 Batch  368/2303   train_loss = 5.695\n",
      "Epoch   4 Batch  388/2303   train_loss = 4.694\n",
      "Epoch   4 Batch  408/2303   train_loss = 4.332\n",
      "Epoch   4 Batch  428/2303   train_loss = 4.009\n",
      "Epoch   4 Batch  448/2303   train_loss = 3.692\n",
      "Epoch   4 Batch  468/2303   train_loss = 3.227\n",
      "Epoch   4 Batch  488/2303   train_loss = 4.967\n",
      "Epoch   4 Batch  508/2303   train_loss = 3.298\n",
      "Epoch   4 Batch  528/2303   train_loss = 3.730\n",
      "Epoch   4 Batch  548/2303   train_loss = 4.338\n",
      "Epoch   4 Batch  568/2303   train_loss = 5.467\n",
      "Epoch   4 Batch  588/2303   train_loss = 4.717\n",
      "Epoch   4 Batch  608/2303   train_loss = 4.235\n",
      "Epoch   4 Batch  628/2303   train_loss = 3.673\n",
      "Epoch   4 Batch  648/2303   train_loss = 3.979\n",
      "Epoch   4 Batch  668/2303   train_loss = 3.777\n",
      "Epoch   4 Batch  688/2303   train_loss = 4.385\n",
      "Epoch   4 Batch  708/2303   train_loss = 4.961\n",
      "Epoch   4 Batch  728/2303   train_loss = 5.224\n",
      "Epoch   4 Batch  748/2303   train_loss = 4.144\n",
      "Epoch   4 Batch  768/2303   train_loss = 3.991\n",
      "Epoch   4 Batch  788/2303   train_loss = 4.463\n",
      "Epoch   4 Batch  808/2303   train_loss = 3.384\n",
      "Epoch   4 Batch  828/2303   train_loss = 4.061\n",
      "Epoch   4 Batch  848/2303   train_loss = 5.270\n",
      "Epoch   4 Batch  868/2303   train_loss = 3.871\n",
      "Epoch   4 Batch  888/2303   train_loss = 4.001\n",
      "Epoch   4 Batch  908/2303   train_loss = 4.832\n",
      "Epoch   4 Batch  928/2303   train_loss = 4.274\n",
      "Epoch   4 Batch  948/2303   train_loss = 5.295\n",
      "Epoch   4 Batch  968/2303   train_loss = 3.466\n",
      "Epoch   4 Batch  988/2303   train_loss = 3.842\n",
      "Epoch   4 Batch 1008/2303   train_loss = 4.468\n",
      "Epoch   4 Batch 1028/2303   train_loss = 4.448\n",
      "Epoch   4 Batch 1048/2303   train_loss = 4.656\n",
      "Epoch   4 Batch 1068/2303   train_loss = 3.954\n",
      "Epoch   4 Batch 1088/2303   train_loss = 4.257\n",
      "Epoch   4 Batch 1108/2303   train_loss = 4.627\n",
      "Epoch   4 Batch 1128/2303   train_loss = 3.984\n",
      "Epoch   4 Batch 1148/2303   train_loss = 4.591\n",
      "Epoch   4 Batch 1168/2303   train_loss = 4.229\n",
      "Epoch   4 Batch 1188/2303   train_loss = 4.165\n",
      "Epoch   4 Batch 1208/2303   train_loss = 4.436\n",
      "Epoch   4 Batch 1228/2303   train_loss = 4.722\n",
      "Epoch   4 Batch 1248/2303   train_loss = 4.617\n",
      "Epoch   4 Batch 1268/2303   train_loss = 4.156\n",
      "Epoch   4 Batch 1288/2303   train_loss = 4.851\n",
      "Epoch   4 Batch 1308/2303   train_loss = 4.877\n",
      "Epoch   4 Batch 1328/2303   train_loss = 4.629\n",
      "Epoch   4 Batch 1348/2303   train_loss = 3.640\n",
      "Epoch   4 Batch 1368/2303   train_loss = 4.548\n",
      "Epoch   4 Batch 1388/2303   train_loss = 4.093\n",
      "Epoch   4 Batch 1408/2303   train_loss = 4.807\n",
      "Epoch   4 Batch 1428/2303   train_loss = 4.479\n",
      "Epoch   4 Batch 1448/2303   train_loss = 4.071\n",
      "Epoch   4 Batch 1468/2303   train_loss = 4.065\n",
      "Epoch   4 Batch 1488/2303   train_loss = 4.569\n",
      "Epoch   4 Batch 1508/2303   train_loss = 4.895\n",
      "Epoch   4 Batch 1528/2303   train_loss = 4.411\n",
      "Epoch   4 Batch 1548/2303   train_loss = 5.078\n",
      "Epoch   4 Batch 1568/2303   train_loss = 5.642\n",
      "Epoch   4 Batch 1588/2303   train_loss = 4.210\n",
      "Epoch   4 Batch 1608/2303   train_loss = 4.427\n",
      "Epoch   4 Batch 1628/2303   train_loss = 4.774\n",
      "Epoch   4 Batch 1648/2303   train_loss = 4.163\n",
      "Epoch   4 Batch 1668/2303   train_loss = 3.750\n",
      "Epoch   4 Batch 1688/2303   train_loss = 5.057\n",
      "Epoch   4 Batch 1708/2303   train_loss = 4.464\n",
      "Epoch   4 Batch 1728/2303   train_loss = 4.473\n",
      "Epoch   4 Batch 1748/2303   train_loss = 4.567\n",
      "Epoch   4 Batch 1768/2303   train_loss = 4.879\n",
      "Epoch   4 Batch 1788/2303   train_loss = 3.987\n",
      "Epoch   4 Batch 1808/2303   train_loss = 4.457\n",
      "Epoch   4 Batch 1828/2303   train_loss = 4.978\n",
      "Epoch   4 Batch 1848/2303   train_loss = 4.276\n",
      "Epoch   4 Batch 1868/2303   train_loss = 3.805\n",
      "Epoch   4 Batch 1888/2303   train_loss = 4.491\n",
      "Epoch   4 Batch 1908/2303   train_loss = 5.394\n",
      "Epoch   4 Batch 1928/2303   train_loss = 4.477\n",
      "Epoch   4 Batch 1948/2303   train_loss = 4.342\n",
      "Epoch   4 Batch 1968/2303   train_loss = 3.632\n",
      "Epoch   4 Batch 1988/2303   train_loss = 4.652\n",
      "Epoch   4 Batch 2008/2303   train_loss = 3.691\n",
      "Epoch   4 Batch 2028/2303   train_loss = 4.369\n",
      "Epoch   4 Batch 2048/2303   train_loss = 4.220\n",
      "Epoch   4 Batch 2068/2303   train_loss = 3.726\n",
      "Epoch   4 Batch 2088/2303   train_loss = 4.947\n",
      "Epoch   4 Batch 2108/2303   train_loss = 3.984\n",
      "Epoch   4 Batch 2128/2303   train_loss = 4.041\n",
      "Epoch   4 Batch 2148/2303   train_loss = 3.687\n",
      "Epoch   4 Batch 2168/2303   train_loss = 3.730\n",
      "Epoch   4 Batch 2188/2303   train_loss = 4.048\n",
      "Epoch   4 Batch 2208/2303   train_loss = 4.348\n",
      "Epoch   4 Batch 2228/2303   train_loss = 4.536\n",
      "Epoch   4 Batch 2248/2303   train_loss = 3.792\n",
      "Epoch   4 Batch 2268/2303   train_loss = 3.622\n",
      "Epoch   4 Batch 2288/2303   train_loss = 4.517\n",
      "Epoch   5 Batch    5/2303   train_loss = 3.972\n",
      "Epoch   5 Batch   25/2303   train_loss = 4.088\n",
      "Epoch   5 Batch   45/2303   train_loss = 3.624\n",
      "Epoch   5 Batch   65/2303   train_loss = 3.554\n",
      "Epoch   5 Batch   85/2303   train_loss = 3.869\n",
      "Epoch   5 Batch  105/2303   train_loss = 4.041\n",
      "Epoch   5 Batch  125/2303   train_loss = 4.162\n",
      "Epoch   5 Batch  145/2303   train_loss = 4.460\n",
      "Epoch   5 Batch  165/2303   train_loss = 5.644\n",
      "Epoch   5 Batch  185/2303   train_loss = 3.899\n",
      "Epoch   5 Batch  205/2303   train_loss = 4.396\n",
      "Epoch   5 Batch  225/2303   train_loss = 3.844\n",
      "Epoch   5 Batch  245/2303   train_loss = 3.862\n",
      "Epoch   5 Batch  265/2303   train_loss = 4.521\n",
      "Epoch   5 Batch  285/2303   train_loss = 4.204\n",
      "Epoch   5 Batch  305/2303   train_loss = 5.315\n",
      "Epoch   5 Batch  325/2303   train_loss = 4.403\n",
      "Epoch   5 Batch  345/2303   train_loss = 4.337\n",
      "Epoch   5 Batch  365/2303   train_loss = 4.223\n",
      "Epoch   5 Batch  385/2303   train_loss = 4.915\n",
      "Epoch   5 Batch  405/2303   train_loss = 3.772\n",
      "Epoch   5 Batch  425/2303   train_loss = 4.065\n",
      "Epoch   5 Batch  445/2303   train_loss = 4.384\n",
      "Epoch   5 Batch  465/2303   train_loss = 4.631\n",
      "Epoch   5 Batch  485/2303   train_loss = 4.601\n",
      "Epoch   5 Batch  505/2303   train_loss = 4.366\n",
      "Epoch   5 Batch  525/2303   train_loss = 3.305\n",
      "Epoch   5 Batch  545/2303   train_loss = 5.292\n",
      "Epoch   5 Batch  565/2303   train_loss = 4.769\n",
      "Epoch   5 Batch  585/2303   train_loss = 4.674\n",
      "Epoch   5 Batch  605/2303   train_loss = 4.720\n",
      "Epoch   5 Batch  625/2303   train_loss = 5.030\n",
      "Epoch   5 Batch  645/2303   train_loss = 5.388\n",
      "Epoch   5 Batch  665/2303   train_loss = 4.098\n",
      "Epoch   5 Batch  685/2303   train_loss = 3.802\n",
      "Epoch   5 Batch  705/2303   train_loss = 3.890\n",
      "Epoch   5 Batch  725/2303   train_loss = 4.113\n",
      "Epoch   5 Batch  745/2303   train_loss = 4.021\n",
      "Epoch   5 Batch  765/2303   train_loss = 4.096\n",
      "Epoch   5 Batch  785/2303   train_loss = 5.018\n",
      "Epoch   5 Batch  805/2303   train_loss = 4.639\n",
      "Epoch   5 Batch  825/2303   train_loss = 3.545\n",
      "Epoch   5 Batch  845/2303   train_loss = 4.891\n",
      "Epoch   5 Batch  865/2303   train_loss = 4.358\n",
      "Epoch   5 Batch  885/2303   train_loss = 4.541\n",
      "Epoch   5 Batch  905/2303   train_loss = 4.051\n",
      "Epoch   5 Batch  925/2303   train_loss = 4.627\n",
      "Epoch   5 Batch  945/2303   train_loss = 3.839\n",
      "Epoch   5 Batch  965/2303   train_loss = 5.266\n",
      "Epoch   5 Batch  985/2303   train_loss = 4.899\n",
      "Epoch   5 Batch 1005/2303   train_loss = 3.753\n",
      "Epoch   5 Batch 1025/2303   train_loss = 4.280\n",
      "Epoch   5 Batch 1045/2303   train_loss = 4.736\n",
      "Epoch   5 Batch 1065/2303   train_loss = 3.448\n",
      "Epoch   5 Batch 1085/2303   train_loss = 3.791\n",
      "Epoch   5 Batch 1105/2303   train_loss = 4.464\n",
      "Epoch   5 Batch 1125/2303   train_loss = 4.320\n",
      "Epoch   5 Batch 1145/2303   train_loss = 4.545\n",
      "Epoch   5 Batch 1165/2303   train_loss = 5.353\n",
      "Epoch   5 Batch 1185/2303   train_loss = 4.177\n",
      "Epoch   5 Batch 1205/2303   train_loss = 3.705\n",
      "Epoch   5 Batch 1225/2303   train_loss = 4.252\n",
      "Epoch   5 Batch 1245/2303   train_loss = 4.007\n",
      "Epoch   5 Batch 1265/2303   train_loss = 4.164\n",
      "Epoch   5 Batch 1285/2303   train_loss = 4.575\n",
      "Epoch   5 Batch 1305/2303   train_loss = 4.073\n",
      "Epoch   5 Batch 1325/2303   train_loss = 4.249\n",
      "Epoch   5 Batch 1345/2303   train_loss = 4.289\n",
      "Epoch   5 Batch 1365/2303   train_loss = 4.504\n",
      "Epoch   5 Batch 1385/2303   train_loss = 5.060\n",
      "Epoch   5 Batch 1405/2303   train_loss = 4.497\n",
      "Epoch   5 Batch 1425/2303   train_loss = 4.731\n",
      "Epoch   5 Batch 1445/2303   train_loss = 4.754\n",
      "Epoch   5 Batch 1465/2303   train_loss = 4.189\n",
      "Epoch   5 Batch 1485/2303   train_loss = 5.117\n",
      "Epoch   5 Batch 1505/2303   train_loss = 3.768\n",
      "Epoch   5 Batch 1525/2303   train_loss = 4.802\n",
      "Epoch   5 Batch 1545/2303   train_loss = 4.943\n",
      "Epoch   5 Batch 1565/2303   train_loss = 4.431\n",
      "Epoch   5 Batch 1585/2303   train_loss = 4.213\n",
      "Epoch   5 Batch 1605/2303   train_loss = 3.834\n",
      "Epoch   5 Batch 1625/2303   train_loss = 4.014\n",
      "Epoch   5 Batch 1645/2303   train_loss = 5.087\n",
      "Epoch   5 Batch 1665/2303   train_loss = 4.470\n",
      "Epoch   5 Batch 1685/2303   train_loss = 4.988\n",
      "Epoch   5 Batch 1705/2303   train_loss = 4.242\n",
      "Epoch   5 Batch 1725/2303   train_loss = 4.363\n",
      "Epoch   5 Batch 1745/2303   train_loss = 3.510\n",
      "Epoch   5 Batch 1765/2303   train_loss = 3.895\n",
      "Epoch   5 Batch 1785/2303   train_loss = 4.526\n",
      "Epoch   5 Batch 1805/2303   train_loss = 4.637\n",
      "Epoch   5 Batch 1825/2303   train_loss = 5.400\n",
      "Epoch   5 Batch 1845/2303   train_loss = 4.382\n",
      "Epoch   5 Batch 1865/2303   train_loss = 4.060\n",
      "Epoch   5 Batch 1885/2303   train_loss = 4.168\n",
      "Epoch   5 Batch 1905/2303   train_loss = 4.041\n",
      "Epoch   5 Batch 1925/2303   train_loss = 4.337\n",
      "Epoch   5 Batch 1945/2303   train_loss = 4.485\n",
      "Epoch   5 Batch 1965/2303   train_loss = 4.365\n",
      "Epoch   5 Batch 1985/2303   train_loss = 5.435\n",
      "Epoch   5 Batch 2005/2303   train_loss = 4.014\n",
      "Epoch   5 Batch 2025/2303   train_loss = 4.570\n",
      "Epoch   5 Batch 2045/2303   train_loss = 3.829\n",
      "Epoch   5 Batch 2065/2303   train_loss = 4.683\n",
      "Epoch   5 Batch 2085/2303   train_loss = 3.892\n",
      "Epoch   5 Batch 2105/2303   train_loss = 4.166\n",
      "Epoch   5 Batch 2125/2303   train_loss = 5.185\n",
      "Epoch   5 Batch 2145/2303   train_loss = 4.407\n",
      "Epoch   5 Batch 2165/2303   train_loss = 5.389\n",
      "Epoch   5 Batch 2185/2303   train_loss = 4.799\n",
      "Epoch   5 Batch 2205/2303   train_loss = 3.545\n",
      "Epoch   5 Batch 2225/2303   train_loss = 4.121\n",
      "Epoch   5 Batch 2245/2303   train_loss = 4.482\n",
      "Epoch   5 Batch 2265/2303   train_loss = 3.472\n",
      "Epoch   5 Batch 2285/2303   train_loss = 3.696\n",
      "Epoch   6 Batch    2/2303   train_loss = 3.493\n",
      "Epoch   6 Batch   22/2303   train_loss = 4.525\n",
      "Epoch   6 Batch   42/2303   train_loss = 4.572\n",
      "Epoch   6 Batch   62/2303   train_loss = 3.876\n",
      "Epoch   6 Batch   82/2303   train_loss = 3.170\n",
      "Epoch   6 Batch  102/2303   train_loss = 3.540\n",
      "Epoch   6 Batch  122/2303   train_loss = 5.324\n",
      "Epoch   6 Batch  142/2303   train_loss = 3.799\n",
      "Epoch   6 Batch  162/2303   train_loss = 3.513\n",
      "Epoch   6 Batch  182/2303   train_loss = 4.207\n",
      "Epoch   6 Batch  202/2303   train_loss = 3.420\n",
      "Epoch   6 Batch  222/2303   train_loss = 4.439\n",
      "Epoch   6 Batch  242/2303   train_loss = 4.152\n",
      "Epoch   6 Batch  262/2303   train_loss = 3.752\n",
      "Epoch   6 Batch  282/2303   train_loss = 3.921\n",
      "Epoch   6 Batch  302/2303   train_loss = 4.375\n",
      "Epoch   6 Batch  322/2303   train_loss = 3.928\n",
      "Epoch   6 Batch  342/2303   train_loss = 4.543\n",
      "Epoch   6 Batch  362/2303   train_loss = 3.900\n",
      "Epoch   6 Batch  382/2303   train_loss = 4.596\n",
      "Epoch   6 Batch  402/2303   train_loss = 4.636\n",
      "Epoch   6 Batch  422/2303   train_loss = 4.899\n",
      "Epoch   6 Batch  442/2303   train_loss = 3.420\n",
      "Epoch   6 Batch  462/2303   train_loss = 3.762\n",
      "Epoch   6 Batch  482/2303   train_loss = 4.100\n",
      "Epoch   6 Batch  502/2303   train_loss = 4.883\n",
      "Epoch   6 Batch  522/2303   train_loss = 4.236\n",
      "Epoch   6 Batch  542/2303   train_loss = 4.673\n",
      "Epoch   6 Batch  562/2303   train_loss = 4.450\n",
      "Epoch   6 Batch  582/2303   train_loss = 3.883\n",
      "Epoch   6 Batch  602/2303   train_loss = 4.615\n",
      "Epoch   6 Batch  622/2303   train_loss = 4.376\n",
      "Epoch   6 Batch  642/2303   train_loss = 3.949\n",
      "Epoch   6 Batch  662/2303   train_loss = 5.214\n",
      "Epoch   6 Batch  682/2303   train_loss = 4.277\n",
      "Epoch   6 Batch  702/2303   train_loss = 4.353\n",
      "Epoch   6 Batch  722/2303   train_loss = 4.934\n",
      "Epoch   6 Batch  742/2303   train_loss = 3.979\n",
      "Epoch   6 Batch  762/2303   train_loss = 4.536\n",
      "Epoch   6 Batch  782/2303   train_loss = 3.717\n",
      "Epoch   6 Batch  802/2303   train_loss = 4.290\n",
      "Epoch   6 Batch  822/2303   train_loss = 3.224\n",
      "Epoch   6 Batch  842/2303   train_loss = 4.096\n",
      "Epoch   6 Batch  862/2303   train_loss = 3.091\n",
      "Epoch   6 Batch  882/2303   train_loss = 3.893\n",
      "Epoch   6 Batch  902/2303   train_loss = 3.905\n",
      "Epoch   6 Batch  922/2303   train_loss = 4.181\n",
      "Epoch   6 Batch  942/2303   train_loss = 5.073\n",
      "Epoch   6 Batch  962/2303   train_loss = 5.392\n",
      "Epoch   6 Batch  982/2303   train_loss = 4.336\n",
      "Epoch   6 Batch 1002/2303   train_loss = 4.409\n",
      "Epoch   6 Batch 1022/2303   train_loss = 3.846\n",
      "Epoch   6 Batch 1042/2303   train_loss = 3.642\n",
      "Epoch   6 Batch 1062/2303   train_loss = 2.962\n",
      "Epoch   6 Batch 1082/2303   train_loss = 5.779\n",
      "Epoch   6 Batch 1102/2303   train_loss = 4.151\n",
      "Epoch   6 Batch 1122/2303   train_loss = 3.688\n",
      "Epoch   6 Batch 1142/2303   train_loss = 3.033\n",
      "Epoch   6 Batch 1162/2303   train_loss = 4.430\n",
      "Epoch   6 Batch 1182/2303   train_loss = 4.279\n",
      "Epoch   6 Batch 1202/2303   train_loss = 4.544\n",
      "Epoch   6 Batch 1222/2303   train_loss = 3.839\n",
      "Epoch   6 Batch 1242/2303   train_loss = 4.487\n",
      "Epoch   6 Batch 1262/2303   train_loss = 3.913\n",
      "Epoch   6 Batch 1282/2303   train_loss = 3.853\n",
      "Epoch   6 Batch 1302/2303   train_loss = 4.285\n",
      "Epoch   6 Batch 1322/2303   train_loss = 4.851\n",
      "Epoch   6 Batch 1342/2303   train_loss = 4.461\n",
      "Epoch   6 Batch 1362/2303   train_loss = 4.042\n",
      "Epoch   6 Batch 1382/2303   train_loss = 3.283\n",
      "Epoch   6 Batch 1402/2303   train_loss = 4.412\n",
      "Epoch   6 Batch 1422/2303   train_loss = 4.492\n",
      "Epoch   6 Batch 1442/2303   train_loss = 3.594\n",
      "Epoch   6 Batch 1462/2303   train_loss = 4.340\n",
      "Epoch   6 Batch 1482/2303   train_loss = 4.181\n",
      "Epoch   6 Batch 1502/2303   train_loss = 3.822\n",
      "Epoch   6 Batch 1522/2303   train_loss = 3.707\n",
      "Epoch   6 Batch 1542/2303   train_loss = 4.535\n",
      "Epoch   6 Batch 1562/2303   train_loss = 4.478\n",
      "Epoch   6 Batch 1582/2303   train_loss = 4.150\n",
      "Epoch   6 Batch 1602/2303   train_loss = 3.992\n",
      "Epoch   6 Batch 1622/2303   train_loss = 4.358\n",
      "Epoch   6 Batch 1642/2303   train_loss = 3.420\n",
      "Epoch   6 Batch 1662/2303   train_loss = 4.752\n",
      "Epoch   6 Batch 1682/2303   train_loss = 4.633\n",
      "Epoch   6 Batch 1702/2303   train_loss = 4.260\n",
      "Epoch   6 Batch 1722/2303   train_loss = 4.261\n",
      "Epoch   6 Batch 1742/2303   train_loss = 3.291\n",
      "Epoch   6 Batch 1762/2303   train_loss = 4.116\n",
      "Epoch   6 Batch 1782/2303   train_loss = 4.622\n",
      "Epoch   6 Batch 1802/2303   train_loss = 4.213\n",
      "Epoch   6 Batch 1822/2303   train_loss = 4.597\n",
      "Epoch   6 Batch 1842/2303   train_loss = 3.628\n",
      "Epoch   6 Batch 1862/2303   train_loss = 3.733\n",
      "Epoch   6 Batch 1882/2303   train_loss = 3.594\n",
      "Epoch   6 Batch 1902/2303   train_loss = 3.517\n",
      "Epoch   6 Batch 1922/2303   train_loss = 4.745\n",
      "Epoch   6 Batch 1942/2303   train_loss = 5.182\n",
      "Epoch   6 Batch 1962/2303   train_loss = 4.063\n",
      "Epoch   6 Batch 1982/2303   train_loss = 4.022\n",
      "Epoch   6 Batch 2002/2303   train_loss = 4.430\n",
      "Epoch   6 Batch 2022/2303   train_loss = 4.918\n",
      "Epoch   6 Batch 2042/2303   train_loss = 3.309\n",
      "Epoch   6 Batch 2062/2303   train_loss = 4.604\n",
      "Epoch   6 Batch 2082/2303   train_loss = 3.377\n",
      "Epoch   6 Batch 2102/2303   train_loss = 4.217\n",
      "Epoch   6 Batch 2122/2303   train_loss = 4.941\n",
      "Epoch   6 Batch 2142/2303   train_loss = 4.746\n",
      "Epoch   6 Batch 2162/2303   train_loss = 4.461\n",
      "Epoch   6 Batch 2182/2303   train_loss = 3.538\n",
      "Epoch   6 Batch 2202/2303   train_loss = 4.819\n",
      "Epoch   6 Batch 2222/2303   train_loss = 4.332\n",
      "Epoch   6 Batch 2242/2303   train_loss = 3.597\n",
      "Epoch   6 Batch 2262/2303   train_loss = 3.283\n",
      "Epoch   6 Batch 2282/2303   train_loss = 4.324\n",
      "Epoch   6 Batch 2302/2303   train_loss = 3.580\n",
      "Epoch   7 Batch   19/2303   train_loss = 2.900\n",
      "Epoch   7 Batch   39/2303   train_loss = 3.887\n",
      "Epoch   7 Batch   59/2303   train_loss = 4.541\n",
      "Epoch   7 Batch   79/2303   train_loss = 4.375\n",
      "Epoch   7 Batch   99/2303   train_loss = 3.707\n",
      "Epoch   7 Batch  119/2303   train_loss = 3.584\n",
      "Epoch   7 Batch  139/2303   train_loss = 4.319\n",
      "Epoch   7 Batch  159/2303   train_loss = 3.200\n",
      "Epoch   7 Batch  179/2303   train_loss = 4.274\n",
      "Epoch   7 Batch  199/2303   train_loss = 4.396\n",
      "Epoch   7 Batch  219/2303   train_loss = 3.858\n",
      "Epoch   7 Batch  239/2303   train_loss = 3.864\n",
      "Epoch   7 Batch  259/2303   train_loss = 4.254\n",
      "Epoch   7 Batch  279/2303   train_loss = 3.822\n",
      "Epoch   7 Batch  299/2303   train_loss = 4.937\n",
      "Epoch   7 Batch  319/2303   train_loss = 5.226\n",
      "Epoch   7 Batch  339/2303   train_loss = 4.129\n",
      "Epoch   7 Batch  359/2303   train_loss = 3.814\n",
      "Epoch   7 Batch  379/2303   train_loss = 3.272\n",
      "Epoch   7 Batch  399/2303   train_loss = 3.598\n",
      "Epoch   7 Batch  419/2303   train_loss = 4.089\n",
      "Epoch   7 Batch  439/2303   train_loss = 4.054\n",
      "Epoch   7 Batch  459/2303   train_loss = 4.308\n",
      "Epoch   7 Batch  479/2303   train_loss = 4.367\n",
      "Epoch   7 Batch  499/2303   train_loss = 3.774\n",
      "Epoch   7 Batch  519/2303   train_loss = 3.859\n",
      "Epoch   7 Batch  539/2303   train_loss = 4.444\n",
      "Epoch   7 Batch  559/2303   train_loss = 4.063\n",
      "Epoch   7 Batch  579/2303   train_loss = 4.409\n",
      "Epoch   7 Batch  599/2303   train_loss = 4.288\n",
      "Epoch   7 Batch  619/2303   train_loss = 4.001\n",
      "Epoch   7 Batch  639/2303   train_loss = 4.197\n",
      "Epoch   7 Batch  659/2303   train_loss = 5.093\n",
      "Epoch   7 Batch  679/2303   train_loss = 4.463\n",
      "Epoch   7 Batch  699/2303   train_loss = 4.337\n",
      "Epoch   7 Batch  719/2303   train_loss = 3.995\n",
      "Epoch   7 Batch  739/2303   train_loss = 2.963\n",
      "Epoch   7 Batch  759/2303   train_loss = 3.990\n",
      "Epoch   7 Batch  779/2303   train_loss = 4.033\n",
      "Epoch   7 Batch  799/2303   train_loss = 3.406\n",
      "Epoch   7 Batch  819/2303   train_loss = 3.304\n",
      "Epoch   7 Batch  839/2303   train_loss = 3.805\n",
      "Epoch   7 Batch  859/2303   train_loss = 3.488\n",
      "Epoch   7 Batch  879/2303   train_loss = 3.155\n",
      "Epoch   7 Batch  899/2303   train_loss = 4.461\n",
      "Epoch   7 Batch  919/2303   train_loss = 3.133\n",
      "Epoch   7 Batch  939/2303   train_loss = 4.894\n",
      "Epoch   7 Batch  959/2303   train_loss = 4.121\n",
      "Epoch   7 Batch  979/2303   train_loss = 4.340\n",
      "Epoch   7 Batch  999/2303   train_loss = 3.864\n",
      "Epoch   7 Batch 1019/2303   train_loss = 3.526\n",
      "Epoch   7 Batch 1039/2303   train_loss = 3.305\n",
      "Epoch   7 Batch 1059/2303   train_loss = 2.889\n",
      "Epoch   7 Batch 1079/2303   train_loss = 4.491\n",
      "Epoch   7 Batch 1099/2303   train_loss = 4.281\n",
      "Epoch   7 Batch 1119/2303   train_loss = 4.177\n",
      "Epoch   7 Batch 1139/2303   train_loss = 4.303\n",
      "Epoch   7 Batch 1159/2303   train_loss = 4.184\n",
      "Epoch   7 Batch 1179/2303   train_loss = 3.650\n",
      "Epoch   7 Batch 1199/2303   train_loss = 3.727\n",
      "Epoch   7 Batch 1219/2303   train_loss = 3.756\n",
      "Epoch   7 Batch 1239/2303   train_loss = 4.799\n",
      "Epoch   7 Batch 1259/2303   train_loss = 5.160\n",
      "Epoch   7 Batch 1279/2303   train_loss = 4.236\n",
      "Epoch   7 Batch 1299/2303   train_loss = 4.196\n",
      "Epoch   7 Batch 1319/2303   train_loss = 4.353\n",
      "Epoch   7 Batch 1339/2303   train_loss = 4.783\n",
      "Epoch   7 Batch 1359/2303   train_loss = 4.906\n",
      "Epoch   7 Batch 1379/2303   train_loss = 3.537\n",
      "Epoch   7 Batch 1399/2303   train_loss = 4.188\n",
      "Epoch   7 Batch 1419/2303   train_loss = 4.416\n",
      "Epoch   7 Batch 1439/2303   train_loss = 4.830\n",
      "Epoch   7 Batch 1459/2303   train_loss = 3.675\n",
      "Epoch   7 Batch 1479/2303   train_loss = 4.814\n",
      "Epoch   7 Batch 1499/2303   train_loss = 3.870\n",
      "Epoch   7 Batch 1519/2303   train_loss = 4.369\n",
      "Epoch   7 Batch 1539/2303   train_loss = 3.455\n",
      "Epoch   7 Batch 1559/2303   train_loss = 4.317\n",
      "Epoch   7 Batch 1579/2303   train_loss = 4.176\n",
      "Epoch   7 Batch 1599/2303   train_loss = 4.570\n",
      "Epoch   7 Batch 1619/2303   train_loss = 4.787\n",
      "Epoch   7 Batch 1639/2303   train_loss = 3.962\n",
      "Epoch   7 Batch 1659/2303   train_loss = 4.500\n",
      "Epoch   7 Batch 1679/2303   train_loss = 3.554\n",
      "Epoch   7 Batch 1699/2303   train_loss = 4.275\n",
      "Epoch   7 Batch 1719/2303   train_loss = 4.154\n",
      "Epoch   7 Batch 1739/2303   train_loss = 4.544\n",
      "Epoch   7 Batch 1759/2303   train_loss = 4.573\n",
      "Epoch   7 Batch 1779/2303   train_loss = 3.735\n",
      "Epoch   7 Batch 1799/2303   train_loss = 3.409\n",
      "Epoch   7 Batch 1819/2303   train_loss = 4.650\n",
      "Epoch   7 Batch 1839/2303   train_loss = 4.140\n",
      "Epoch   7 Batch 1859/2303   train_loss = 4.178\n",
      "Epoch   7 Batch 1879/2303   train_loss = 4.147\n",
      "Epoch   7 Batch 1899/2303   train_loss = 4.049\n",
      "Epoch   7 Batch 1919/2303   train_loss = 3.264\n",
      "Epoch   7 Batch 1939/2303   train_loss = 3.832\n",
      "Epoch   7 Batch 1959/2303   train_loss = 3.974\n",
      "Epoch   7 Batch 1979/2303   train_loss = 4.201\n",
      "Epoch   7 Batch 1999/2303   train_loss = 3.329\n",
      "Epoch   7 Batch 2019/2303   train_loss = 3.604\n",
      "Epoch   7 Batch 2039/2303   train_loss = 3.594\n",
      "Epoch   7 Batch 2059/2303   train_loss = 4.610\n",
      "Epoch   7 Batch 2079/2303   train_loss = 4.556\n",
      "Epoch   7 Batch 2099/2303   train_loss = 3.243\n",
      "Epoch   7 Batch 2119/2303   train_loss = 3.967\n",
      "Epoch   7 Batch 2139/2303   train_loss = 4.051\n",
      "Epoch   7 Batch 2159/2303   train_loss = 3.367\n",
      "Epoch   7 Batch 2179/2303   train_loss = 3.864\n",
      "Epoch   7 Batch 2199/2303   train_loss = 3.742\n",
      "Epoch   7 Batch 2219/2303   train_loss = 3.644\n",
      "Epoch   7 Batch 2239/2303   train_loss = 4.424\n",
      "Epoch   7 Batch 2259/2303   train_loss = 4.096\n",
      "Epoch   7 Batch 2279/2303   train_loss = 4.513\n",
      "Epoch   7 Batch 2299/2303   train_loss = 5.354\n",
      "Epoch   8 Batch   16/2303   train_loss = 4.316\n",
      "Epoch   8 Batch   36/2303   train_loss = 4.246\n",
      "Epoch   8 Batch   56/2303   train_loss = 4.796\n",
      "Epoch   8 Batch   76/2303   train_loss = 4.084\n",
      "Epoch   8 Batch   96/2303   train_loss = 4.389\n",
      "Epoch   8 Batch  116/2303   train_loss = 3.680\n",
      "Epoch   8 Batch  136/2303   train_loss = 4.178\n",
      "Epoch   8 Batch  156/2303   train_loss = 3.280\n",
      "Epoch   8 Batch  176/2303   train_loss = 4.776\n",
      "Epoch   8 Batch  196/2303   train_loss = 4.339\n",
      "Epoch   8 Batch  216/2303   train_loss = 3.817\n",
      "Epoch   8 Batch  236/2303   train_loss = 3.737\n",
      "Epoch   8 Batch  256/2303   train_loss = 3.964\n",
      "Epoch   8 Batch  276/2303   train_loss = 4.208\n",
      "Epoch   8 Batch  296/2303   train_loss = 4.816\n",
      "Epoch   8 Batch  316/2303   train_loss = 3.625\n",
      "Epoch   8 Batch  336/2303   train_loss = 3.929\n",
      "Epoch   8 Batch  356/2303   train_loss = 3.719\n",
      "Epoch   8 Batch  376/2303   train_loss = 3.478\n",
      "Epoch   8 Batch  396/2303   train_loss = 3.827\n",
      "Epoch   8 Batch  416/2303   train_loss = 4.158\n",
      "Epoch   8 Batch  436/2303   train_loss = 3.874\n",
      "Epoch   8 Batch  456/2303   train_loss = 4.342\n",
      "Epoch   8 Batch  476/2303   train_loss = 5.287\n",
      "Epoch   8 Batch  496/2303   train_loss = 4.687\n",
      "Epoch   8 Batch  516/2303   train_loss = 3.938\n",
      "Epoch   8 Batch  536/2303   train_loss = 5.284\n",
      "Epoch   8 Batch  556/2303   train_loss = 4.173\n",
      "Epoch   8 Batch  576/2303   train_loss = 3.978\n",
      "Epoch   8 Batch  596/2303   train_loss = 4.310\n",
      "Epoch   8 Batch  616/2303   train_loss = 3.353\n",
      "Epoch   8 Batch  636/2303   train_loss = 5.369\n",
      "Epoch   8 Batch  656/2303   train_loss = 4.041\n",
      "Epoch   8 Batch  676/2303   train_loss = 3.749\n",
      "Epoch   8 Batch  696/2303   train_loss = 4.522\n",
      "Epoch   8 Batch  716/2303   train_loss = 4.088\n",
      "Epoch   8 Batch  736/2303   train_loss = 3.417\n",
      "Epoch   8 Batch  756/2303   train_loss = 3.754\n",
      "Epoch   8 Batch  776/2303   train_loss = 3.565\n",
      "Epoch   8 Batch  796/2303   train_loss = 3.708\n",
      "Epoch   8 Batch  816/2303   train_loss = 3.867\n",
      "Epoch   8 Batch  836/2303   train_loss = 3.925\n",
      "Epoch   8 Batch  856/2303   train_loss = 4.110\n",
      "Epoch   8 Batch  876/2303   train_loss = 3.659\n",
      "Epoch   8 Batch  896/2303   train_loss = 3.626\n",
      "Epoch   8 Batch  916/2303   train_loss = 3.352\n",
      "Epoch   8 Batch  936/2303   train_loss = 4.996\n",
      "Epoch   8 Batch  956/2303   train_loss = 4.223\n",
      "Epoch   8 Batch  976/2303   train_loss = 3.604\n",
      "Epoch   8 Batch  996/2303   train_loss = 4.003\n",
      "Epoch   8 Batch 1016/2303   train_loss = 3.701\n",
      "Epoch   8 Batch 1036/2303   train_loss = 3.627\n",
      "Epoch   8 Batch 1056/2303   train_loss = 3.287\n",
      "Epoch   8 Batch 1076/2303   train_loss = 3.802\n",
      "Epoch   8 Batch 1096/2303   train_loss = 3.785\n",
      "Epoch   8 Batch 1116/2303   train_loss = 3.375\n",
      "Epoch   8 Batch 1136/2303   train_loss = 3.952\n",
      "Epoch   8 Batch 1156/2303   train_loss = 4.324\n",
      "Epoch   8 Batch 1176/2303   train_loss = 4.185\n",
      "Epoch   8 Batch 1196/2303   train_loss = 3.955\n",
      "Epoch   8 Batch 1216/2303   train_loss = 3.852\n",
      "Epoch   8 Batch 1236/2303   train_loss = 3.917\n",
      "Epoch   8 Batch 1256/2303   train_loss = 4.326\n",
      "Epoch   8 Batch 1276/2303   train_loss = 4.139\n",
      "Epoch   8 Batch 1296/2303   train_loss = 4.057\n",
      "Epoch   8 Batch 1316/2303   train_loss = 4.494\n",
      "Epoch   8 Batch 1336/2303   train_loss = 3.682\n",
      "Epoch   8 Batch 1356/2303   train_loss = 3.655\n",
      "Epoch   8 Batch 1376/2303   train_loss = 4.827\n",
      "Epoch   8 Batch 1396/2303   train_loss = 3.457\n",
      "Epoch   8 Batch 1416/2303   train_loss = 3.392\n",
      "Epoch   8 Batch 1436/2303   train_loss = 4.056\n",
      "Epoch   8 Batch 1456/2303   train_loss = 4.843\n",
      "Epoch   8 Batch 1476/2303   train_loss = 3.828\n",
      "Epoch   8 Batch 1496/2303   train_loss = 3.344\n",
      "Epoch   8 Batch 1516/2303   train_loss = 4.900\n",
      "Epoch   8 Batch 1536/2303   train_loss = 4.225\n",
      "Epoch   8 Batch 1556/2303   train_loss = 4.188\n",
      "Epoch   8 Batch 1576/2303   train_loss = 3.894\n",
      "Epoch   8 Batch 1596/2303   train_loss = 4.487\n",
      "Epoch   8 Batch 1616/2303   train_loss = 3.884\n",
      "Epoch   8 Batch 1636/2303   train_loss = 5.047\n",
      "Epoch   8 Batch 1656/2303   train_loss = 4.287\n",
      "Epoch   8 Batch 1676/2303   train_loss = 4.956\n",
      "Epoch   8 Batch 1696/2303   train_loss = 4.291\n",
      "Epoch   8 Batch 1716/2303   train_loss = 4.058\n",
      "Epoch   8 Batch 1736/2303   train_loss = 4.133\n",
      "Epoch   8 Batch 1756/2303   train_loss = 3.838\n",
      "Epoch   8 Batch 1776/2303   train_loss = 5.074\n",
      "Epoch   8 Batch 1796/2303   train_loss = 4.387\n",
      "Epoch   8 Batch 1816/2303   train_loss = 4.914\n",
      "Epoch   8 Batch 1836/2303   train_loss = 3.726\n",
      "Epoch   8 Batch 1856/2303   train_loss = 3.590\n",
      "Epoch   8 Batch 1876/2303   train_loss = 4.879\n",
      "Epoch   8 Batch 1896/2303   train_loss = 3.995\n",
      "Epoch   8 Batch 1916/2303   train_loss = 3.811\n",
      "Epoch   8 Batch 1936/2303   train_loss = 3.580\n",
      "Epoch   8 Batch 1956/2303   train_loss = 4.075\n",
      "Epoch   8 Batch 1976/2303   train_loss = 3.875\n",
      "Epoch   8 Batch 1996/2303   train_loss = 3.964\n",
      "Epoch   8 Batch 2016/2303   train_loss = 3.238\n",
      "Epoch   8 Batch 2036/2303   train_loss = 4.623\n",
      "Epoch   8 Batch 2056/2303   train_loss = 3.779\n",
      "Epoch   8 Batch 2076/2303   train_loss = 3.945\n",
      "Epoch   8 Batch 2096/2303   train_loss = 4.564\n",
      "Epoch   8 Batch 2116/2303   train_loss = 3.727\n",
      "Epoch   8 Batch 2136/2303   train_loss = 3.316\n",
      "Epoch   8 Batch 2156/2303   train_loss = 4.300\n",
      "Epoch   8 Batch 2176/2303   train_loss = 3.884\n",
      "Epoch   8 Batch 2196/2303   train_loss = 5.295\n",
      "Epoch   8 Batch 2216/2303   train_loss = 4.347\n",
      "Epoch   8 Batch 2236/2303   train_loss = 3.865\n",
      "Epoch   8 Batch 2256/2303   train_loss = 4.028\n",
      "Epoch   8 Batch 2276/2303   train_loss = 4.526\n",
      "Epoch   8 Batch 2296/2303   train_loss = 3.919\n",
      "Epoch   9 Batch   13/2303   train_loss = 3.224\n",
      "Epoch   9 Batch   33/2303   train_loss = 5.392\n",
      "Epoch   9 Batch   53/2303   train_loss = 4.675\n",
      "Epoch   9 Batch   73/2303   train_loss = 5.393\n",
      "Epoch   9 Batch   93/2303   train_loss = 3.960\n",
      "Epoch   9 Batch  113/2303   train_loss = 3.374\n",
      "Epoch   9 Batch  133/2303   train_loss = 4.763\n",
      "Epoch   9 Batch  153/2303   train_loss = 4.183\n",
      "Epoch   9 Batch  173/2303   train_loss = 3.434\n",
      "Epoch   9 Batch  193/2303   train_loss = 3.548\n",
      "Epoch   9 Batch  213/2303   train_loss = 5.078\n",
      "Epoch   9 Batch  233/2303   train_loss = 3.999\n",
      "Epoch   9 Batch  253/2303   train_loss = 3.975\n",
      "Epoch   9 Batch  273/2303   train_loss = 4.006\n",
      "Epoch   9 Batch  293/2303   train_loss = 4.486\n",
      "Epoch   9 Batch  313/2303   train_loss = 3.527\n",
      "Epoch   9 Batch  333/2303   train_loss = 3.866\n",
      "Epoch   9 Batch  353/2303   train_loss = 3.535\n",
      "Epoch   9 Batch  373/2303   train_loss = 4.214\n",
      "Epoch   9 Batch  393/2303   train_loss = 4.366\n",
      "Epoch   9 Batch  413/2303   train_loss = 3.505\n",
      "Epoch   9 Batch  433/2303   train_loss = 4.283\n",
      "Epoch   9 Batch  453/2303   train_loss = 4.170\n",
      "Epoch   9 Batch  473/2303   train_loss = 3.793\n",
      "Epoch   9 Batch  493/2303   train_loss = 4.206\n",
      "Epoch   9 Batch  513/2303   train_loss = 4.004\n",
      "Epoch   9 Batch  533/2303   train_loss = 3.643\n",
      "Epoch   9 Batch  553/2303   train_loss = 4.050\n",
      "Epoch   9 Batch  573/2303   train_loss = 3.873\n",
      "Epoch   9 Batch  593/2303   train_loss = 3.396\n",
      "Epoch   9 Batch  613/2303   train_loss = 3.889\n",
      "Epoch   9 Batch  633/2303   train_loss = 4.657\n",
      "Epoch   9 Batch  653/2303   train_loss = 3.633\n",
      "Epoch   9 Batch  673/2303   train_loss = 4.439\n",
      "Epoch   9 Batch  693/2303   train_loss = 3.771\n",
      "Epoch   9 Batch  713/2303   train_loss = 4.483\n",
      "Epoch   9 Batch  733/2303   train_loss = 3.555\n",
      "Epoch   9 Batch  753/2303   train_loss = 4.129\n",
      "Epoch   9 Batch  773/2303   train_loss = 4.373\n",
      "Epoch   9 Batch  793/2303   train_loss = 4.078\n",
      "Epoch   9 Batch  813/2303   train_loss = 3.693\n",
      "Epoch   9 Batch  833/2303   train_loss = 3.651\n",
      "Epoch   9 Batch  853/2303   train_loss = 4.631\n",
      "Epoch   9 Batch  873/2303   train_loss = 4.079\n",
      "Epoch   9 Batch  893/2303   train_loss = 3.822\n",
      "Epoch   9 Batch  913/2303   train_loss = 3.996\n",
      "Epoch   9 Batch  933/2303   train_loss = 4.238\n",
      "Epoch   9 Batch  953/2303   train_loss = 3.779\n",
      "Epoch   9 Batch  973/2303   train_loss = 3.807\n",
      "Epoch   9 Batch  993/2303   train_loss = 3.846\n",
      "Epoch   9 Batch 1013/2303   train_loss = 3.777\n",
      "Epoch   9 Batch 1033/2303   train_loss = 4.089\n",
      "Epoch   9 Batch 1053/2303   train_loss = 4.039\n",
      "Epoch   9 Batch 1073/2303   train_loss = 4.018\n",
      "Epoch   9 Batch 1093/2303   train_loss = 4.017\n",
      "Epoch   9 Batch 1113/2303   train_loss = 3.762\n",
      "Epoch   9 Batch 1133/2303   train_loss = 4.265\n",
      "Epoch   9 Batch 1153/2303   train_loss = 3.319\n",
      "Epoch   9 Batch 1173/2303   train_loss = 4.044\n",
      "Epoch   9 Batch 1193/2303   train_loss = 4.594\n",
      "Epoch   9 Batch 1213/2303   train_loss = 3.381\n",
      "Epoch   9 Batch 1233/2303   train_loss = 3.847\n",
      "Epoch   9 Batch 1253/2303   train_loss = 4.912\n",
      "Epoch   9 Batch 1273/2303   train_loss = 4.035\n",
      "Epoch   9 Batch 1293/2303   train_loss = 4.800\n",
      "Epoch   9 Batch 1313/2303   train_loss = 3.608\n",
      "Epoch   9 Batch 1333/2303   train_loss = 4.142\n",
      "Epoch   9 Batch 1353/2303   train_loss = 3.475\n",
      "Epoch   9 Batch 1373/2303   train_loss = 3.953\n",
      "Epoch   9 Batch 1393/2303   train_loss = 3.208\n",
      "Epoch   9 Batch 1413/2303   train_loss = 4.279\n",
      "Epoch   9 Batch 1433/2303   train_loss = 4.888\n",
      "Epoch   9 Batch 1453/2303   train_loss = 4.091\n",
      "Epoch   9 Batch 1473/2303   train_loss = 5.037\n",
      "Epoch   9 Batch 1493/2303   train_loss = 4.273\n",
      "Epoch   9 Batch 1513/2303   train_loss = 4.423\n",
      "Epoch   9 Batch 1533/2303   train_loss = 4.805\n",
      "Epoch   9 Batch 1553/2303   train_loss = 4.110\n",
      "Epoch   9 Batch 1573/2303   train_loss = 4.670\n",
      "Epoch   9 Batch 1593/2303   train_loss = 4.336\n",
      "Epoch   9 Batch 1613/2303   train_loss = 4.014\n",
      "Epoch   9 Batch 1633/2303   train_loss = 4.260\n",
      "Epoch   9 Batch 1653/2303   train_loss = 3.465\n",
      "Epoch   9 Batch 1673/2303   train_loss = 3.613\n",
      "Epoch   9 Batch 1693/2303   train_loss = 3.911\n",
      "Epoch   9 Batch 1713/2303   train_loss = 4.086\n",
      "Epoch   9 Batch 1733/2303   train_loss = 4.585\n",
      "Epoch   9 Batch 1753/2303   train_loss = 3.817\n",
      "Epoch   9 Batch 1773/2303   train_loss = 3.644\n",
      "Epoch   9 Batch 1793/2303   train_loss = 3.776\n",
      "Epoch   9 Batch 1813/2303   train_loss = 3.559\n",
      "Epoch   9 Batch 1833/2303   train_loss = 4.336\n",
      "Epoch   9 Batch 1853/2303   train_loss = 4.079\n",
      "Epoch   9 Batch 1873/2303   train_loss = 3.736\n",
      "Epoch   9 Batch 1893/2303   train_loss = 4.196\n",
      "Epoch   9 Batch 1913/2303   train_loss = 4.464\n",
      "Epoch   9 Batch 1933/2303   train_loss = 4.316\n",
      "Epoch   9 Batch 1953/2303   train_loss = 4.675\n",
      "Epoch   9 Batch 1973/2303   train_loss = 3.909\n",
      "Epoch   9 Batch 1993/2303   train_loss = 4.126\n",
      "Epoch   9 Batch 2013/2303   train_loss = 3.861\n",
      "Epoch   9 Batch 2033/2303   train_loss = 5.029\n",
      "Epoch   9 Batch 2053/2303   train_loss = 4.559\n",
      "Epoch   9 Batch 2073/2303   train_loss = 4.172\n",
      "Epoch   9 Batch 2093/2303   train_loss = 4.230\n",
      "Epoch   9 Batch 2113/2303   train_loss = 4.214\n",
      "Epoch   9 Batch 2133/2303   train_loss = 3.976\n",
      "Epoch   9 Batch 2153/2303   train_loss = 4.107\n",
      "Epoch   9 Batch 2173/2303   train_loss = 4.010\n",
      "Epoch   9 Batch 2193/2303   train_loss = 4.270\n",
      "Epoch   9 Batch 2213/2303   train_loss = 4.212\n",
      "Epoch   9 Batch 2233/2303   train_loss = 3.791\n",
      "Epoch   9 Batch 2253/2303   train_loss = 4.132\n",
      "Epoch   9 Batch 2273/2303   train_loss = 4.653\n",
      "Epoch   9 Batch 2293/2303   train_loss = 3.512\n",
      "Epoch  10 Batch   10/2303   train_loss = 4.465\n",
      "Epoch  10 Batch   30/2303   train_loss = 4.028\n",
      "Epoch  10 Batch   50/2303   train_loss = 4.605\n",
      "Epoch  10 Batch   70/2303   train_loss = 4.255\n",
      "Epoch  10 Batch   90/2303   train_loss = 5.149\n",
      "Epoch  10 Batch  110/2303   train_loss = 4.309\n",
      "Epoch  10 Batch  130/2303   train_loss = 3.897\n",
      "Epoch  10 Batch  150/2303   train_loss = 3.705\n",
      "Epoch  10 Batch  170/2303   train_loss = 4.668\n",
      "Epoch  10 Batch  190/2303   train_loss = 4.741\n",
      "Epoch  10 Batch  210/2303   train_loss = 3.329\n",
      "Epoch  10 Batch  230/2303   train_loss = 3.391\n",
      "Epoch  10 Batch  250/2303   train_loss = 3.007\n",
      "Epoch  10 Batch  270/2303   train_loss = 3.908\n",
      "Epoch  10 Batch  290/2303   train_loss = 3.407\n",
      "Epoch  10 Batch  310/2303   train_loss = 5.099\n",
      "Epoch  10 Batch  330/2303   train_loss = 4.981\n",
      "Epoch  10 Batch  350/2303   train_loss = 4.722\n",
      "Epoch  10 Batch  370/2303   train_loss = 4.024\n",
      "Epoch  10 Batch  390/2303   train_loss = 4.144\n",
      "Epoch  10 Batch  410/2303   train_loss = 4.397\n",
      "Epoch  10 Batch  430/2303   train_loss = 3.120\n",
      "Epoch  10 Batch  450/2303   train_loss = 3.784\n",
      "Epoch  10 Batch  470/2303   train_loss = 3.949\n",
      "Epoch  10 Batch  490/2303   train_loss = 4.249\n",
      "Epoch  10 Batch  510/2303   train_loss = 4.361\n",
      "Epoch  10 Batch  530/2303   train_loss = 4.024\n",
      "Epoch  10 Batch  550/2303   train_loss = 3.753\n",
      "Epoch  10 Batch  570/2303   train_loss = 4.093\n",
      "Epoch  10 Batch  590/2303   train_loss = 4.093\n",
      "Epoch  10 Batch  610/2303   train_loss = 3.635\n",
      "Epoch  10 Batch  630/2303   train_loss = 4.229\n",
      "Epoch  10 Batch  650/2303   train_loss = 3.877\n",
      "Epoch  10 Batch  670/2303   train_loss = 3.871\n",
      "Epoch  10 Batch  690/2303   train_loss = 4.220\n",
      "Epoch  10 Batch  710/2303   train_loss = 4.009\n",
      "Epoch  10 Batch  730/2303   train_loss = 4.230\n",
      "Epoch  10 Batch  750/2303   train_loss = 3.971\n",
      "Epoch  10 Batch  770/2303   train_loss = 4.556\n",
      "Epoch  10 Batch  790/2303   train_loss = 3.355\n",
      "Epoch  10 Batch  810/2303   train_loss = 4.269\n",
      "Epoch  10 Batch  830/2303   train_loss = 3.949\n",
      "Epoch  10 Batch  850/2303   train_loss = 4.143\n",
      "Epoch  10 Batch  870/2303   train_loss = 2.933\n",
      "Epoch  10 Batch  890/2303   train_loss = 3.913\n",
      "Epoch  10 Batch  910/2303   train_loss = 3.876\n",
      "Epoch  10 Batch  930/2303   train_loss = 3.860\n",
      "Epoch  10 Batch  950/2303   train_loss = 3.420\n",
      "Epoch  10 Batch  970/2303   train_loss = 3.655\n",
      "Epoch  10 Batch  990/2303   train_loss = 3.594\n",
      "Epoch  10 Batch 1010/2303   train_loss = 3.956\n",
      "Epoch  10 Batch 1030/2303   train_loss = 3.471\n",
      "Epoch  10 Batch 1050/2303   train_loss = 3.807\n",
      "Epoch  10 Batch 1070/2303   train_loss = 4.266\n",
      "Epoch  10 Batch 1090/2303   train_loss = 5.123\n",
      "Epoch  10 Batch 1110/2303   train_loss = 3.328\n",
      "Epoch  10 Batch 1130/2303   train_loss = 3.939\n",
      "Epoch  10 Batch 1150/2303   train_loss = 4.425\n",
      "Epoch  10 Batch 1170/2303   train_loss = 5.028\n",
      "Epoch  10 Batch 1190/2303   train_loss = 3.267\n",
      "Epoch  10 Batch 1210/2303   train_loss = 4.246\n",
      "Epoch  10 Batch 1230/2303   train_loss = 4.024\n",
      "Epoch  10 Batch 1250/2303   train_loss = 3.797\n",
      "Epoch  10 Batch 1270/2303   train_loss = 3.812\n",
      "Epoch  10 Batch 1290/2303   train_loss = 3.336\n",
      "Epoch  10 Batch 1310/2303   train_loss = 4.314\n",
      "Epoch  10 Batch 1330/2303   train_loss = 4.222\n",
      "Epoch  10 Batch 1350/2303   train_loss = 3.593\n",
      "Epoch  10 Batch 1370/2303   train_loss = 4.023\n",
      "Epoch  10 Batch 1390/2303   train_loss = 3.514\n",
      "Epoch  10 Batch 1410/2303   train_loss = 4.739\n",
      "Epoch  10 Batch 1430/2303   train_loss = 3.991\n",
      "Epoch  10 Batch 1450/2303   train_loss = 3.855\n",
      "Epoch  10 Batch 1470/2303   train_loss = 3.302\n",
      "Epoch  10 Batch 1490/2303   train_loss = 4.798\n",
      "Epoch  10 Batch 1510/2303   train_loss = 4.397\n",
      "Epoch  10 Batch 1530/2303   train_loss = 4.864\n",
      "Epoch  10 Batch 1550/2303   train_loss = 4.555\n",
      "Epoch  10 Batch 1570/2303   train_loss = 4.311\n",
      "Epoch  10 Batch 1590/2303   train_loss = 4.323\n",
      "Epoch  10 Batch 1610/2303   train_loss = 3.103\n",
      "Epoch  10 Batch 1630/2303   train_loss = 3.644\n",
      "Epoch  10 Batch 1650/2303   train_loss = 4.457\n",
      "Epoch  10 Batch 1670/2303   train_loss = 3.819\n",
      "Epoch  10 Batch 1690/2303   train_loss = 4.707\n",
      "Epoch  10 Batch 1710/2303   train_loss = 3.315\n",
      "Epoch  10 Batch 1730/2303   train_loss = 4.313\n",
      "Epoch  10 Batch 1750/2303   train_loss = 3.606\n",
      "Epoch  10 Batch 1770/2303   train_loss = 3.801\n",
      "Epoch  10 Batch 1790/2303   train_loss = 4.382\n",
      "Epoch  10 Batch 1810/2303   train_loss = 3.841\n",
      "Epoch  10 Batch 1830/2303   train_loss = 3.735\n",
      "Epoch  10 Batch 1850/2303   train_loss = 4.407\n",
      "Epoch  10 Batch 1870/2303   train_loss = 3.045\n",
      "Epoch  10 Batch 1890/2303   train_loss = 4.276\n",
      "Epoch  10 Batch 1910/2303   train_loss = 4.594\n",
      "Epoch  10 Batch 1930/2303   train_loss = 3.369\n",
      "Epoch  10 Batch 1950/2303   train_loss = 3.599\n",
      "Epoch  10 Batch 1970/2303   train_loss = 4.165\n",
      "Epoch  10 Batch 1990/2303   train_loss = 3.913\n",
      "Epoch  10 Batch 2010/2303   train_loss = 3.577\n",
      "Epoch  10 Batch 2030/2303   train_loss = 4.533\n",
      "Epoch  10 Batch 2050/2303   train_loss = 5.067\n",
      "Epoch  10 Batch 2070/2303   train_loss = 4.307\n",
      "Epoch  10 Batch 2090/2303   train_loss = 3.365\n",
      "Epoch  10 Batch 2110/2303   train_loss = 4.354\n",
      "Epoch  10 Batch 2130/2303   train_loss = 3.679\n",
      "Epoch  10 Batch 2150/2303   train_loss = 4.122\n",
      "Epoch  10 Batch 2170/2303   train_loss = 3.967\n",
      "Epoch  10 Batch 2190/2303   train_loss = 3.894\n",
      "Epoch  10 Batch 2210/2303   train_loss = 3.774\n",
      "Epoch  10 Batch 2230/2303   train_loss = 4.111\n",
      "Epoch  10 Batch 2250/2303   train_loss = 2.826\n",
      "Epoch  10 Batch 2270/2303   train_loss = 3.681\n",
      "Epoch  10 Batch 2290/2303   train_loss = 2.994\n",
      "Epoch  11 Batch    7/2303   train_loss = 4.337\n",
      "Epoch  11 Batch   27/2303   train_loss = 3.889\n",
      "Epoch  11 Batch   47/2303   train_loss = 4.088\n",
      "Epoch  11 Batch   67/2303   train_loss = 3.843\n",
      "Epoch  11 Batch   87/2303   train_loss = 4.688\n",
      "Epoch  11 Batch  107/2303   train_loss = 4.545\n",
      "Epoch  11 Batch  127/2303   train_loss = 4.702\n",
      "Epoch  11 Batch  147/2303   train_loss = 3.452\n",
      "Epoch  11 Batch  167/2303   train_loss = 3.976\n",
      "Epoch  11 Batch  187/2303   train_loss = 3.855\n",
      "Epoch  11 Batch  207/2303   train_loss = 4.452\n",
      "Epoch  11 Batch  227/2303   train_loss = 4.073\n",
      "Epoch  11 Batch  247/2303   train_loss = 4.355\n",
      "Epoch  11 Batch  267/2303   train_loss = 3.711\n",
      "Epoch  11 Batch  287/2303   train_loss = 3.209\n",
      "Epoch  11 Batch  307/2303   train_loss = 3.342\n",
      "Epoch  11 Batch  327/2303   train_loss = 3.742\n",
      "Epoch  11 Batch  347/2303   train_loss = 4.646\n",
      "Epoch  11 Batch  367/2303   train_loss = 3.494\n",
      "Epoch  11 Batch  387/2303   train_loss = 3.422\n",
      "Epoch  11 Batch  407/2303   train_loss = 4.000\n",
      "Epoch  11 Batch  427/2303   train_loss = 3.356\n",
      "Epoch  11 Batch  447/2303   train_loss = 3.335\n",
      "Epoch  11 Batch  467/2303   train_loss = 3.652\n",
      "Epoch  11 Batch  487/2303   train_loss = 4.144\n",
      "Epoch  11 Batch  507/2303   train_loss = 3.488\n",
      "Epoch  11 Batch  527/2303   train_loss = 3.561\n",
      "Epoch  11 Batch  547/2303   train_loss = 4.301\n",
      "Epoch  11 Batch  567/2303   train_loss = 3.834\n",
      "Epoch  11 Batch  587/2303   train_loss = 3.834\n",
      "Epoch  11 Batch  607/2303   train_loss = 3.574\n",
      "Epoch  11 Batch  627/2303   train_loss = 3.811\n",
      "Epoch  11 Batch  647/2303   train_loss = 4.607\n",
      "Epoch  11 Batch  667/2303   train_loss = 4.327\n",
      "Epoch  11 Batch  687/2303   train_loss = 3.639\n",
      "Epoch  11 Batch  707/2303   train_loss = 3.941\n",
      "Epoch  11 Batch  727/2303   train_loss = 4.126\n",
      "Epoch  11 Batch  747/2303   train_loss = 3.224\n",
      "Epoch  11 Batch  767/2303   train_loss = 4.704\n",
      "Epoch  11 Batch  787/2303   train_loss = 4.351\n",
      "Epoch  11 Batch  807/2303   train_loss = 3.571\n",
      "Epoch  11 Batch  827/2303   train_loss = 4.189\n",
      "Epoch  11 Batch  847/2303   train_loss = 4.291\n",
      "Epoch  11 Batch  867/2303   train_loss = 3.282\n",
      "Epoch  11 Batch  887/2303   train_loss = 3.599\n",
      "Epoch  11 Batch  907/2303   train_loss = 4.180\n",
      "Epoch  11 Batch  927/2303   train_loss = 3.911\n",
      "Epoch  11 Batch  947/2303   train_loss = 4.014\n",
      "Epoch  11 Batch  967/2303   train_loss = 3.382\n",
      "Epoch  11 Batch  987/2303   train_loss = 4.695\n",
      "Epoch  11 Batch 1007/2303   train_loss = 4.362\n",
      "Epoch  11 Batch 1027/2303   train_loss = 3.607\n",
      "Epoch  11 Batch 1047/2303   train_loss = 4.023\n",
      "Epoch  11 Batch 1067/2303   train_loss = 3.483\n",
      "Epoch  11 Batch 1087/2303   train_loss = 3.235\n",
      "Epoch  11 Batch 1107/2303   train_loss = 4.453\n",
      "Epoch  11 Batch 1127/2303   train_loss = 3.249\n",
      "Epoch  11 Batch 1147/2303   train_loss = 4.516\n",
      "Epoch  11 Batch 1167/2303   train_loss = 3.627\n",
      "Epoch  11 Batch 1187/2303   train_loss = 3.560\n",
      "Epoch  11 Batch 1207/2303   train_loss = 3.972\n",
      "Epoch  11 Batch 1227/2303   train_loss = 4.485\n",
      "Epoch  11 Batch 1247/2303   train_loss = 4.462\n",
      "Epoch  11 Batch 1267/2303   train_loss = 3.746\n",
      "Epoch  11 Batch 1287/2303   train_loss = 4.922\n",
      "Epoch  11 Batch 1307/2303   train_loss = 3.836\n",
      "Epoch  11 Batch 1327/2303   train_loss = 4.650\n",
      "Epoch  11 Batch 1347/2303   train_loss = 4.343\n",
      "Epoch  11 Batch 1367/2303   train_loss = 4.716\n",
      "Epoch  11 Batch 1387/2303   train_loss = 4.347\n",
      "Epoch  11 Batch 1407/2303   train_loss = 4.018\n",
      "Epoch  11 Batch 1427/2303   train_loss = 3.532\n",
      "Epoch  11 Batch 1447/2303   train_loss = 3.885\n",
      "Epoch  11 Batch 1467/2303   train_loss = 3.596\n",
      "Epoch  11 Batch 1487/2303   train_loss = 3.815\n",
      "Epoch  11 Batch 1507/2303   train_loss = 4.335\n",
      "Epoch  11 Batch 1527/2303   train_loss = 3.940\n",
      "Epoch  11 Batch 1547/2303   train_loss = 2.759\n",
      "Epoch  11 Batch 1567/2303   train_loss = 3.804\n",
      "Epoch  11 Batch 1587/2303   train_loss = 3.368\n",
      "Epoch  11 Batch 1607/2303   train_loss = 3.518\n",
      "Epoch  11 Batch 1627/2303   train_loss = 3.707\n",
      "Epoch  11 Batch 1647/2303   train_loss = 4.311\n",
      "Epoch  11 Batch 1667/2303   train_loss = 4.032\n",
      "Epoch  11 Batch 1687/2303   train_loss = 5.113\n",
      "Epoch  11 Batch 1707/2303   train_loss = 4.606\n",
      "Epoch  11 Batch 1727/2303   train_loss = 3.980\n",
      "Epoch  11 Batch 1747/2303   train_loss = 3.361\n",
      "Epoch  11 Batch 1767/2303   train_loss = 2.758\n",
      "Epoch  11 Batch 1787/2303   train_loss = 3.950\n",
      "Epoch  11 Batch 1807/2303   train_loss = 4.096\n",
      "Epoch  11 Batch 1827/2303   train_loss = 4.068\n",
      "Epoch  11 Batch 1847/2303   train_loss = 3.730\n",
      "Epoch  11 Batch 1867/2303   train_loss = 3.531\n",
      "Epoch  11 Batch 1887/2303   train_loss = 3.776\n",
      "Epoch  11 Batch 1907/2303   train_loss = 3.661\n",
      "Epoch  11 Batch 1927/2303   train_loss = 4.143\n",
      "Epoch  11 Batch 1947/2303   train_loss = 4.278\n",
      "Epoch  11 Batch 1967/2303   train_loss = 3.410\n",
      "Epoch  11 Batch 1987/2303   train_loss = 4.164\n",
      "Epoch  11 Batch 2007/2303   train_loss = 3.692\n",
      "Epoch  11 Batch 2027/2303   train_loss = 4.244\n",
      "Epoch  11 Batch 2047/2303   train_loss = 4.009\n",
      "Epoch  11 Batch 2067/2303   train_loss = 4.146\n",
      "Epoch  11 Batch 2087/2303   train_loss = 3.708\n",
      "Epoch  11 Batch 2107/2303   train_loss = 3.711\n",
      "Epoch  11 Batch 2127/2303   train_loss = 4.093\n",
      "Epoch  11 Batch 2147/2303   train_loss = 3.194\n",
      "Epoch  11 Batch 2167/2303   train_loss = 4.392\n",
      "Epoch  11 Batch 2187/2303   train_loss = 3.941\n",
      "Epoch  11 Batch 2207/2303   train_loss = 3.861\n",
      "Epoch  11 Batch 2227/2303   train_loss = 3.955\n",
      "Epoch  11 Batch 2247/2303   train_loss = 3.161\n",
      "Epoch  11 Batch 2267/2303   train_loss = 5.489\n",
      "Epoch  11 Batch 2287/2303   train_loss = 4.019\n",
      "Epoch  12 Batch    4/2303   train_loss = 3.802\n",
      "Epoch  12 Batch   24/2303   train_loss = 3.463\n",
      "Epoch  12 Batch   44/2303   train_loss = 4.075\n",
      "Epoch  12 Batch   64/2303   train_loss = 4.121\n",
      "Epoch  12 Batch   84/2303   train_loss = 3.479\n",
      "Epoch  12 Batch  104/2303   train_loss = 4.284\n",
      "Epoch  12 Batch  124/2303   train_loss = 4.445\n",
      "Epoch  12 Batch  144/2303   train_loss = 4.259\n",
      "Epoch  12 Batch  164/2303   train_loss = 4.441\n",
      "Epoch  12 Batch  184/2303   train_loss = 3.952\n",
      "Epoch  12 Batch  204/2303   train_loss = 3.294\n",
      "Epoch  12 Batch  224/2303   train_loss = 3.095\n",
      "Epoch  12 Batch  244/2303   train_loss = 4.232\n",
      "Epoch  12 Batch  264/2303   train_loss = 4.434\n",
      "Epoch  12 Batch  284/2303   train_loss = 4.059\n",
      "Epoch  12 Batch  304/2303   train_loss = 4.081\n",
      "Epoch  12 Batch  324/2303   train_loss = 3.568\n",
      "Epoch  12 Batch  344/2303   train_loss = 3.212\n",
      "Epoch  12 Batch  364/2303   train_loss = 4.161\n",
      "Epoch  12 Batch  384/2303   train_loss = 4.019\n",
      "Epoch  12 Batch  404/2303   train_loss = 4.371\n",
      "Epoch  12 Batch  424/2303   train_loss = 4.289\n",
      "Epoch  12 Batch  444/2303   train_loss = 4.228\n",
      "Epoch  12 Batch  464/2303   train_loss = 3.735\n",
      "Epoch  12 Batch  484/2303   train_loss = 4.454\n",
      "Epoch  12 Batch  504/2303   train_loss = 2.858\n",
      "Epoch  12 Batch  524/2303   train_loss = 3.723\n",
      "Epoch  12 Batch  544/2303   train_loss = 4.808\n",
      "Epoch  12 Batch  564/2303   train_loss = 4.698\n",
      "Epoch  12 Batch  584/2303   train_loss = 3.640\n",
      "Epoch  12 Batch  604/2303   train_loss = 3.566\n",
      "Epoch  12 Batch  624/2303   train_loss = 3.302\n",
      "Epoch  12 Batch  644/2303   train_loss = 3.806\n",
      "Epoch  12 Batch  664/2303   train_loss = 3.051\n",
      "Epoch  12 Batch  684/2303   train_loss = 3.460\n",
      "Epoch  12 Batch  704/2303   train_loss = 3.888\n",
      "Epoch  12 Batch  724/2303   train_loss = 4.514\n",
      "Epoch  12 Batch  744/2303   train_loss = 5.024\n",
      "Epoch  12 Batch  764/2303   train_loss = 4.732\n",
      "Epoch  12 Batch  784/2303   train_loss = 3.173\n",
      "Epoch  12 Batch  804/2303   train_loss = 4.316\n",
      "Epoch  12 Batch  824/2303   train_loss = 4.367\n",
      "Epoch  12 Batch  844/2303   train_loss = 4.316\n",
      "Epoch  12 Batch  864/2303   train_loss = 3.181\n",
      "Epoch  12 Batch  884/2303   train_loss = 4.165\n",
      "Epoch  12 Batch  904/2303   train_loss = 3.831\n",
      "Epoch  12 Batch  924/2303   train_loss = 3.299\n",
      "Epoch  12 Batch  944/2303   train_loss = 3.201\n",
      "Epoch  12 Batch  964/2303   train_loss = 3.338\n",
      "Epoch  12 Batch  984/2303   train_loss = 4.097\n",
      "Epoch  12 Batch 1004/2303   train_loss = 4.362\n",
      "Epoch  12 Batch 1024/2303   train_loss = 3.250\n",
      "Epoch  12 Batch 1044/2303   train_loss = 4.302\n",
      "Epoch  12 Batch 1064/2303   train_loss = 3.204\n",
      "Epoch  12 Batch 1084/2303   train_loss = 3.658\n",
      "Epoch  12 Batch 1104/2303   train_loss = 4.756\n",
      "Epoch  12 Batch 1124/2303   train_loss = 4.054\n",
      "Epoch  12 Batch 1144/2303   train_loss = 3.288\n",
      "Epoch  12 Batch 1164/2303   train_loss = 3.570\n",
      "Epoch  12 Batch 1184/2303   train_loss = 4.017\n",
      "Epoch  12 Batch 1204/2303   train_loss = 3.871\n",
      "Epoch  12 Batch 1224/2303   train_loss = 3.753\n",
      "Epoch  12 Batch 1244/2303   train_loss = 3.691\n",
      "Epoch  12 Batch 1264/2303   train_loss = 3.931\n",
      "Epoch  12 Batch 1284/2303   train_loss = 3.806\n",
      "Epoch  12 Batch 1304/2303   train_loss = 3.662\n",
      "Epoch  12 Batch 1324/2303   train_loss = 3.831\n",
      "Epoch  12 Batch 1344/2303   train_loss = 4.014\n",
      "Epoch  12 Batch 1364/2303   train_loss = 3.412\n",
      "Epoch  12 Batch 1384/2303   train_loss = 4.281\n",
      "Epoch  12 Batch 1404/2303   train_loss = 5.247\n",
      "Epoch  12 Batch 1424/2303   train_loss = 4.069\n",
      "Epoch  12 Batch 1444/2303   train_loss = 4.147\n",
      "Epoch  12 Batch 1464/2303   train_loss = 4.055\n",
      "Epoch  12 Batch 1484/2303   train_loss = 3.641\n",
      "Epoch  12 Batch 1504/2303   train_loss = 4.809\n",
      "Epoch  12 Batch 1524/2303   train_loss = 2.959\n",
      "Epoch  12 Batch 1544/2303   train_loss = 4.142\n",
      "Epoch  12 Batch 1564/2303   train_loss = 3.362\n",
      "Epoch  12 Batch 1584/2303   train_loss = 4.608\n",
      "Epoch  12 Batch 1604/2303   train_loss = 4.301\n",
      "Epoch  12 Batch 1624/2303   train_loss = 4.229\n",
      "Epoch  12 Batch 1644/2303   train_loss = 4.467\n",
      "Epoch  12 Batch 1664/2303   train_loss = 3.913\n",
      "Epoch  12 Batch 1684/2303   train_loss = 3.876\n",
      "Epoch  12 Batch 1704/2303   train_loss = 4.728\n",
      "Epoch  12 Batch 1724/2303   train_loss = 3.842\n",
      "Epoch  12 Batch 1744/2303   train_loss = 4.473\n",
      "Epoch  12 Batch 1764/2303   train_loss = 4.303\n",
      "Epoch  12 Batch 1784/2303   train_loss = 3.454\n",
      "Epoch  12 Batch 1804/2303   train_loss = 4.055\n",
      "Epoch  12 Batch 1824/2303   train_loss = 3.926\n",
      "Epoch  12 Batch 1844/2303   train_loss = 3.574\n",
      "Epoch  12 Batch 1864/2303   train_loss = 3.884\n",
      "Epoch  12 Batch 1884/2303   train_loss = 3.967\n",
      "Epoch  12 Batch 1904/2303   train_loss = 3.692\n",
      "Epoch  12 Batch 1924/2303   train_loss = 4.134\n",
      "Epoch  12 Batch 1944/2303   train_loss = 3.833\n",
      "Epoch  12 Batch 1964/2303   train_loss = 3.265\n",
      "Epoch  12 Batch 1984/2303   train_loss = 3.501\n",
      "Epoch  12 Batch 2004/2303   train_loss = 4.298\n",
      "Epoch  12 Batch 2024/2303   train_loss = 4.134\n",
      "Epoch  12 Batch 2044/2303   train_loss = 3.919\n",
      "Epoch  12 Batch 2064/2303   train_loss = 3.303\n",
      "Epoch  12 Batch 2084/2303   train_loss = 4.094\n",
      "Epoch  12 Batch 2104/2303   train_loss = 4.789\n",
      "Epoch  12 Batch 2124/2303   train_loss = 4.008\n",
      "Epoch  12 Batch 2144/2303   train_loss = 4.420\n",
      "Epoch  12 Batch 2164/2303   train_loss = 4.361\n",
      "Epoch  12 Batch 2184/2303   train_loss = 3.853\n",
      "Epoch  12 Batch 2204/2303   train_loss = 3.758\n",
      "Epoch  12 Batch 2224/2303   train_loss = 3.881\n",
      "Epoch  12 Batch 2244/2303   train_loss = 4.665\n",
      "Epoch  12 Batch 2264/2303   train_loss = 2.825\n",
      "Epoch  12 Batch 2284/2303   train_loss = 3.407\n",
      "Epoch  13 Batch    1/2303   train_loss = 3.790\n",
      "Epoch  13 Batch   21/2303   train_loss = 3.111\n",
      "Epoch  13 Batch   41/2303   train_loss = 3.572\n",
      "Epoch  13 Batch   61/2303   train_loss = 3.820\n",
      "Epoch  13 Batch   81/2303   train_loss = 3.303\n",
      "Epoch  13 Batch  101/2303   train_loss = 4.294\n",
      "Epoch  13 Batch  121/2303   train_loss = 4.169\n",
      "Epoch  13 Batch  141/2303   train_loss = 3.094\n",
      "Epoch  13 Batch  161/2303   train_loss = 2.808\n",
      "Epoch  13 Batch  181/2303   train_loss = 3.453\n",
      "Epoch  13 Batch  201/2303   train_loss = 4.045\n",
      "Epoch  13 Batch  221/2303   train_loss = 3.606\n",
      "Epoch  13 Batch  241/2303   train_loss = 4.196\n",
      "Epoch  13 Batch  261/2303   train_loss = 4.439\n",
      "Epoch  13 Batch  281/2303   train_loss = 3.519\n",
      "Epoch  13 Batch  301/2303   train_loss = 3.570\n",
      "Epoch  13 Batch  321/2303   train_loss = 4.435\n",
      "Epoch  13 Batch  341/2303   train_loss = 4.709\n",
      "Epoch  13 Batch  361/2303   train_loss = 3.722\n",
      "Epoch  13 Batch  381/2303   train_loss = 4.127\n",
      "Epoch  13 Batch  401/2303   train_loss = 3.870\n",
      "Epoch  13 Batch  421/2303   train_loss = 4.268\n",
      "Epoch  13 Batch  441/2303   train_loss = 2.764\n",
      "Epoch  13 Batch  461/2303   train_loss = 3.909\n",
      "Epoch  13 Batch  481/2303   train_loss = 3.833\n",
      "Epoch  13 Batch  501/2303   train_loss = 4.168\n",
      "Epoch  13 Batch  521/2303   train_loss = 4.340\n",
      "Epoch  13 Batch  541/2303   train_loss = 3.413\n",
      "Epoch  13 Batch  561/2303   train_loss = 3.707\n",
      "Epoch  13 Batch  581/2303   train_loss = 3.639\n",
      "Epoch  13 Batch  601/2303   train_loss = 4.723\n",
      "Epoch  13 Batch  621/2303   train_loss = 3.882\n",
      "Epoch  13 Batch  641/2303   train_loss = 4.086\n",
      "Epoch  13 Batch  661/2303   train_loss = 3.791\n",
      "Epoch  13 Batch  681/2303   train_loss = 3.895\n",
      "Epoch  13 Batch  701/2303   train_loss = 4.316\n",
      "Epoch  13 Batch  721/2303   train_loss = 3.484\n",
      "Epoch  13 Batch  741/2303   train_loss = 3.617\n",
      "Epoch  13 Batch  761/2303   train_loss = 3.973\n",
      "Epoch  13 Batch  781/2303   train_loss = 4.112\n",
      "Epoch  13 Batch  801/2303   train_loss = 4.144\n",
      "Epoch  13 Batch  821/2303   train_loss = 3.623\n",
      "Epoch  13 Batch  841/2303   train_loss = 4.060\n",
      "Epoch  13 Batch  861/2303   train_loss = 3.855\n",
      "Epoch  13 Batch  881/2303   train_loss = 3.374\n",
      "Epoch  13 Batch  901/2303   train_loss = 3.280\n",
      "Epoch  13 Batch  921/2303   train_loss = 3.530\n",
      "Epoch  13 Batch  941/2303   train_loss = 3.359\n",
      "Epoch  13 Batch  961/2303   train_loss = 3.781\n",
      "Epoch  13 Batch  981/2303   train_loss = 4.160\n",
      "Epoch  13 Batch 1001/2303   train_loss = 4.427\n",
      "Epoch  13 Batch 1021/2303   train_loss = 4.191\n",
      "Epoch  13 Batch 1041/2303   train_loss = 4.067\n",
      "Epoch  13 Batch 1061/2303   train_loss = 3.667\n",
      "Epoch  13 Batch 1081/2303   train_loss = 3.429\n",
      "Epoch  13 Batch 1101/2303   train_loss = 3.281\n",
      "Epoch  13 Batch 1121/2303   train_loss = 3.748\n",
      "Epoch  13 Batch 1141/2303   train_loss = 4.023\n",
      "Epoch  13 Batch 1161/2303   train_loss = 3.810\n",
      "Epoch  13 Batch 1181/2303   train_loss = 4.179\n",
      "Epoch  13 Batch 1201/2303   train_loss = 3.820\n",
      "Epoch  13 Batch 1221/2303   train_loss = 3.739\n",
      "Epoch  13 Batch 1241/2303   train_loss = 4.474\n",
      "Epoch  13 Batch 1261/2303   train_loss = 4.594\n",
      "Epoch  13 Batch 1281/2303   train_loss = 4.296\n",
      "Epoch  13 Batch 1301/2303   train_loss = 3.942\n",
      "Epoch  13 Batch 1321/2303   train_loss = 3.828\n",
      "Epoch  13 Batch 1341/2303   train_loss = 3.912\n",
      "Epoch  13 Batch 1361/2303   train_loss = 3.340\n",
      "Epoch  13 Batch 1381/2303   train_loss = 4.577\n",
      "Epoch  13 Batch 1401/2303   train_loss = 3.261\n",
      "Epoch  13 Batch 1421/2303   train_loss = 3.438\n",
      "Epoch  13 Batch 1441/2303   train_loss = 3.594\n",
      "Epoch  13 Batch 1461/2303   train_loss = 3.489\n",
      "Epoch  13 Batch 1481/2303   train_loss = 3.536\n",
      "Epoch  13 Batch 1501/2303   train_loss = 3.596\n",
      "Epoch  13 Batch 1521/2303   train_loss = 4.117\n",
      "Epoch  13 Batch 1541/2303   train_loss = 4.394\n",
      "Epoch  13 Batch 1561/2303   train_loss = 4.178\n",
      "Epoch  13 Batch 1581/2303   train_loss = 4.204\n",
      "Epoch  13 Batch 1601/2303   train_loss = 3.814\n",
      "Epoch  13 Batch 1621/2303   train_loss = 4.292\n",
      "Epoch  13 Batch 1641/2303   train_loss = 3.434\n",
      "Epoch  13 Batch 1661/2303   train_loss = 4.521\n",
      "Epoch  13 Batch 1681/2303   train_loss = 3.531\n",
      "Epoch  13 Batch 1701/2303   train_loss = 4.372\n",
      "Epoch  13 Batch 1721/2303   train_loss = 3.828\n",
      "Epoch  13 Batch 1741/2303   train_loss = 3.817\n",
      "Epoch  13 Batch 1761/2303   train_loss = 4.481\n",
      "Epoch  13 Batch 1781/2303   train_loss = 3.446\n",
      "Epoch  13 Batch 1801/2303   train_loss = 3.817\n",
      "Epoch  13 Batch 1821/2303   train_loss = 2.769\n",
      "Epoch  13 Batch 1841/2303   train_loss = 3.679\n",
      "Epoch  13 Batch 1861/2303   train_loss = 3.530\n",
      "Epoch  13 Batch 1881/2303   train_loss = 4.795\n",
      "Epoch  13 Batch 1901/2303   train_loss = 4.109\n",
      "Epoch  13 Batch 1921/2303   train_loss = 3.729\n",
      "Epoch  13 Batch 1941/2303   train_loss = 3.966\n",
      "Epoch  13 Batch 1961/2303   train_loss = 3.854\n",
      "Epoch  13 Batch 1981/2303   train_loss = 3.547\n",
      "Epoch  13 Batch 2001/2303   train_loss = 4.931\n",
      "Epoch  13 Batch 2021/2303   train_loss = 3.763\n",
      "Epoch  13 Batch 2041/2303   train_loss = 3.323\n",
      "Epoch  13 Batch 2061/2303   train_loss = 3.550\n",
      "Epoch  13 Batch 2081/2303   train_loss = 3.950\n",
      "Epoch  13 Batch 2101/2303   train_loss = 3.995\n",
      "Epoch  13 Batch 2121/2303   train_loss = 4.546\n",
      "Epoch  13 Batch 2141/2303   train_loss = 4.625\n",
      "Epoch  13 Batch 2161/2303   train_loss = 3.941\n",
      "Epoch  13 Batch 2181/2303   train_loss = 4.735\n",
      "Epoch  13 Batch 2201/2303   train_loss = 4.083\n",
      "Epoch  13 Batch 2221/2303   train_loss = 3.258\n",
      "Epoch  13 Batch 2241/2303   train_loss = 3.204\n",
      "Epoch  13 Batch 2261/2303   train_loss = 2.882\n",
      "Epoch  13 Batch 2281/2303   train_loss = 4.121\n",
      "Epoch  13 Batch 2301/2303   train_loss = 3.590\n",
      "Epoch  14 Batch   18/2303   train_loss = 3.810\n",
      "Epoch  14 Batch   38/2303   train_loss = 3.284\n",
      "Epoch  14 Batch   58/2303   train_loss = 4.040\n",
      "Epoch  14 Batch   78/2303   train_loss = 3.905\n",
      "Epoch  14 Batch   98/2303   train_loss = 3.649\n",
      "Epoch  14 Batch  118/2303   train_loss = 3.474\n",
      "Epoch  14 Batch  138/2303   train_loss = 4.069\n",
      "Epoch  14 Batch  158/2303   train_loss = 3.458\n",
      "Epoch  14 Batch  178/2303   train_loss = 3.374\n",
      "Epoch  14 Batch  198/2303   train_loss = 3.648\n",
      "Epoch  14 Batch  218/2303   train_loss = 3.623\n",
      "Epoch  14 Batch  238/2303   train_loss = 3.524\n",
      "Epoch  14 Batch  258/2303   train_loss = 4.255\n",
      "Epoch  14 Batch  278/2303   train_loss = 3.463\n",
      "Epoch  14 Batch  298/2303   train_loss = 3.650\n",
      "Epoch  14 Batch  318/2303   train_loss = 4.232\n",
      "Epoch  14 Batch  338/2303   train_loss = 4.335\n",
      "Epoch  14 Batch  358/2303   train_loss = 3.884\n",
      "Epoch  14 Batch  378/2303   train_loss = 3.986\n",
      "Epoch  14 Batch  398/2303   train_loss = 4.124\n",
      "Epoch  14 Batch  418/2303   train_loss = 4.349\n",
      "Epoch  14 Batch  438/2303   train_loss = 4.252\n",
      "Epoch  14 Batch  458/2303   train_loss = 4.185\n",
      "Epoch  14 Batch  478/2303   train_loss = 3.797\n",
      "Epoch  14 Batch  498/2303   train_loss = 3.702\n",
      "Epoch  14 Batch  518/2303   train_loss = 4.706\n",
      "Epoch  14 Batch  538/2303   train_loss = 3.782\n",
      "Epoch  14 Batch  558/2303   train_loss = 4.484\n",
      "Epoch  14 Batch  578/2303   train_loss = 3.556\n",
      "Epoch  14 Batch  598/2303   train_loss = 3.982\n",
      "Epoch  14 Batch  618/2303   train_loss = 4.166\n",
      "Epoch  14 Batch  638/2303   train_loss = 2.899\n",
      "Epoch  14 Batch  658/2303   train_loss = 4.045\n",
      "Epoch  14 Batch  678/2303   train_loss = 3.543\n",
      "Epoch  14 Batch  698/2303   train_loss = 3.755\n",
      "Epoch  14 Batch  718/2303   train_loss = 3.973\n",
      "Epoch  14 Batch  738/2303   train_loss = 3.960\n",
      "Epoch  14 Batch  758/2303   train_loss = 3.826\n",
      "Epoch  14 Batch  778/2303   train_loss = 5.417\n",
      "Epoch  14 Batch  798/2303   train_loss = 3.302\n",
      "Epoch  14 Batch  818/2303   train_loss = 4.064\n",
      "Epoch  14 Batch  838/2303   train_loss = 3.514\n",
      "Epoch  14 Batch  858/2303   train_loss = 3.529\n",
      "Epoch  14 Batch  878/2303   train_loss = 2.911\n",
      "Epoch  14 Batch  898/2303   train_loss = 2.810\n",
      "Epoch  14 Batch  918/2303   train_loss = 3.920\n",
      "Epoch  14 Batch  938/2303   train_loss = 3.391\n",
      "Epoch  14 Batch  958/2303   train_loss = 3.631\n",
      "Epoch  14 Batch  978/2303   train_loss = 4.336\n",
      "Epoch  14 Batch  998/2303   train_loss = 4.230\n",
      "Epoch  14 Batch 1018/2303   train_loss = 4.181\n",
      "Epoch  14 Batch 1038/2303   train_loss = 3.417\n",
      "Epoch  14 Batch 1058/2303   train_loss = 4.085\n",
      "Epoch  14 Batch 1078/2303   train_loss = 4.345\n",
      "Epoch  14 Batch 1098/2303   train_loss = 3.579\n",
      "Epoch  14 Batch 1118/2303   train_loss = 3.491\n",
      "Epoch  14 Batch 1138/2303   train_loss = 4.432\n",
      "Epoch  14 Batch 1158/2303   train_loss = 3.886\n",
      "Epoch  14 Batch 1178/2303   train_loss = 3.818\n",
      "Epoch  14 Batch 1198/2303   train_loss = 3.366\n",
      "Epoch  14 Batch 1218/2303   train_loss = 3.568\n",
      "Epoch  14 Batch 1238/2303   train_loss = 3.858\n",
      "Epoch  14 Batch 1258/2303   train_loss = 4.111\n",
      "Epoch  14 Batch 1278/2303   train_loss = 3.566\n",
      "Epoch  14 Batch 1298/2303   train_loss = 4.572\n",
      "Epoch  14 Batch 1318/2303   train_loss = 4.300\n",
      "Epoch  14 Batch 1338/2303   train_loss = 3.175\n",
      "Epoch  14 Batch 1358/2303   train_loss = 3.680\n",
      "Epoch  14 Batch 1378/2303   train_loss = 3.842\n",
      "Epoch  14 Batch 1398/2303   train_loss = 3.551\n",
      "Epoch  14 Batch 1418/2303   train_loss = 3.761\n",
      "Epoch  14 Batch 1438/2303   train_loss = 3.866\n",
      "Epoch  14 Batch 1458/2303   train_loss = 3.736\n",
      "Epoch  14 Batch 1478/2303   train_loss = 4.307\n",
      "Epoch  14 Batch 1498/2303   train_loss = 3.256\n",
      "Epoch  14 Batch 1518/2303   train_loss = 3.915\n",
      "Epoch  14 Batch 1538/2303   train_loss = 4.438\n",
      "Epoch  14 Batch 1558/2303   train_loss = 4.021\n",
      "Epoch  14 Batch 1578/2303   train_loss = 3.701\n",
      "Epoch  14 Batch 1598/2303   train_loss = 3.778\n",
      "Epoch  14 Batch 1618/2303   train_loss = 3.405\n",
      "Epoch  14 Batch 1638/2303   train_loss = 3.764\n",
      "Epoch  14 Batch 1658/2303   train_loss = 3.406\n",
      "Epoch  14 Batch 1678/2303   train_loss = 4.328\n",
      "Epoch  14 Batch 1698/2303   train_loss = 4.250\n",
      "Epoch  14 Batch 1718/2303   train_loss = 3.649\n",
      "Epoch  14 Batch 1738/2303   train_loss = 3.598\n",
      "Epoch  14 Batch 1758/2303   train_loss = 3.950\n",
      "Epoch  14 Batch 1778/2303   train_loss = 3.619\n",
      "Epoch  14 Batch 1798/2303   train_loss = 3.797\n",
      "Epoch  14 Batch 1818/2303   train_loss = 4.401\n",
      "Epoch  14 Batch 1838/2303   train_loss = 3.834\n",
      "Epoch  14 Batch 1858/2303   train_loss = 4.545\n",
      "Epoch  14 Batch 1878/2303   train_loss = 3.585\n",
      "Epoch  14 Batch 1898/2303   train_loss = 3.951\n",
      "Epoch  14 Batch 1918/2303   train_loss = 3.890\n",
      "Epoch  14 Batch 1938/2303   train_loss = 4.450\n",
      "Epoch  14 Batch 1958/2303   train_loss = 4.493\n",
      "Epoch  14 Batch 1978/2303   train_loss = 3.854\n",
      "Epoch  14 Batch 1998/2303   train_loss = 4.039\n",
      "Epoch  14 Batch 2018/2303   train_loss = 3.498\n",
      "Epoch  14 Batch 2038/2303   train_loss = 4.529\n",
      "Epoch  14 Batch 2058/2303   train_loss = 3.802\n",
      "Epoch  14 Batch 2078/2303   train_loss = 3.776\n",
      "Epoch  14 Batch 2098/2303   train_loss = 3.383\n",
      "Epoch  14 Batch 2118/2303   train_loss = 4.585\n",
      "Epoch  14 Batch 2138/2303   train_loss = 3.582\n",
      "Epoch  14 Batch 2158/2303   train_loss = 4.200\n",
      "Epoch  14 Batch 2178/2303   train_loss = 3.893\n",
      "Epoch  14 Batch 2198/2303   train_loss = 3.978\n",
      "Epoch  14 Batch 2218/2303   train_loss = 3.563\n",
      "Epoch  14 Batch 2238/2303   train_loss = 3.675\n",
      "Epoch  14 Batch 2258/2303   train_loss = 3.680\n",
      "Epoch  14 Batch 2278/2303   train_loss = 3.883\n",
      "Epoch  14 Batch 2298/2303   train_loss = 3.465\n",
      "Epoch  15 Batch   15/2303   train_loss = 3.303\n",
      "Epoch  15 Batch   35/2303   train_loss = 3.267\n",
      "Epoch  15 Batch   55/2303   train_loss = 3.999\n",
      "Epoch  15 Batch   75/2303   train_loss = 4.384\n",
      "Epoch  15 Batch   95/2303   train_loss = 3.741\n",
      "Epoch  15 Batch  115/2303   train_loss = 3.492\n",
      "Epoch  15 Batch  135/2303   train_loss = 3.867\n",
      "Epoch  15 Batch  155/2303   train_loss = 3.754\n",
      "Epoch  15 Batch  175/2303   train_loss = 4.565\n",
      "Epoch  15 Batch  195/2303   train_loss = 3.344\n",
      "Epoch  15 Batch  215/2303   train_loss = 2.651\n",
      "Epoch  15 Batch  235/2303   train_loss = 3.253\n",
      "Epoch  15 Batch  255/2303   train_loss = 4.408\n",
      "Epoch  15 Batch  275/2303   train_loss = 3.572\n",
      "Epoch  15 Batch  295/2303   train_loss = 4.119\n",
      "Epoch  15 Batch  315/2303   train_loss = 4.284\n",
      "Epoch  15 Batch  335/2303   train_loss = 5.094\n",
      "Epoch  15 Batch  355/2303   train_loss = 3.742\n",
      "Epoch  15 Batch  375/2303   train_loss = 4.012\n",
      "Epoch  15 Batch  395/2303   train_loss = 3.045\n",
      "Epoch  15 Batch  415/2303   train_loss = 4.229\n",
      "Epoch  15 Batch  435/2303   train_loss = 3.796\n",
      "Epoch  15 Batch  455/2303   train_loss = 4.320\n",
      "Epoch  15 Batch  475/2303   train_loss = 3.920\n",
      "Epoch  15 Batch  495/2303   train_loss = 3.280\n",
      "Epoch  15 Batch  515/2303   train_loss = 4.204\n",
      "Epoch  15 Batch  535/2303   train_loss = 4.874\n",
      "Epoch  15 Batch  555/2303   train_loss = 4.298\n",
      "Epoch  15 Batch  575/2303   train_loss = 4.093\n",
      "Epoch  15 Batch  595/2303   train_loss = 3.375\n",
      "Epoch  15 Batch  615/2303   train_loss = 3.305\n",
      "Epoch  15 Batch  635/2303   train_loss = 3.998\n",
      "Epoch  15 Batch  655/2303   train_loss = 3.268\n",
      "Epoch  15 Batch  675/2303   train_loss = 3.492\n",
      "Epoch  15 Batch  695/2303   train_loss = 3.974\n",
      "Epoch  15 Batch  715/2303   train_loss = 3.632\n",
      "Epoch  15 Batch  735/2303   train_loss = 3.483\n",
      "Epoch  15 Batch  755/2303   train_loss = 3.819\n",
      "Epoch  15 Batch  775/2303   train_loss = 3.727\n",
      "Epoch  15 Batch  795/2303   train_loss = 4.083\n",
      "Epoch  15 Batch  815/2303   train_loss = 4.005\n",
      "Epoch  15 Batch  835/2303   train_loss = 4.017\n",
      "Epoch  15 Batch  855/2303   train_loss = 3.750\n",
      "Epoch  15 Batch  875/2303   train_loss = 4.405\n",
      "Epoch  15 Batch  895/2303   train_loss = 4.624\n",
      "Epoch  15 Batch  915/2303   train_loss = 3.200\n",
      "Epoch  15 Batch  935/2303   train_loss = 3.665\n",
      "Epoch  15 Batch  955/2303   train_loss = 3.645\n",
      "Epoch  15 Batch  975/2303   train_loss = 4.490\n",
      "Epoch  15 Batch  995/2303   train_loss = 3.445\n",
      "Epoch  15 Batch 1015/2303   train_loss = 3.434\n",
      "Epoch  15 Batch 1035/2303   train_loss = 4.935\n",
      "Epoch  15 Batch 1055/2303   train_loss = 4.696\n",
      "Epoch  15 Batch 1075/2303   train_loss = 4.131\n",
      "Epoch  15 Batch 1095/2303   train_loss = 3.496\n",
      "Epoch  15 Batch 1115/2303   train_loss = 4.545\n",
      "Epoch  15 Batch 1135/2303   train_loss = 3.690\n",
      "Epoch  15 Batch 1155/2303   train_loss = 4.934\n",
      "Epoch  15 Batch 1175/2303   train_loss = 4.219\n",
      "Epoch  15 Batch 1195/2303   train_loss = 3.881\n",
      "Epoch  15 Batch 1215/2303   train_loss = 4.399\n",
      "Epoch  15 Batch 1235/2303   train_loss = 3.844\n",
      "Epoch  15 Batch 1255/2303   train_loss = 3.805\n",
      "Epoch  15 Batch 1275/2303   train_loss = 2.796\n",
      "Epoch  15 Batch 1295/2303   train_loss = 4.236\n",
      "Epoch  15 Batch 1315/2303   train_loss = 3.877\n",
      "Epoch  15 Batch 1335/2303   train_loss = 4.484\n",
      "Epoch  15 Batch 1355/2303   train_loss = 3.990\n",
      "Epoch  15 Batch 1375/2303   train_loss = 3.411\n",
      "Epoch  15 Batch 1395/2303   train_loss = 3.246\n",
      "Epoch  15 Batch 1415/2303   train_loss = 3.384\n",
      "Epoch  15 Batch 1435/2303   train_loss = 3.331\n",
      "Epoch  15 Batch 1455/2303   train_loss = 3.779\n",
      "Epoch  15 Batch 1475/2303   train_loss = 4.004\n",
      "Epoch  15 Batch 1495/2303   train_loss = 3.473\n",
      "Epoch  15 Batch 1515/2303   train_loss = 4.103\n",
      "Epoch  15 Batch 1535/2303   train_loss = 3.681\n",
      "Epoch  15 Batch 1555/2303   train_loss = 3.884\n",
      "Epoch  15 Batch 1575/2303   train_loss = 4.459\n",
      "Epoch  15 Batch 1595/2303   train_loss = 4.242\n",
      "Epoch  15 Batch 1615/2303   train_loss = 3.959\n",
      "Epoch  15 Batch 1635/2303   train_loss = 3.782\n",
      "Epoch  15 Batch 1655/2303   train_loss = 3.809\n",
      "Epoch  15 Batch 1675/2303   train_loss = 3.745\n",
      "Epoch  15 Batch 1695/2303   train_loss = 3.955\n",
      "Epoch  15 Batch 1715/2303   train_loss = 4.234\n",
      "Epoch  15 Batch 1735/2303   train_loss = 4.968\n",
      "Epoch  15 Batch 1755/2303   train_loss = 3.652\n",
      "Epoch  15 Batch 1775/2303   train_loss = 3.578\n",
      "Epoch  15 Batch 1795/2303   train_loss = 4.177\n",
      "Epoch  15 Batch 1815/2303   train_loss = 3.875\n",
      "Epoch  15 Batch 1835/2303   train_loss = 4.076\n",
      "Epoch  15 Batch 1855/2303   train_loss = 4.087\n",
      "Epoch  15 Batch 1875/2303   train_loss = 4.356\n",
      "Epoch  15 Batch 1895/2303   train_loss = 3.804\n",
      "Epoch  15 Batch 1915/2303   train_loss = 4.516\n",
      "Epoch  15 Batch 1935/2303   train_loss = 3.721\n",
      "Epoch  15 Batch 1955/2303   train_loss = 3.811\n",
      "Epoch  15 Batch 1975/2303   train_loss = 4.249\n",
      "Epoch  15 Batch 1995/2303   train_loss = 3.385\n",
      "Epoch  15 Batch 2015/2303   train_loss = 3.989\n",
      "Epoch  15 Batch 2035/2303   train_loss = 3.624\n",
      "Epoch  15 Batch 2055/2303   train_loss = 3.617\n",
      "Epoch  15 Batch 2075/2303   train_loss = 4.314\n",
      "Epoch  15 Batch 2095/2303   train_loss = 3.203\n",
      "Epoch  15 Batch 2115/2303   train_loss = 4.102\n",
      "Epoch  15 Batch 2135/2303   train_loss = 3.618\n",
      "Epoch  15 Batch 2155/2303   train_loss = 3.407\n",
      "Epoch  15 Batch 2175/2303   train_loss = 3.219\n",
      "Epoch  15 Batch 2195/2303   train_loss = 4.200\n",
      "Epoch  15 Batch 2215/2303   train_loss = 4.654\n",
      "Epoch  15 Batch 2235/2303   train_loss = 3.478\n",
      "Epoch  15 Batch 2255/2303   train_loss = 4.024\n",
      "Epoch  15 Batch 2275/2303   train_loss = 4.456\n",
      "Epoch  15 Batch 2295/2303   train_loss = 3.806\n",
      "Epoch  16 Batch   12/2303   train_loss = 4.283\n",
      "Epoch  16 Batch   32/2303   train_loss = 2.325\n",
      "Epoch  16 Batch   52/2303   train_loss = 4.208\n",
      "Epoch  16 Batch   72/2303   train_loss = 3.176\n",
      "Epoch  16 Batch   92/2303   train_loss = 3.979\n",
      "Epoch  16 Batch  112/2303   train_loss = 3.007\n",
      "Epoch  16 Batch  132/2303   train_loss = 4.016\n",
      "Epoch  16 Batch  152/2303   train_loss = 3.764\n",
      "Epoch  16 Batch  172/2303   train_loss = 3.448\n",
      "Epoch  16 Batch  192/2303   train_loss = 3.291\n",
      "Epoch  16 Batch  212/2303   train_loss = 2.264\n",
      "Epoch  16 Batch  232/2303   train_loss = 4.062\n",
      "Epoch  16 Batch  252/2303   train_loss = 4.206\n",
      "Epoch  16 Batch  272/2303   train_loss = 3.315\n",
      "Epoch  16 Batch  292/2303   train_loss = 4.445\n",
      "Epoch  16 Batch  312/2303   train_loss = 3.043\n",
      "Epoch  16 Batch  332/2303   train_loss = 4.379\n",
      "Epoch  16 Batch  352/2303   train_loss = 3.839\n",
      "Epoch  16 Batch  372/2303   train_loss = 4.259\n",
      "Epoch  16 Batch  392/2303   train_loss = 3.476\n",
      "Epoch  16 Batch  412/2303   train_loss = 3.777\n",
      "Epoch  16 Batch  432/2303   train_loss = 4.164\n",
      "Epoch  16 Batch  452/2303   train_loss = 4.801\n",
      "Epoch  16 Batch  472/2303   train_loss = 4.723\n",
      "Epoch  16 Batch  492/2303   train_loss = 4.139\n",
      "Epoch  16 Batch  512/2303   train_loss = 4.127\n",
      "Epoch  16 Batch  532/2303   train_loss = 3.731\n",
      "Epoch  16 Batch  552/2303   train_loss = 4.217\n",
      "Epoch  16 Batch  572/2303   train_loss = 3.968\n",
      "Epoch  16 Batch  592/2303   train_loss = 3.800\n",
      "Epoch  16 Batch  612/2303   train_loss = 3.480\n",
      "Epoch  16 Batch  632/2303   train_loss = 3.909\n",
      "Epoch  16 Batch  652/2303   train_loss = 4.368\n",
      "Epoch  16 Batch  672/2303   train_loss = 3.759\n",
      "Epoch  16 Batch  692/2303   train_loss = 3.995\n",
      "Epoch  16 Batch  712/2303   train_loss = 4.480\n",
      "Epoch  16 Batch  732/2303   train_loss = 3.519\n",
      "Epoch  16 Batch  752/2303   train_loss = 4.420\n",
      "Epoch  16 Batch  772/2303   train_loss = 3.213\n",
      "Epoch  16 Batch  792/2303   train_loss = 4.586\n",
      "Epoch  16 Batch  812/2303   train_loss = 4.744\n",
      "Epoch  16 Batch  832/2303   train_loss = 3.884\n",
      "Epoch  16 Batch  852/2303   train_loss = 3.622\n",
      "Epoch  16 Batch  872/2303   train_loss = 3.823\n",
      "Epoch  16 Batch  892/2303   train_loss = 3.541\n",
      "Epoch  16 Batch  912/2303   train_loss = 4.141\n",
      "Epoch  16 Batch  932/2303   train_loss = 4.309\n",
      "Epoch  16 Batch  952/2303   train_loss = 4.266\n",
      "Epoch  16 Batch  972/2303   train_loss = 3.498\n",
      "Epoch  16 Batch  992/2303   train_loss = 4.856\n",
      "Epoch  16 Batch 1012/2303   train_loss = 3.772\n",
      "Epoch  16 Batch 1032/2303   train_loss = 5.325\n",
      "Epoch  16 Batch 1052/2303   train_loss = 3.783\n",
      "Epoch  16 Batch 1072/2303   train_loss = 3.702\n",
      "Epoch  16 Batch 1092/2303   train_loss = 3.970\n",
      "Epoch  16 Batch 1112/2303   train_loss = 3.771\n",
      "Epoch  16 Batch 1132/2303   train_loss = 4.492\n",
      "Epoch  16 Batch 1152/2303   train_loss = 4.056\n",
      "Epoch  16 Batch 1172/2303   train_loss = 4.777\n",
      "Epoch  16 Batch 1192/2303   train_loss = 3.878\n",
      "Epoch  16 Batch 1212/2303   train_loss = 3.842\n",
      "Epoch  16 Batch 1232/2303   train_loss = 3.831\n",
      "Epoch  16 Batch 1252/2303   train_loss = 4.157\n",
      "Epoch  16 Batch 1272/2303   train_loss = 3.245\n",
      "Epoch  16 Batch 1292/2303   train_loss = 4.881\n",
      "Epoch  16 Batch 1312/2303   train_loss = 3.107\n",
      "Epoch  16 Batch 1332/2303   train_loss = 3.668\n",
      "Epoch  16 Batch 1352/2303   train_loss = 3.745\n",
      "Epoch  16 Batch 1372/2303   train_loss = 3.901\n",
      "Epoch  16 Batch 1392/2303   train_loss = 3.889\n",
      "Epoch  16 Batch 1412/2303   train_loss = 4.088\n",
      "Epoch  16 Batch 1432/2303   train_loss = 4.382\n",
      "Epoch  16 Batch 1452/2303   train_loss = 3.617\n",
      "Epoch  16 Batch 1472/2303   train_loss = 4.580\n",
      "Epoch  16 Batch 1492/2303   train_loss = 3.660\n",
      "Epoch  16 Batch 1512/2303   train_loss = 3.665\n",
      "Epoch  16 Batch 1532/2303   train_loss = 3.475\n",
      "Epoch  16 Batch 1552/2303   train_loss = 4.420\n",
      "Epoch  16 Batch 1572/2303   train_loss = 3.529\n",
      "Epoch  16 Batch 1592/2303   train_loss = 3.765\n",
      "Epoch  16 Batch 1612/2303   train_loss = 4.134\n",
      "Epoch  16 Batch 1632/2303   train_loss = 3.509\n",
      "Epoch  16 Batch 1652/2303   train_loss = 2.714\n",
      "Epoch  16 Batch 1672/2303   train_loss = 3.693\n",
      "Epoch  16 Batch 1692/2303   train_loss = 4.733\n",
      "Epoch  16 Batch 1712/2303   train_loss = 4.660\n",
      "Epoch  16 Batch 1732/2303   train_loss = 4.407\n",
      "Epoch  16 Batch 1752/2303   train_loss = 3.841\n",
      "Epoch  16 Batch 1772/2303   train_loss = 3.609\n",
      "Epoch  16 Batch 1792/2303   train_loss = 5.183\n",
      "Epoch  16 Batch 1812/2303   train_loss = 3.901\n",
      "Epoch  16 Batch 1832/2303   train_loss = 4.580\n",
      "Epoch  16 Batch 1852/2303   train_loss = 3.821\n",
      "Epoch  16 Batch 1872/2303   train_loss = 3.526\n",
      "Epoch  16 Batch 1892/2303   train_loss = 4.243\n",
      "Epoch  16 Batch 1912/2303   train_loss = 4.613\n",
      "Epoch  16 Batch 1932/2303   train_loss = 4.409\n",
      "Epoch  16 Batch 1952/2303   train_loss = 3.672\n",
      "Epoch  16 Batch 1972/2303   train_loss = 4.078\n",
      "Epoch  16 Batch 1992/2303   train_loss = 3.631\n",
      "Epoch  16 Batch 2012/2303   train_loss = 4.081\n",
      "Epoch  16 Batch 2032/2303   train_loss = 3.980\n",
      "Epoch  16 Batch 2052/2303   train_loss = 3.599\n",
      "Epoch  16 Batch 2072/2303   train_loss = 3.821\n",
      "Epoch  16 Batch 2092/2303   train_loss = 3.739\n",
      "Epoch  16 Batch 2112/2303   train_loss = 3.178\n",
      "Epoch  16 Batch 2132/2303   train_loss = 3.727\n",
      "Epoch  16 Batch 2152/2303   train_loss = 3.722\n",
      "Epoch  16 Batch 2172/2303   train_loss = 4.371\n",
      "Epoch  16 Batch 2192/2303   train_loss = 4.118\n",
      "Epoch  16 Batch 2212/2303   train_loss = 4.464\n",
      "Epoch  16 Batch 2232/2303   train_loss = 4.025\n",
      "Epoch  16 Batch 2252/2303   train_loss = 3.549\n",
      "Epoch  16 Batch 2272/2303   train_loss = 4.070\n",
      "Epoch  16 Batch 2292/2303   train_loss = 3.771\n",
      "Epoch  17 Batch    9/2303   train_loss = 3.661\n",
      "Epoch  17 Batch   29/2303   train_loss = 3.953\n",
      "Epoch  17 Batch   49/2303   train_loss = 3.345\n",
      "Epoch  17 Batch   69/2303   train_loss = 3.198\n",
      "Epoch  17 Batch   89/2303   train_loss = 4.154\n",
      "Epoch  17 Batch  109/2303   train_loss = 3.044\n",
      "Epoch  17 Batch  129/2303   train_loss = 5.148\n",
      "Epoch  17 Batch  149/2303   train_loss = 3.161\n",
      "Epoch  17 Batch  169/2303   train_loss = 3.361\n",
      "Epoch  17 Batch  189/2303   train_loss = 4.244\n",
      "Epoch  17 Batch  209/2303   train_loss = 3.867\n",
      "Epoch  17 Batch  229/2303   train_loss = 5.231\n",
      "Epoch  17 Batch  249/2303   train_loss = 3.777\n",
      "Epoch  17 Batch  269/2303   train_loss = 3.934\n",
      "Epoch  17 Batch  289/2303   train_loss = 3.920\n",
      "Epoch  17 Batch  309/2303   train_loss = 4.696\n",
      "Epoch  17 Batch  329/2303   train_loss = 3.277\n",
      "Epoch  17 Batch  349/2303   train_loss = 3.944\n",
      "Epoch  17 Batch  369/2303   train_loss = 4.440\n",
      "Epoch  17 Batch  389/2303   train_loss = 3.959\n",
      "Epoch  17 Batch  409/2303   train_loss = 3.291\n",
      "Epoch  17 Batch  429/2303   train_loss = 3.836\n",
      "Epoch  17 Batch  449/2303   train_loss = 4.047\n",
      "Epoch  17 Batch  469/2303   train_loss = 4.372\n",
      "Epoch  17 Batch  489/2303   train_loss = 3.774\n",
      "Epoch  17 Batch  509/2303   train_loss = 3.746\n",
      "Epoch  17 Batch  529/2303   train_loss = 3.780\n",
      "Epoch  17 Batch  549/2303   train_loss = 4.230\n",
      "Epoch  17 Batch  569/2303   train_loss = 3.307\n",
      "Epoch  17 Batch  589/2303   train_loss = 3.732\n",
      "Epoch  17 Batch  609/2303   train_loss = 4.002\n",
      "Epoch  17 Batch  629/2303   train_loss = 3.234\n",
      "Epoch  17 Batch  649/2303   train_loss = 4.727\n",
      "Epoch  17 Batch  669/2303   train_loss = 3.797\n",
      "Epoch  17 Batch  689/2303   train_loss = 3.953\n",
      "Epoch  17 Batch  709/2303   train_loss = 3.284\n",
      "Epoch  17 Batch  729/2303   train_loss = 2.762\n",
      "Epoch  17 Batch  749/2303   train_loss = 4.119\n",
      "Epoch  17 Batch  769/2303   train_loss = 3.967\n",
      "Epoch  17 Batch  789/2303   train_loss = 3.613\n",
      "Epoch  17 Batch  809/2303   train_loss = 3.872\n",
      "Epoch  17 Batch  829/2303   train_loss = 3.632\n",
      "Epoch  17 Batch  849/2303   train_loss = 3.537\n",
      "Epoch  17 Batch  869/2303   train_loss = 3.387\n",
      "Epoch  17 Batch  889/2303   train_loss = 3.802\n",
      "Epoch  17 Batch  909/2303   train_loss = 4.038\n",
      "Epoch  17 Batch  929/2303   train_loss = 4.115\n",
      "Epoch  17 Batch  949/2303   train_loss = 3.401\n",
      "Epoch  17 Batch  969/2303   train_loss = 3.799\n",
      "Epoch  17 Batch  989/2303   train_loss = 4.381\n",
      "Epoch  17 Batch 1009/2303   train_loss = 4.054\n",
      "Epoch  17 Batch 1029/2303   train_loss = 3.363\n",
      "Epoch  17 Batch 1049/2303   train_loss = 3.496\n",
      "Epoch  17 Batch 1069/2303   train_loss = 3.667\n",
      "Epoch  17 Batch 1089/2303   train_loss = 3.964\n",
      "Epoch  17 Batch 1109/2303   train_loss = 2.028\n",
      "Epoch  17 Batch 1129/2303   train_loss = 4.109\n",
      "Epoch  17 Batch 1149/2303   train_loss = 3.753\n",
      "Epoch  17 Batch 1169/2303   train_loss = 4.051\n",
      "Epoch  17 Batch 1189/2303   train_loss = 3.552\n",
      "Epoch  17 Batch 1209/2303   train_loss = 4.050\n",
      "Epoch  17 Batch 1229/2303   train_loss = 3.513\n",
      "Epoch  17 Batch 1249/2303   train_loss = 4.365\n",
      "Epoch  17 Batch 1269/2303   train_loss = 2.995\n",
      "Epoch  17 Batch 1289/2303   train_loss = 3.725\n",
      "Epoch  17 Batch 1309/2303   train_loss = 4.016\n",
      "Epoch  17 Batch 1329/2303   train_loss = 4.399\n",
      "Epoch  17 Batch 1349/2303   train_loss = 4.674\n",
      "Epoch  17 Batch 1369/2303   train_loss = 3.310\n",
      "Epoch  17 Batch 1389/2303   train_loss = 3.471\n",
      "Epoch  17 Batch 1409/2303   train_loss = 4.134\n",
      "Epoch  17 Batch 1429/2303   train_loss = 4.088\n",
      "Epoch  17 Batch 1449/2303   train_loss = 3.462\n",
      "Epoch  17 Batch 1469/2303   train_loss = 3.675\n",
      "Epoch  17 Batch 1489/2303   train_loss = 3.734\n",
      "Epoch  17 Batch 1509/2303   train_loss = 4.542\n",
      "Epoch  17 Batch 1529/2303   train_loss = 4.107\n",
      "Epoch  17 Batch 1549/2303   train_loss = 4.785\n",
      "Epoch  17 Batch 1569/2303   train_loss = 3.638\n",
      "Epoch  17 Batch 1589/2303   train_loss = 4.333\n",
      "Epoch  17 Batch 1609/2303   train_loss = 3.584\n",
      "Epoch  17 Batch 1629/2303   train_loss = 4.183\n",
      "Epoch  17 Batch 1649/2303   train_loss = 4.583\n",
      "Epoch  17 Batch 1669/2303   train_loss = 3.882\n",
      "Epoch  17 Batch 1689/2303   train_loss = 3.613\n",
      "Epoch  17 Batch 1709/2303   train_loss = 3.357\n",
      "Epoch  17 Batch 1729/2303   train_loss = 4.877\n",
      "Epoch  17 Batch 1749/2303   train_loss = 4.336\n",
      "Epoch  17 Batch 1769/2303   train_loss = 3.804\n",
      "Epoch  17 Batch 1789/2303   train_loss = 4.280\n",
      "Epoch  17 Batch 1809/2303   train_loss = 3.984\n",
      "Epoch  17 Batch 1829/2303   train_loss = 3.300\n",
      "Epoch  17 Batch 1849/2303   train_loss = 4.053\n",
      "Epoch  17 Batch 1869/2303   train_loss = 3.842\n",
      "Epoch  17 Batch 1889/2303   train_loss = 4.428\n",
      "Epoch  17 Batch 1909/2303   train_loss = 3.794\n",
      "Epoch  17 Batch 1929/2303   train_loss = 3.656\n",
      "Epoch  17 Batch 1949/2303   train_loss = 4.446\n",
      "Epoch  17 Batch 1969/2303   train_loss = 4.004\n",
      "Epoch  17 Batch 1989/2303   train_loss = 3.997\n",
      "Epoch  17 Batch 2009/2303   train_loss = 3.617\n",
      "Epoch  17 Batch 2029/2303   train_loss = 4.365\n",
      "Epoch  17 Batch 2049/2303   train_loss = 3.078\n",
      "Epoch  17 Batch 2069/2303   train_loss = 3.120\n",
      "Epoch  17 Batch 2089/2303   train_loss = 3.641\n",
      "Epoch  17 Batch 2109/2303   train_loss = 3.855\n",
      "Epoch  17 Batch 2129/2303   train_loss = 3.964\n",
      "Epoch  17 Batch 2149/2303   train_loss = 3.792\n",
      "Epoch  17 Batch 2169/2303   train_loss = 4.975\n",
      "Epoch  17 Batch 2189/2303   train_loss = 4.960\n",
      "Epoch  17 Batch 2209/2303   train_loss = 4.803\n",
      "Epoch  17 Batch 2229/2303   train_loss = 3.942\n",
      "Epoch  17 Batch 2249/2303   train_loss = 3.579\n",
      "Epoch  17 Batch 2269/2303   train_loss = 3.209\n",
      "Epoch  17 Batch 2289/2303   train_loss = 3.603\n",
      "Epoch  18 Batch    6/2303   train_loss = 3.885\n",
      "Epoch  18 Batch   26/2303   train_loss = 3.585\n",
      "Epoch  18 Batch   46/2303   train_loss = 3.395\n",
      "Epoch  18 Batch   66/2303   train_loss = 3.442\n",
      "Epoch  18 Batch   86/2303   train_loss = 4.098\n",
      "Epoch  18 Batch  106/2303   train_loss = 3.525\n",
      "Epoch  18 Batch  126/2303   train_loss = 4.854\n",
      "Epoch  18 Batch  146/2303   train_loss = 4.435\n",
      "Epoch  18 Batch  166/2303   train_loss = 4.037\n",
      "Epoch  18 Batch  186/2303   train_loss = 3.711\n",
      "Epoch  18 Batch  206/2303   train_loss = 4.052\n",
      "Epoch  18 Batch  226/2303   train_loss = 3.890\n",
      "Epoch  18 Batch  246/2303   train_loss = 3.970\n",
      "Epoch  18 Batch  266/2303   train_loss = 3.559\n",
      "Epoch  18 Batch  286/2303   train_loss = 4.124\n",
      "Epoch  18 Batch  306/2303   train_loss = 4.064\n",
      "Epoch  18 Batch  326/2303   train_loss = 3.277\n",
      "Epoch  18 Batch  346/2303   train_loss = 4.529\n",
      "Epoch  18 Batch  366/2303   train_loss = 4.327\n",
      "Epoch  18 Batch  386/2303   train_loss = 4.257\n",
      "Epoch  18 Batch  406/2303   train_loss = 3.381\n",
      "Epoch  18 Batch  426/2303   train_loss = 3.383\n",
      "Epoch  18 Batch  446/2303   train_loss = 4.090\n",
      "Epoch  18 Batch  466/2303   train_loss = 4.490\n",
      "Epoch  18 Batch  486/2303   train_loss = 4.021\n",
      "Epoch  18 Batch  506/2303   train_loss = 3.990\n",
      "Epoch  18 Batch  526/2303   train_loss = 2.436\n",
      "Epoch  18 Batch  546/2303   train_loss = 4.608\n",
      "Epoch  18 Batch  566/2303   train_loss = 3.402\n",
      "Epoch  18 Batch  586/2303   train_loss = 3.854\n",
      "Epoch  18 Batch  606/2303   train_loss = 4.208\n",
      "Epoch  18 Batch  626/2303   train_loss = 3.767\n",
      "Epoch  18 Batch  646/2303   train_loss = 3.666\n",
      "Epoch  18 Batch  666/2303   train_loss = 4.262\n",
      "Epoch  18 Batch  686/2303   train_loss = 4.032\n",
      "Epoch  18 Batch  706/2303   train_loss = 4.328\n",
      "Epoch  18 Batch  726/2303   train_loss = 4.081\n",
      "Epoch  18 Batch  746/2303   train_loss = 3.926\n",
      "Epoch  18 Batch  766/2303   train_loss = 4.035\n",
      "Epoch  18 Batch  786/2303   train_loss = 3.563\n",
      "Epoch  18 Batch  806/2303   train_loss = 3.841\n",
      "Epoch  18 Batch  826/2303   train_loss = 3.473\n",
      "Epoch  18 Batch  846/2303   train_loss = 4.265\n",
      "Epoch  18 Batch  866/2303   train_loss = 3.741\n",
      "Epoch  18 Batch  886/2303   train_loss = 3.609\n",
      "Epoch  18 Batch  906/2303   train_loss = 3.461\n",
      "Epoch  18 Batch  926/2303   train_loss = 3.379\n",
      "Epoch  18 Batch  946/2303   train_loss = 3.836\n",
      "Epoch  18 Batch  966/2303   train_loss = 3.756\n",
      "Epoch  18 Batch  986/2303   train_loss = 3.793\n",
      "Epoch  18 Batch 1006/2303   train_loss = 2.981\n",
      "Epoch  18 Batch 1026/2303   train_loss = 3.288\n",
      "Epoch  18 Batch 1046/2303   train_loss = 5.376\n",
      "Epoch  18 Batch 1066/2303   train_loss = 3.531\n",
      "Epoch  18 Batch 1086/2303   train_loss = 3.670\n",
      "Epoch  18 Batch 1106/2303   train_loss = 4.148\n",
      "Epoch  18 Batch 1126/2303   train_loss = 4.219\n",
      "Epoch  18 Batch 1146/2303   train_loss = 3.653\n",
      "Epoch  18 Batch 1166/2303   train_loss = 3.717\n",
      "Epoch  18 Batch 1186/2303   train_loss = 4.216\n",
      "Epoch  18 Batch 1206/2303   train_loss = 3.571\n",
      "Epoch  18 Batch 1226/2303   train_loss = 3.674\n",
      "Epoch  18 Batch 1246/2303   train_loss = 4.173\n",
      "Epoch  18 Batch 1266/2303   train_loss = 3.590\n",
      "Epoch  18 Batch 1286/2303   train_loss = 3.662\n",
      "Epoch  18 Batch 1306/2303   train_loss = 3.719\n",
      "Epoch  18 Batch 1326/2303   train_loss = 4.188\n",
      "Epoch  18 Batch 1346/2303   train_loss = 3.288\n",
      "Epoch  18 Batch 1366/2303   train_loss = 3.246\n",
      "Epoch  18 Batch 1386/2303   train_loss = 4.697\n",
      "Epoch  18 Batch 1406/2303   train_loss = 4.196\n",
      "Epoch  18 Batch 1426/2303   train_loss = 4.047\n",
      "Epoch  18 Batch 1446/2303   train_loss = 3.843\n",
      "Epoch  18 Batch 1466/2303   train_loss = 3.806\n",
      "Epoch  18 Batch 1486/2303   train_loss = 3.576\n",
      "Epoch  18 Batch 1506/2303   train_loss = 4.350\n",
      "Epoch  18 Batch 1526/2303   train_loss = 3.807\n",
      "Epoch  18 Batch 1546/2303   train_loss = 3.277\n",
      "Epoch  18 Batch 1566/2303   train_loss = 4.218\n",
      "Epoch  18 Batch 1586/2303   train_loss = 2.459\n",
      "Epoch  18 Batch 1606/2303   train_loss = 3.720\n",
      "Epoch  18 Batch 1626/2303   train_loss = 3.997\n",
      "Epoch  18 Batch 1646/2303   train_loss = 3.838\n",
      "Epoch  18 Batch 1666/2303   train_loss = 3.772\n",
      "Epoch  18 Batch 1686/2303   train_loss = 3.892\n",
      "Epoch  18 Batch 1706/2303   train_loss = 3.567\n",
      "Epoch  18 Batch 1726/2303   train_loss = 3.292\n",
      "Epoch  18 Batch 1746/2303   train_loss = 4.423\n",
      "Epoch  18 Batch 1766/2303   train_loss = 3.857\n",
      "Epoch  18 Batch 1786/2303   train_loss = 3.290\n",
      "Epoch  18 Batch 1806/2303   train_loss = 3.693\n",
      "Epoch  18 Batch 1826/2303   train_loss = 3.949\n",
      "Epoch  18 Batch 1846/2303   train_loss = 3.378\n",
      "Epoch  18 Batch 1866/2303   train_loss = 4.290\n",
      "Epoch  18 Batch 1886/2303   train_loss = 3.532\n",
      "Epoch  18 Batch 1906/2303   train_loss = 3.915\n",
      "Epoch  18 Batch 1926/2303   train_loss = 4.301\n",
      "Epoch  18 Batch 1946/2303   train_loss = 3.817\n",
      "Epoch  18 Batch 1966/2303   train_loss = 3.996\n",
      "Epoch  18 Batch 1986/2303   train_loss = 4.086\n",
      "Epoch  18 Batch 2006/2303   train_loss = 3.817\n",
      "Epoch  18 Batch 2026/2303   train_loss = 4.690\n",
      "Epoch  18 Batch 2046/2303   train_loss = 3.880\n",
      "Epoch  18 Batch 2066/2303   train_loss = 5.245\n",
      "Epoch  18 Batch 2086/2303   train_loss = 3.856\n",
      "Epoch  18 Batch 2106/2303   train_loss = 3.172\n",
      "Epoch  18 Batch 2126/2303   train_loss = 3.010\n",
      "Epoch  18 Batch 2146/2303   train_loss = 3.693\n",
      "Epoch  18 Batch 2166/2303   train_loss = 3.848\n",
      "Epoch  18 Batch 2186/2303   train_loss = 4.042\n",
      "Epoch  18 Batch 2206/2303   train_loss = 3.729\n",
      "Epoch  18 Batch 2226/2303   train_loss = 3.184\n",
      "Epoch  18 Batch 2246/2303   train_loss = 5.035\n",
      "Epoch  18 Batch 2266/2303   train_loss = 3.233\n",
      "Epoch  18 Batch 2286/2303   train_loss = 3.213\n",
      "Epoch  19 Batch    3/2303   train_loss = 3.688\n",
      "Epoch  19 Batch   23/2303   train_loss = 3.808\n",
      "Epoch  19 Batch   43/2303   train_loss = 3.428\n",
      "Epoch  19 Batch   63/2303   train_loss = 3.447\n",
      "Epoch  19 Batch   83/2303   train_loss = 3.671\n",
      "Epoch  19 Batch  103/2303   train_loss = 4.258\n",
      "Epoch  19 Batch  123/2303   train_loss = 4.574\n",
      "Epoch  19 Batch  143/2303   train_loss = 3.307\n",
      "Epoch  19 Batch  163/2303   train_loss = 4.583\n",
      "Epoch  19 Batch  183/2303   train_loss = 4.059\n",
      "Epoch  19 Batch  203/2303   train_loss = 3.726\n",
      "Epoch  19 Batch  223/2303   train_loss = 5.163\n",
      "Epoch  19 Batch  243/2303   train_loss = 3.560\n",
      "Epoch  19 Batch  263/2303   train_loss = 3.121\n",
      "Epoch  19 Batch  283/2303   train_loss = 4.175\n",
      "Epoch  19 Batch  303/2303   train_loss = 3.769\n",
      "Epoch  19 Batch  323/2303   train_loss = 3.960\n",
      "Epoch  19 Batch  343/2303   train_loss = 3.803\n",
      "Epoch  19 Batch  363/2303   train_loss = 3.293\n",
      "Epoch  19 Batch  383/2303   train_loss = 3.850\n",
      "Epoch  19 Batch  403/2303   train_loss = 3.888\n",
      "Epoch  19 Batch  423/2303   train_loss = 3.590\n",
      "Epoch  19 Batch  443/2303   train_loss = 3.578\n",
      "Epoch  19 Batch  463/2303   train_loss = 2.877\n",
      "Epoch  19 Batch  483/2303   train_loss = 3.901\n",
      "Epoch  19 Batch  503/2303   train_loss = 3.908\n",
      "Epoch  19 Batch  523/2303   train_loss = 4.350\n",
      "Epoch  19 Batch  543/2303   train_loss = 3.718\n",
      "Epoch  19 Batch  563/2303   train_loss = 3.694\n",
      "Epoch  19 Batch  583/2303   train_loss = 3.844\n",
      "Epoch  19 Batch  603/2303   train_loss = 4.063\n",
      "Epoch  19 Batch  623/2303   train_loss = 4.115\n",
      "Epoch  19 Batch  643/2303   train_loss = 3.690\n",
      "Epoch  19 Batch  663/2303   train_loss = 4.251\n",
      "Epoch  19 Batch  683/2303   train_loss = 3.797\n",
      "Epoch  19 Batch  703/2303   train_loss = 3.378\n",
      "Epoch  19 Batch  723/2303   train_loss = 4.216\n",
      "Epoch  19 Batch  743/2303   train_loss = 3.706\n",
      "Epoch  19 Batch  763/2303   train_loss = 3.231\n",
      "Epoch  19 Batch  783/2303   train_loss = 3.699\n",
      "Epoch  19 Batch  803/2303   train_loss = 2.791\n",
      "Epoch  19 Batch  823/2303   train_loss = 4.395\n",
      "Epoch  19 Batch  843/2303   train_loss = 4.181\n",
      "Epoch  19 Batch  863/2303   train_loss = 3.881\n",
      "Epoch  19 Batch  883/2303   train_loss = 3.289\n",
      "Epoch  19 Batch  903/2303   train_loss = 3.029\n",
      "Epoch  19 Batch  923/2303   train_loss = 3.137\n",
      "Epoch  19 Batch  943/2303   train_loss = 3.668\n",
      "Epoch  19 Batch  963/2303   train_loss = 2.794\n",
      "Epoch  19 Batch  983/2303   train_loss = 4.108\n",
      "Epoch  19 Batch 1003/2303   train_loss = 3.999\n",
      "Epoch  19 Batch 1023/2303   train_loss = 4.013\n",
      "Epoch  19 Batch 1043/2303   train_loss = 3.541\n",
      "Epoch  19 Batch 1063/2303   train_loss = 3.669\n",
      "Epoch  19 Batch 1083/2303   train_loss = 4.335\n",
      "Epoch  19 Batch 1103/2303   train_loss = 3.056\n",
      "Epoch  19 Batch 1123/2303   train_loss = 3.766\n",
      "Epoch  19 Batch 1143/2303   train_loss = 4.125\n",
      "Epoch  19 Batch 1163/2303   train_loss = 3.255\n",
      "Epoch  19 Batch 1183/2303   train_loss = 3.984\n",
      "Epoch  19 Batch 1203/2303   train_loss = 3.276\n",
      "Epoch  19 Batch 1223/2303   train_loss = 3.547\n",
      "Epoch  19 Batch 1243/2303   train_loss = 4.235\n",
      "Epoch  19 Batch 1263/2303   train_loss = 4.128\n",
      "Epoch  19 Batch 1283/2303   train_loss = 3.823\n",
      "Epoch  19 Batch 1303/2303   train_loss = 3.674\n",
      "Epoch  19 Batch 1323/2303   train_loss = 3.805\n",
      "Epoch  19 Batch 1343/2303   train_loss = 3.031\n",
      "Epoch  19 Batch 1363/2303   train_loss = 3.401\n",
      "Epoch  19 Batch 1383/2303   train_loss = 3.654\n",
      "Epoch  19 Batch 1403/2303   train_loss = 3.860\n",
      "Epoch  19 Batch 1423/2303   train_loss = 3.777\n",
      "Epoch  19 Batch 1443/2303   train_loss = 2.979\n",
      "Epoch  19 Batch 1463/2303   train_loss = 3.779\n",
      "Epoch  19 Batch 1483/2303   train_loss = 4.333\n",
      "Epoch  19 Batch 1503/2303   train_loss = 4.081\n",
      "Epoch  19 Batch 1523/2303   train_loss = 3.288\n",
      "Epoch  19 Batch 1543/2303   train_loss = 3.888\n",
      "Epoch  19 Batch 1563/2303   train_loss = 5.059\n",
      "Epoch  19 Batch 1583/2303   train_loss = 3.540\n",
      "Epoch  19 Batch 1603/2303   train_loss = 4.435\n",
      "Epoch  19 Batch 1623/2303   train_loss = 4.145\n",
      "Epoch  19 Batch 1643/2303   train_loss = 4.230\n",
      "Epoch  19 Batch 1663/2303   train_loss = 3.127\n",
      "Epoch  19 Batch 1683/2303   train_loss = 3.553\n",
      "Epoch  19 Batch 1703/2303   train_loss = 3.523\n",
      "Epoch  19 Batch 1723/2303   train_loss = 4.640\n",
      "Epoch  19 Batch 1743/2303   train_loss = 4.046\n",
      "Epoch  19 Batch 1763/2303   train_loss = 4.892\n",
      "Epoch  19 Batch 1783/2303   train_loss = 3.882\n",
      "Epoch  19 Batch 1803/2303   train_loss = 2.731\n",
      "Epoch  19 Batch 1823/2303   train_loss = 3.961\n",
      "Epoch  19 Batch 1843/2303   train_loss = 3.701\n",
      "Epoch  19 Batch 1863/2303   train_loss = 4.602\n",
      "Epoch  19 Batch 1883/2303   train_loss = 3.965\n",
      "Epoch  19 Batch 1903/2303   train_loss = 4.210\n",
      "Epoch  19 Batch 1923/2303   train_loss = 3.793\n",
      "Epoch  19 Batch 1943/2303   train_loss = 4.273\n",
      "Epoch  19 Batch 1963/2303   train_loss = 4.097\n",
      "Epoch  19 Batch 1983/2303   train_loss = 4.612\n",
      "Epoch  19 Batch 2003/2303   train_loss = 3.788\n",
      "Epoch  19 Batch 2023/2303   train_loss = 3.409\n",
      "Epoch  19 Batch 2043/2303   train_loss = 3.931\n",
      "Epoch  19 Batch 2063/2303   train_loss = 3.807\n",
      "Epoch  19 Batch 2083/2303   train_loss = 3.923\n",
      "Epoch  19 Batch 2103/2303   train_loss = 4.513\n",
      "Epoch  19 Batch 2123/2303   train_loss = 3.590\n",
      "Epoch  19 Batch 2143/2303   train_loss = 3.064\n",
      "Epoch  19 Batch 2163/2303   train_loss = 3.789\n",
      "Epoch  19 Batch 2183/2303   train_loss = 4.304\n",
      "Epoch  19 Batch 2203/2303   train_loss = 3.903\n",
      "Epoch  19 Batch 2223/2303   train_loss = 4.142\n",
      "Epoch  19 Batch 2243/2303   train_loss = 3.755\n",
      "Epoch  19 Batch 2263/2303   train_loss = 3.035\n",
      "Epoch  19 Batch 2283/2303   train_loss = 2.963\n",
      "Epoch  20 Batch    0/2303   train_loss = 4.082\n",
      "Epoch  20 Batch   20/2303   train_loss = 4.007\n",
      "Epoch  20 Batch   40/2303   train_loss = 3.368\n",
      "Epoch  20 Batch   60/2303   train_loss = 4.099\n",
      "Epoch  20 Batch   80/2303   train_loss = 3.939\n",
      "Epoch  20 Batch  100/2303   train_loss = 3.448\n",
      "Epoch  20 Batch  120/2303   train_loss = 2.947\n",
      "Epoch  20 Batch  140/2303   train_loss = 4.370\n",
      "Epoch  20 Batch  160/2303   train_loss = 3.154\n",
      "Epoch  20 Batch  180/2303   train_loss = 3.404\n",
      "Epoch  20 Batch  200/2303   train_loss = 3.819\n",
      "Epoch  20 Batch  220/2303   train_loss = 3.838\n",
      "Epoch  20 Batch  240/2303   train_loss = 3.324\n",
      "Epoch  20 Batch  260/2303   train_loss = 4.336\n",
      "Epoch  20 Batch  280/2303   train_loss = 4.418\n",
      "Epoch  20 Batch  300/2303   train_loss = 4.394\n",
      "Epoch  20 Batch  320/2303   train_loss = 3.956\n",
      "Epoch  20 Batch  340/2303   train_loss = 4.565\n",
      "Epoch  20 Batch  360/2303   train_loss = 3.811\n",
      "Epoch  20 Batch  380/2303   train_loss = 3.472\n",
      "Epoch  20 Batch  400/2303   train_loss = 3.671\n",
      "Epoch  20 Batch  420/2303   train_loss = 3.898\n",
      "Epoch  20 Batch  440/2303   train_loss = 2.459\n",
      "Epoch  20 Batch  460/2303   train_loss = 3.539\n",
      "Epoch  20 Batch  480/2303   train_loss = 3.858\n",
      "Epoch  20 Batch  500/2303   train_loss = 3.593\n",
      "Epoch  20 Batch  520/2303   train_loss = 5.013\n",
      "Epoch  20 Batch  540/2303   train_loss = 3.764\n",
      "Epoch  20 Batch  560/2303   train_loss = 4.138\n",
      "Epoch  20 Batch  580/2303   train_loss = 4.175\n",
      "Epoch  20 Batch  600/2303   train_loss = 4.179\n",
      "Epoch  20 Batch  620/2303   train_loss = 3.657\n",
      "Epoch  20 Batch  640/2303   train_loss = 3.822\n",
      "Epoch  20 Batch  660/2303   train_loss = 3.915\n",
      "Epoch  20 Batch  680/2303   train_loss = 4.119\n",
      "Epoch  20 Batch  700/2303   train_loss = 3.618\n",
      "Epoch  20 Batch  720/2303   train_loss = 3.363\n",
      "Epoch  20 Batch  740/2303   train_loss = 3.760\n",
      "Epoch  20 Batch  760/2303   train_loss = 3.905\n",
      "Epoch  20 Batch  780/2303   train_loss = 3.508\n",
      "Epoch  20 Batch  800/2303   train_loss = 4.423\n",
      "Epoch  20 Batch  820/2303   train_loss = 3.250\n",
      "Epoch  20 Batch  840/2303   train_loss = 3.385\n",
      "Epoch  20 Batch  860/2303   train_loss = 3.212\n",
      "Epoch  20 Batch  880/2303   train_loss = 2.910\n",
      "Epoch  20 Batch  900/2303   train_loss = 3.368\n",
      "Epoch  20 Batch  920/2303   train_loss = 3.681\n",
      "Epoch  20 Batch  940/2303   train_loss = 3.998\n",
      "Epoch  20 Batch  960/2303   train_loss = 4.243\n",
      "Epoch  20 Batch  980/2303   train_loss = 3.150\n",
      "Epoch  20 Batch 1000/2303   train_loss = 3.698\n",
      "Epoch  20 Batch 1020/2303   train_loss = 3.902\n",
      "Epoch  20 Batch 1040/2303   train_loss = 4.313\n",
      "Epoch  20 Batch 1060/2303   train_loss = 3.045\n",
      "Epoch  20 Batch 1080/2303   train_loss = 4.623\n",
      "Epoch  20 Batch 1100/2303   train_loss = 4.438\n",
      "Epoch  20 Batch 1120/2303   train_loss = 3.902\n",
      "Epoch  20 Batch 1140/2303   train_loss = 3.721\n",
      "Epoch  20 Batch 1160/2303   train_loss = 3.972\n",
      "Epoch  20 Batch 1180/2303   train_loss = 3.155\n",
      "Epoch  20 Batch 1200/2303   train_loss = 4.188\n",
      "Epoch  20 Batch 1220/2303   train_loss = 4.548\n",
      "Epoch  20 Batch 1240/2303   train_loss = 3.959\n",
      "Epoch  20 Batch 1260/2303   train_loss = 4.234\n",
      "Epoch  20 Batch 1280/2303   train_loss = 3.943\n",
      "Epoch  20 Batch 1300/2303   train_loss = 4.399\n",
      "Epoch  20 Batch 1320/2303   train_loss = 3.814\n",
      "Epoch  20 Batch 1340/2303   train_loss = 3.350\n",
      "Epoch  20 Batch 1360/2303   train_loss = 3.707\n",
      "Epoch  20 Batch 1380/2303   train_loss = 3.498\n",
      "Epoch  20 Batch 1400/2303   train_loss = 3.329\n",
      "Epoch  20 Batch 1420/2303   train_loss = 3.513\n",
      "Epoch  20 Batch 1440/2303   train_loss = 4.286\n",
      "Epoch  20 Batch 1460/2303   train_loss = 4.276\n",
      "Epoch  20 Batch 1480/2303   train_loss = 3.839\n",
      "Epoch  20 Batch 1500/2303   train_loss = 4.049\n",
      "Epoch  20 Batch 1520/2303   train_loss = 3.608\n",
      "Epoch  20 Batch 1540/2303   train_loss = 3.110\n",
      "Epoch  20 Batch 1560/2303   train_loss = 2.890\n",
      "Epoch  20 Batch 1580/2303   train_loss = 2.880\n",
      "Epoch  20 Batch 1600/2303   train_loss = 5.086\n",
      "Epoch  20 Batch 1620/2303   train_loss = 4.313\n",
      "Epoch  20 Batch 1640/2303   train_loss = 3.832\n",
      "Epoch  20 Batch 1660/2303   train_loss = 4.456\n",
      "Epoch  20 Batch 1680/2303   train_loss = 3.417\n",
      "Epoch  20 Batch 1700/2303   train_loss = 3.218\n",
      "Epoch  20 Batch 1720/2303   train_loss = 4.095\n",
      "Epoch  20 Batch 1740/2303   train_loss = 4.107\n",
      "Epoch  20 Batch 1760/2303   train_loss = 3.823\n",
      "Epoch  20 Batch 1780/2303   train_loss = 2.927\n",
      "Epoch  20 Batch 1800/2303   train_loss = 3.771\n",
      "Epoch  20 Batch 1820/2303   train_loss = 3.604\n",
      "Epoch  20 Batch 1840/2303   train_loss = 4.134\n",
      "Epoch  20 Batch 1860/2303   train_loss = 3.370\n",
      "Epoch  20 Batch 1880/2303   train_loss = 3.978\n",
      "Epoch  20 Batch 1900/2303   train_loss = 4.434\n",
      "Epoch  20 Batch 1920/2303   train_loss = 3.929\n",
      "Epoch  20 Batch 1940/2303   train_loss = 3.713\n",
      "Epoch  20 Batch 1960/2303   train_loss = 3.958\n",
      "Epoch  20 Batch 1980/2303   train_loss = 3.708\n",
      "Epoch  20 Batch 2000/2303   train_loss = 4.052\n",
      "Epoch  20 Batch 2020/2303   train_loss = 4.024\n",
      "Epoch  20 Batch 2040/2303   train_loss = 3.687\n",
      "Epoch  20 Batch 2060/2303   train_loss = 3.766\n",
      "Epoch  20 Batch 2080/2303   train_loss = 3.408\n",
      "Epoch  20 Batch 2100/2303   train_loss = 4.994\n",
      "Epoch  20 Batch 2120/2303   train_loss = 3.229\n",
      "Epoch  20 Batch 2140/2303   train_loss = 3.410\n",
      "Epoch  20 Batch 2160/2303   train_loss = 3.834\n",
      "Epoch  20 Batch 2180/2303   train_loss = 3.989\n",
      "Epoch  20 Batch 2200/2303   train_loss = 2.856\n",
      "Epoch  20 Batch 2220/2303   train_loss = 2.621\n",
      "Epoch  20 Batch 2240/2303   train_loss = 3.532\n",
      "Epoch  20 Batch 2260/2303   train_loss = 3.509\n",
      "Epoch  20 Batch 2280/2303   train_loss = 3.903\n",
      "Epoch  20 Batch 2300/2303   train_loss = 4.609\n",
      "Epoch  21 Batch   17/2303   train_loss = 3.364\n",
      "Epoch  21 Batch   37/2303   train_loss = 4.480\n",
      "Epoch  21 Batch   57/2303   train_loss = 3.495\n",
      "Epoch  21 Batch   77/2303   train_loss = 3.428\n",
      "Epoch  21 Batch   97/2303   train_loss = 4.381\n",
      "Epoch  21 Batch  117/2303   train_loss = 3.109\n",
      "Epoch  21 Batch  137/2303   train_loss = 4.065\n",
      "Epoch  21 Batch  157/2303   train_loss = 3.328\n",
      "Epoch  21 Batch  177/2303   train_loss = 4.506\n",
      "Epoch  21 Batch  197/2303   train_loss = 3.147\n",
      "Epoch  21 Batch  217/2303   train_loss = 3.411\n",
      "Epoch  21 Batch  237/2303   train_loss = 3.923\n",
      "Epoch  21 Batch  257/2303   train_loss = 3.496\n",
      "Epoch  21 Batch  277/2303   train_loss = 3.196\n",
      "Epoch  21 Batch  297/2303   train_loss = 4.026\n",
      "Epoch  21 Batch  317/2303   train_loss = 3.260\n",
      "Epoch  21 Batch  337/2303   train_loss = 4.969\n",
      "Epoch  21 Batch  357/2303   train_loss = 3.633\n",
      "Epoch  21 Batch  377/2303   train_loss = 4.099\n",
      "Epoch  21 Batch  397/2303   train_loss = 3.700\n",
      "Epoch  21 Batch  417/2303   train_loss = 4.220\n",
      "Epoch  21 Batch  437/2303   train_loss = 4.279\n",
      "Epoch  21 Batch  457/2303   train_loss = 4.040\n",
      "Epoch  21 Batch  477/2303   train_loss = 4.189\n",
      "Epoch  21 Batch  497/2303   train_loss = 4.099\n",
      "Epoch  21 Batch  517/2303   train_loss = 3.805\n",
      "Epoch  21 Batch  537/2303   train_loss = 3.522\n",
      "Epoch  21 Batch  557/2303   train_loss = 4.059\n",
      "Epoch  21 Batch  577/2303   train_loss = 3.772\n",
      "Epoch  21 Batch  597/2303   train_loss = 3.882\n",
      "Epoch  21 Batch  617/2303   train_loss = 3.399\n",
      "Epoch  21 Batch  637/2303   train_loss = 3.793\n",
      "Epoch  21 Batch  657/2303   train_loss = 3.581\n",
      "Epoch  21 Batch  677/2303   train_loss = 3.627\n",
      "Epoch  21 Batch  697/2303   train_loss = 3.542\n",
      "Epoch  21 Batch  717/2303   train_loss = 3.324\n",
      "Epoch  21 Batch  737/2303   train_loss = 3.017\n",
      "Epoch  21 Batch  757/2303   train_loss = 4.068\n",
      "Epoch  21 Batch  777/2303   train_loss = 3.929\n",
      "Epoch  21 Batch  797/2303   train_loss = 3.798\n",
      "Epoch  21 Batch  817/2303   train_loss = 4.160\n",
      "Epoch  21 Batch  837/2303   train_loss = 3.405\n",
      "Epoch  21 Batch  857/2303   train_loss = 5.030\n",
      "Epoch  21 Batch  877/2303   train_loss = 3.796\n",
      "Epoch  21 Batch  897/2303   train_loss = 3.448\n",
      "Epoch  21 Batch  917/2303   train_loss = 3.594\n",
      "Epoch  21 Batch  937/2303   train_loss = 3.807\n",
      "Epoch  21 Batch  957/2303   train_loss = 3.605\n",
      "Epoch  21 Batch  977/2303   train_loss = 3.379\n",
      "Epoch  21 Batch  997/2303   train_loss = 3.596\n",
      "Epoch  21 Batch 1017/2303   train_loss = 3.826\n",
      "Epoch  21 Batch 1037/2303   train_loss = 3.578\n",
      "Epoch  21 Batch 1057/2303   train_loss = 3.490\n",
      "Epoch  21 Batch 1077/2303   train_loss = 4.202\n",
      "Epoch  21 Batch 1097/2303   train_loss = 3.989\n",
      "Epoch  21 Batch 1117/2303   train_loss = 5.777\n",
      "Epoch  21 Batch 1137/2303   train_loss = 4.126\n",
      "Epoch  21 Batch 1157/2303   train_loss = 3.615\n",
      "Epoch  21 Batch 1177/2303   train_loss = 3.847\n",
      "Epoch  21 Batch 1197/2303   train_loss = 4.704\n",
      "Epoch  21 Batch 1217/2303   train_loss = 3.866\n",
      "Epoch  21 Batch 1237/2303   train_loss = 3.624\n",
      "Epoch  21 Batch 1257/2303   train_loss = 3.862\n",
      "Epoch  21 Batch 1277/2303   train_loss = 4.056\n",
      "Epoch  21 Batch 1297/2303   train_loss = 3.518\n",
      "Epoch  21 Batch 1317/2303   train_loss = 4.137\n",
      "Epoch  21 Batch 1337/2303   train_loss = 4.394\n",
      "Epoch  21 Batch 1357/2303   train_loss = 3.182\n",
      "Epoch  21 Batch 1377/2303   train_loss = 3.824\n",
      "Epoch  21 Batch 1397/2303   train_loss = 3.270\n",
      "Epoch  21 Batch 1417/2303   train_loss = 2.436\n",
      "Epoch  21 Batch 1437/2303   train_loss = 4.490\n",
      "Epoch  21 Batch 1457/2303   train_loss = 3.588\n",
      "Epoch  21 Batch 1477/2303   train_loss = 4.794\n",
      "Epoch  21 Batch 1497/2303   train_loss = 3.286\n",
      "Epoch  21 Batch 1517/2303   train_loss = 4.072\n",
      "Epoch  21 Batch 1537/2303   train_loss = 4.514\n",
      "Epoch  21 Batch 1557/2303   train_loss = 4.790\n",
      "Epoch  21 Batch 1577/2303   train_loss = 4.727\n",
      "Epoch  21 Batch 1597/2303   train_loss = 4.119\n",
      "Epoch  21 Batch 1617/2303   train_loss = 4.148\n",
      "Epoch  21 Batch 1637/2303   train_loss = 3.881\n",
      "Epoch  21 Batch 1657/2303   train_loss = 3.157\n",
      "Epoch  21 Batch 1677/2303   train_loss = 3.449\n",
      "Epoch  21 Batch 1697/2303   train_loss = 4.083\n",
      "Epoch  21 Batch 1717/2303   train_loss = 3.790\n",
      "Epoch  21 Batch 1737/2303   train_loss = 4.087\n",
      "Epoch  21 Batch 1757/2303   train_loss = 4.551\n",
      "Epoch  21 Batch 1777/2303   train_loss = 3.920\n",
      "Epoch  21 Batch 1797/2303   train_loss = 3.405\n",
      "Epoch  21 Batch 1817/2303   train_loss = 4.279\n",
      "Epoch  21 Batch 1837/2303   train_loss = 4.909\n",
      "Epoch  21 Batch 1857/2303   train_loss = 3.326\n",
      "Epoch  21 Batch 1877/2303   train_loss = 3.654\n",
      "Epoch  21 Batch 1897/2303   train_loss = 4.452\n",
      "Epoch  21 Batch 1917/2303   train_loss = 3.549\n",
      "Epoch  21 Batch 1937/2303   train_loss = 3.493\n",
      "Epoch  21 Batch 1957/2303   train_loss = 4.285\n",
      "Epoch  21 Batch 1977/2303   train_loss = 3.754\n",
      "Epoch  21 Batch 1997/2303   train_loss = 3.973\n",
      "Epoch  21 Batch 2017/2303   train_loss = 3.307\n",
      "Epoch  21 Batch 2037/2303   train_loss = 3.733\n",
      "Epoch  21 Batch 2057/2303   train_loss = 3.982\n",
      "Epoch  21 Batch 2077/2303   train_loss = 3.733\n",
      "Epoch  21 Batch 2097/2303   train_loss = 3.244\n",
      "Epoch  21 Batch 2117/2303   train_loss = 4.815\n",
      "Epoch  21 Batch 2137/2303   train_loss = 4.120\n",
      "Epoch  21 Batch 2157/2303   train_loss = 4.382\n",
      "Epoch  21 Batch 2177/2303   train_loss = 3.179\n",
      "Epoch  21 Batch 2197/2303   train_loss = 3.918\n",
      "Epoch  21 Batch 2217/2303   train_loss = 3.547\n",
      "Epoch  21 Batch 2237/2303   train_loss = 3.855\n",
      "Epoch  21 Batch 2257/2303   train_loss = 3.496\n",
      "Epoch  21 Batch 2277/2303   train_loss = 3.974\n",
      "Epoch  21 Batch 2297/2303   train_loss = 4.147\n",
      "Epoch  22 Batch   14/2303   train_loss = 4.338\n",
      "Epoch  22 Batch   34/2303   train_loss = 4.036\n",
      "Epoch  22 Batch   54/2303   train_loss = 3.846\n",
      "Epoch  22 Batch   74/2303   train_loss = 4.205\n",
      "Epoch  22 Batch   94/2303   train_loss = 4.117\n",
      "Epoch  22 Batch  114/2303   train_loss = 3.446\n",
      "Epoch  22 Batch  134/2303   train_loss = 3.556\n",
      "Epoch  22 Batch  154/2303   train_loss = 5.293\n",
      "Epoch  22 Batch  174/2303   train_loss = 4.165\n",
      "Epoch  22 Batch  194/2303   train_loss = 4.256\n",
      "Epoch  22 Batch  214/2303   train_loss = 3.981\n",
      "Epoch  22 Batch  234/2303   train_loss = 3.610\n",
      "Epoch  22 Batch  254/2303   train_loss = 4.444\n",
      "Epoch  22 Batch  274/2303   train_loss = 3.438\n",
      "Epoch  22 Batch  294/2303   train_loss = 3.695\n",
      "Epoch  22 Batch  314/2303   train_loss = 3.163\n",
      "Epoch  22 Batch  334/2303   train_loss = 3.429\n",
      "Epoch  22 Batch  354/2303   train_loss = 2.908\n",
      "Epoch  22 Batch  374/2303   train_loss = 3.476\n",
      "Epoch  22 Batch  394/2303   train_loss = 4.493\n",
      "Epoch  22 Batch  414/2303   train_loss = 4.109\n",
      "Epoch  22 Batch  434/2303   train_loss = 3.807\n",
      "Epoch  22 Batch  454/2303   train_loss = 4.130\n",
      "Epoch  22 Batch  474/2303   train_loss = 4.275\n",
      "Epoch  22 Batch  494/2303   train_loss = 4.644\n",
      "Epoch  22 Batch  514/2303   train_loss = 3.772\n",
      "Epoch  22 Batch  534/2303   train_loss = 3.952\n",
      "Epoch  22 Batch  554/2303   train_loss = 5.252\n",
      "Epoch  22 Batch  574/2303   train_loss = 4.289\n",
      "Epoch  22 Batch  594/2303   train_loss = 3.675\n",
      "Epoch  22 Batch  614/2303   train_loss = 3.376\n",
      "Epoch  22 Batch  634/2303   train_loss = 3.517\n",
      "Epoch  22 Batch  654/2303   train_loss = 3.695\n",
      "Epoch  22 Batch  674/2303   train_loss = 4.212\n",
      "Epoch  22 Batch  694/2303   train_loss = 3.592\n",
      "Epoch  22 Batch  714/2303   train_loss = 3.323\n",
      "Epoch  22 Batch  734/2303   train_loss = 3.627\n",
      "Epoch  22 Batch  754/2303   train_loss = 4.047\n",
      "Epoch  22 Batch  774/2303   train_loss = 3.776\n",
      "Epoch  22 Batch  794/2303   train_loss = 4.246\n",
      "Epoch  22 Batch  814/2303   train_loss = 3.218\n",
      "Epoch  22 Batch  834/2303   train_loss = 3.567\n",
      "Epoch  22 Batch  854/2303   train_loss = 3.209\n",
      "Epoch  22 Batch  874/2303   train_loss = 4.008\n",
      "Epoch  22 Batch  894/2303   train_loss = 3.690\n",
      "Epoch  22 Batch  914/2303   train_loss = 3.815\n",
      "Epoch  22 Batch  934/2303   train_loss = 4.533\n",
      "Epoch  22 Batch  954/2303   train_loss = 3.040\n",
      "Epoch  22 Batch  974/2303   train_loss = 3.521\n",
      "Epoch  22 Batch  994/2303   train_loss = 3.160\n",
      "Epoch  22 Batch 1014/2303   train_loss = 4.341\n",
      "Epoch  22 Batch 1034/2303   train_loss = 4.096\n",
      "Epoch  22 Batch 1054/2303   train_loss = 4.081\n",
      "Epoch  22 Batch 1074/2303   train_loss = 3.873\n",
      "Epoch  22 Batch 1094/2303   train_loss = 4.334\n",
      "Epoch  22 Batch 1114/2303   train_loss = 3.800\n",
      "Epoch  22 Batch 1134/2303   train_loss = 3.684\n",
      "Epoch  22 Batch 1154/2303   train_loss = 4.087\n",
      "Epoch  22 Batch 1174/2303   train_loss = 4.017\n",
      "Epoch  22 Batch 1194/2303   train_loss = 3.718\n",
      "Epoch  22 Batch 1214/2303   train_loss = 4.021\n",
      "Epoch  22 Batch 1234/2303   train_loss = 4.185\n",
      "Epoch  22 Batch 1254/2303   train_loss = 3.918\n",
      "Epoch  22 Batch 1274/2303   train_loss = 3.855\n",
      "Epoch  22 Batch 1294/2303   train_loss = 4.766\n",
      "Epoch  22 Batch 1314/2303   train_loss = 3.632\n",
      "Epoch  22 Batch 1334/2303   train_loss = 3.784\n",
      "Epoch  22 Batch 1354/2303   train_loss = 4.090\n",
      "Epoch  22 Batch 1374/2303   train_loss = 3.938\n",
      "Epoch  22 Batch 1394/2303   train_loss = 3.473\n",
      "Epoch  22 Batch 1414/2303   train_loss = 3.353\n",
      "Epoch  22 Batch 1434/2303   train_loss = 4.179\n",
      "Epoch  22 Batch 1454/2303   train_loss = 4.022\n",
      "Epoch  22 Batch 1474/2303   train_loss = 4.247\n",
      "Epoch  22 Batch 1494/2303   train_loss = 4.607\n",
      "Epoch  22 Batch 1514/2303   train_loss = 4.706\n",
      "Epoch  22 Batch 1534/2303   train_loss = 3.826\n",
      "Epoch  22 Batch 1554/2303   train_loss = 3.131\n",
      "Epoch  22 Batch 1574/2303   train_loss = 4.154\n",
      "Epoch  22 Batch 1594/2303   train_loss = 5.121\n",
      "Epoch  22 Batch 1614/2303   train_loss = 4.245\n",
      "Epoch  22 Batch 1634/2303   train_loss = 3.436\n",
      "Epoch  22 Batch 1654/2303   train_loss = 4.353\n",
      "Epoch  22 Batch 1674/2303   train_loss = 4.088\n",
      "Epoch  22 Batch 1694/2303   train_loss = 4.374\n",
      "Epoch  22 Batch 1714/2303   train_loss = 3.725\n",
      "Epoch  22 Batch 1734/2303   train_loss = 4.302\n",
      "Epoch  22 Batch 1754/2303   train_loss = 3.948\n",
      "Epoch  22 Batch 1774/2303   train_loss = 4.360\n",
      "Epoch  22 Batch 1794/2303   train_loss = 3.796\n",
      "Epoch  22 Batch 1814/2303   train_loss = 4.049\n",
      "Epoch  22 Batch 1834/2303   train_loss = 4.278\n",
      "Epoch  22 Batch 1854/2303   train_loss = 3.426\n",
      "Epoch  22 Batch 1874/2303   train_loss = 3.875\n",
      "Epoch  22 Batch 1894/2303   train_loss = 3.686\n",
      "Epoch  22 Batch 1914/2303   train_loss = 3.700\n",
      "Epoch  22 Batch 1934/2303   train_loss = 4.101\n",
      "Epoch  22 Batch 1954/2303   train_loss = 4.268\n",
      "Epoch  22 Batch 1974/2303   train_loss = 3.003\n",
      "Epoch  22 Batch 1994/2303   train_loss = 3.430\n",
      "Epoch  22 Batch 2014/2303   train_loss = 4.326\n",
      "Epoch  22 Batch 2034/2303   train_loss = 4.448\n",
      "Epoch  22 Batch 2054/2303   train_loss = 3.846\n",
      "Epoch  22 Batch 2074/2303   train_loss = 3.163\n",
      "Epoch  22 Batch 2094/2303   train_loss = 3.767\n",
      "Epoch  22 Batch 2114/2303   train_loss = 3.668\n",
      "Epoch  22 Batch 2134/2303   train_loss = 3.560\n",
      "Epoch  22 Batch 2154/2303   train_loss = 3.588\n",
      "Epoch  22 Batch 2174/2303   train_loss = 3.774\n",
      "Epoch  22 Batch 2194/2303   train_loss = 4.541\n",
      "Epoch  22 Batch 2214/2303   train_loss = 4.284\n",
      "Epoch  22 Batch 2234/2303   train_loss = 3.960\n",
      "Epoch  22 Batch 2254/2303   train_loss = 3.671\n",
      "Epoch  22 Batch 2274/2303   train_loss = 3.393\n",
      "Epoch  22 Batch 2294/2303   train_loss = 3.797\n",
      "Epoch  23 Batch   11/2303   train_loss = 3.895\n",
      "Epoch  23 Batch   31/2303   train_loss = 3.926\n",
      "Epoch  23 Batch   51/2303   train_loss = 4.060\n",
      "Epoch  23 Batch   71/2303   train_loss = 3.098\n",
      "Epoch  23 Batch   91/2303   train_loss = 4.521\n",
      "Epoch  23 Batch  111/2303   train_loss = 3.383\n",
      "Epoch  23 Batch  131/2303   train_loss = 3.042\n",
      "Epoch  23 Batch  151/2303   train_loss = 3.537\n",
      "Epoch  23 Batch  171/2303   train_loss = 4.186\n",
      "Epoch  23 Batch  191/2303   train_loss = 3.727\n",
      "Epoch  23 Batch  211/2303   train_loss = 3.980\n",
      "Epoch  23 Batch  231/2303   train_loss = 3.236\n",
      "Epoch  23 Batch  251/2303   train_loss = 3.411\n",
      "Epoch  23 Batch  271/2303   train_loss = 3.403\n",
      "Epoch  23 Batch  291/2303   train_loss = 4.351\n",
      "Epoch  23 Batch  311/2303   train_loss = 4.560\n",
      "Epoch  23 Batch  331/2303   train_loss = 3.938\n",
      "Epoch  23 Batch  351/2303   train_loss = 4.816\n",
      "Epoch  23 Batch  371/2303   train_loss = 4.352\n",
      "Epoch  23 Batch  391/2303   train_loss = 3.501\n",
      "Epoch  23 Batch  411/2303   train_loss = 3.670\n",
      "Epoch  23 Batch  431/2303   train_loss = 4.236\n",
      "Epoch  23 Batch  451/2303   train_loss = 3.878\n",
      "Epoch  23 Batch  471/2303   train_loss = 3.271\n",
      "Epoch  23 Batch  491/2303   train_loss = 4.856\n",
      "Epoch  23 Batch  511/2303   train_loss = 3.050\n",
      "Epoch  23 Batch  531/2303   train_loss = 4.811\n",
      "Epoch  23 Batch  551/2303   train_loss = 4.098\n",
      "Epoch  23 Batch  571/2303   train_loss = 3.098\n",
      "Epoch  23 Batch  591/2303   train_loss = 4.490\n",
      "Epoch  23 Batch  611/2303   train_loss = 3.555\n",
      "Epoch  23 Batch  631/2303   train_loss = 4.537\n",
      "Epoch  23 Batch  651/2303   train_loss = 4.087\n",
      "Epoch  23 Batch  671/2303   train_loss = 3.308\n",
      "Epoch  23 Batch  691/2303   train_loss = 3.973\n",
      "Epoch  23 Batch  711/2303   train_loss = 3.806\n",
      "Epoch  23 Batch  731/2303   train_loss = 3.497\n",
      "Epoch  23 Batch  751/2303   train_loss = 3.200\n",
      "Epoch  23 Batch  771/2303   train_loss = 4.236\n",
      "Epoch  23 Batch  791/2303   train_loss = 4.022\n",
      "Epoch  23 Batch  811/2303   train_loss = 3.429\n",
      "Epoch  23 Batch  831/2303   train_loss = 4.479\n",
      "Epoch  23 Batch  851/2303   train_loss = 4.248\n",
      "Epoch  23 Batch  871/2303   train_loss = 3.596\n",
      "Epoch  23 Batch  891/2303   train_loss = 4.327\n",
      "Epoch  23 Batch  911/2303   train_loss = 4.140\n",
      "Epoch  23 Batch  931/2303   train_loss = 4.473\n",
      "Epoch  23 Batch  951/2303   train_loss = 3.419\n",
      "Epoch  23 Batch  971/2303   train_loss = 4.034\n",
      "Epoch  23 Batch  991/2303   train_loss = 4.179\n",
      "Epoch  23 Batch 1011/2303   train_loss = 4.429\n",
      "Epoch  23 Batch 1031/2303   train_loss = 3.564\n",
      "Epoch  23 Batch 1051/2303   train_loss = 4.192\n",
      "Epoch  23 Batch 1071/2303   train_loss = 3.853\n",
      "Epoch  23 Batch 1091/2303   train_loss = 3.716\n",
      "Epoch  23 Batch 1111/2303   train_loss = 3.810\n",
      "Epoch  23 Batch 1131/2303   train_loss = 4.930\n",
      "Epoch  23 Batch 1151/2303   train_loss = 3.994\n",
      "Epoch  23 Batch 1171/2303   train_loss = 5.008\n",
      "Epoch  23 Batch 1191/2303   train_loss = 5.109\n",
      "Epoch  23 Batch 1211/2303   train_loss = 4.343\n",
      "Epoch  23 Batch 1231/2303   train_loss = 3.132\n",
      "Epoch  23 Batch 1251/2303   train_loss = 3.229\n",
      "Epoch  23 Batch 1271/2303   train_loss = 3.939\n",
      "Epoch  23 Batch 1291/2303   train_loss = 3.923\n",
      "Epoch  23 Batch 1311/2303   train_loss = 4.136\n",
      "Epoch  23 Batch 1331/2303   train_loss = 3.495\n",
      "Epoch  23 Batch 1351/2303   train_loss = 3.428\n",
      "Epoch  23 Batch 1371/2303   train_loss = 3.647\n",
      "Epoch  23 Batch 1391/2303   train_loss = 3.957\n",
      "Epoch  23 Batch 1411/2303   train_loss = 3.479\n",
      "Epoch  23 Batch 1431/2303   train_loss = 3.638\n",
      "Epoch  23 Batch 1451/2303   train_loss = 4.645\n",
      "Epoch  23 Batch 1471/2303   train_loss = 3.739\n",
      "Epoch  23 Batch 1491/2303   train_loss = 3.347\n",
      "Epoch  23 Batch 1511/2303   train_loss = 4.156\n",
      "Epoch  23 Batch 1531/2303   train_loss = 3.321\n",
      "Epoch  23 Batch 1551/2303   train_loss = 3.424\n",
      "Epoch  23 Batch 1571/2303   train_loss = 3.674\n",
      "Epoch  23 Batch 1591/2303   train_loss = 4.015\n",
      "Epoch  23 Batch 1611/2303   train_loss = 4.050\n",
      "Epoch  23 Batch 1631/2303   train_loss = 4.438\n",
      "Epoch  23 Batch 1651/2303   train_loss = 4.721\n",
      "Epoch  23 Batch 1671/2303   train_loss = 2.730\n",
      "Epoch  23 Batch 1691/2303   train_loss = 3.614\n",
      "Epoch  23 Batch 1711/2303   train_loss = 4.192\n",
      "Epoch  23 Batch 1731/2303   train_loss = 4.447\n",
      "Epoch  23 Batch 1751/2303   train_loss = 4.174\n",
      "Epoch  23 Batch 1771/2303   train_loss = 4.823\n",
      "Epoch  23 Batch 1791/2303   train_loss = 4.063\n",
      "Epoch  23 Batch 1811/2303   train_loss = 3.170\n",
      "Epoch  23 Batch 1831/2303   train_loss = 3.405\n",
      "Epoch  23 Batch 1851/2303   train_loss = 3.216\n",
      "Epoch  23 Batch 1871/2303   train_loss = 3.550\n",
      "Epoch  23 Batch 1891/2303   train_loss = 4.308\n",
      "Epoch  23 Batch 1911/2303   train_loss = 3.773\n",
      "Epoch  23 Batch 1931/2303   train_loss = 3.735\n",
      "Epoch  23 Batch 1951/2303   train_loss = 4.211\n",
      "Epoch  23 Batch 1971/2303   train_loss = 3.429\n",
      "Epoch  23 Batch 1991/2303   train_loss = 4.382\n",
      "Epoch  23 Batch 2011/2303   train_loss = 3.341\n",
      "Epoch  23 Batch 2031/2303   train_loss = 4.020\n",
      "Epoch  23 Batch 2051/2303   train_loss = 4.924\n",
      "Epoch  23 Batch 2071/2303   train_loss = 3.594\n",
      "Epoch  23 Batch 2091/2303   train_loss = 3.168\n",
      "Epoch  23 Batch 2111/2303   train_loss = 3.836\n",
      "Epoch  23 Batch 2131/2303   train_loss = 3.045\n",
      "Epoch  23 Batch 2151/2303   train_loss = 3.658\n",
      "Epoch  23 Batch 2171/2303   train_loss = 3.533\n",
      "Epoch  23 Batch 2191/2303   train_loss = 3.282\n",
      "Epoch  23 Batch 2211/2303   train_loss = 4.145\n",
      "Epoch  23 Batch 2231/2303   train_loss = 2.913\n",
      "Epoch  23 Batch 2251/2303   train_loss = 4.360\n",
      "Epoch  23 Batch 2271/2303   train_loss = 5.062\n",
      "Epoch  23 Batch 2291/2303   train_loss = 3.546\n",
      "Epoch  24 Batch    8/2303   train_loss = 3.384\n",
      "Epoch  24 Batch   28/2303   train_loss = 3.970\n",
      "Epoch  24 Batch   48/2303   train_loss = 3.445\n",
      "Epoch  24 Batch   68/2303   train_loss = 3.647\n",
      "Epoch  24 Batch   88/2303   train_loss = 3.332\n",
      "Epoch  24 Batch  108/2303   train_loss = 3.912\n",
      "Epoch  24 Batch  128/2303   train_loss = 3.146\n",
      "Epoch  24 Batch  148/2303   train_loss = 3.411\n",
      "Epoch  24 Batch  168/2303   train_loss = 4.322\n",
      "Epoch  24 Batch  188/2303   train_loss = 3.979\n",
      "Epoch  24 Batch  208/2303   train_loss = 3.252\n",
      "Epoch  24 Batch  228/2303   train_loss = 4.203\n",
      "Epoch  24 Batch  248/2303   train_loss = 3.613\n",
      "Epoch  24 Batch  268/2303   train_loss = 3.286\n",
      "Epoch  24 Batch  288/2303   train_loss = 4.177\n",
      "Epoch  24 Batch  308/2303   train_loss = 3.773\n",
      "Epoch  24 Batch  328/2303   train_loss = 3.721\n",
      "Epoch  24 Batch  348/2303   train_loss = 3.997\n",
      "Epoch  24 Batch  368/2303   train_loss = 4.346\n",
      "Epoch  24 Batch  388/2303   train_loss = 3.707\n",
      "Epoch  24 Batch  408/2303   train_loss = 3.416\n",
      "Epoch  24 Batch  428/2303   train_loss = 3.248\n",
      "Epoch  24 Batch  448/2303   train_loss = 3.226\n",
      "Epoch  24 Batch  468/2303   train_loss = 3.316\n",
      "Epoch  24 Batch  488/2303   train_loss = 4.394\n",
      "Epoch  24 Batch  508/2303   train_loss = 3.287\n",
      "Epoch  24 Batch  528/2303   train_loss = 3.108\n",
      "Epoch  24 Batch  548/2303   train_loss = 4.083\n",
      "Epoch  24 Batch  568/2303   train_loss = 4.302\n",
      "Epoch  24 Batch  588/2303   train_loss = 4.619\n",
      "Epoch  24 Batch  608/2303   train_loss = 2.922\n",
      "Epoch  24 Batch  628/2303   train_loss = 3.162\n",
      "Epoch  24 Batch  648/2303   train_loss = 3.695\n",
      "Epoch  24 Batch  668/2303   train_loss = 3.787\n",
      "Epoch  24 Batch  688/2303   train_loss = 3.493\n",
      "Epoch  24 Batch  708/2303   train_loss = 4.515\n",
      "Epoch  24 Batch  728/2303   train_loss = 4.731\n",
      "Epoch  24 Batch  748/2303   train_loss = 3.653\n",
      "Epoch  24 Batch  768/2303   train_loss = 4.182\n",
      "Epoch  24 Batch  788/2303   train_loss = 3.750\n",
      "Epoch  24 Batch  808/2303   train_loss = 3.553\n",
      "Epoch  24 Batch  828/2303   train_loss = 3.877\n",
      "Epoch  24 Batch  848/2303   train_loss = 4.096\n",
      "Epoch  24 Batch  868/2303   train_loss = 3.523\n",
      "Epoch  24 Batch  888/2303   train_loss = 3.283\n",
      "Epoch  24 Batch  908/2303   train_loss = 3.874\n",
      "Epoch  24 Batch  928/2303   train_loss = 3.473\n",
      "Epoch  24 Batch  948/2303   train_loss = 4.326\n",
      "Epoch  24 Batch  968/2303   train_loss = 3.247\n",
      "Epoch  24 Batch  988/2303   train_loss = 3.141\n",
      "Epoch  24 Batch 1008/2303   train_loss = 4.331\n",
      "Epoch  24 Batch 1028/2303   train_loss = 3.320\n",
      "Epoch  24 Batch 1048/2303   train_loss = 4.633\n",
      "Epoch  24 Batch 1068/2303   train_loss = 3.704\n",
      "Epoch  24 Batch 1088/2303   train_loss = 3.947\n",
      "Epoch  24 Batch 1108/2303   train_loss = 3.955\n",
      "Epoch  24 Batch 1128/2303   train_loss = 3.591\n",
      "Epoch  24 Batch 1148/2303   train_loss = 3.925\n",
      "Epoch  24 Batch 1168/2303   train_loss = 3.972\n",
      "Epoch  24 Batch 1188/2303   train_loss = 3.830\n",
      "Epoch  24 Batch 1208/2303   train_loss = 3.614\n",
      "Epoch  24 Batch 1228/2303   train_loss = 4.098\n",
      "Epoch  24 Batch 1248/2303   train_loss = 4.341\n",
      "Epoch  24 Batch 1268/2303   train_loss = 4.073\n",
      "Epoch  24 Batch 1288/2303   train_loss = 3.938\n",
      "Epoch  24 Batch 1308/2303   train_loss = 3.946\n",
      "Epoch  24 Batch 1328/2303   train_loss = 4.228\n",
      "Epoch  24 Batch 1348/2303   train_loss = 3.427\n",
      "Epoch  24 Batch 1368/2303   train_loss = 4.281\n",
      "Epoch  24 Batch 1388/2303   train_loss = 3.672\n",
      "Epoch  24 Batch 1408/2303   train_loss = 4.066\n",
      "Epoch  24 Batch 1428/2303   train_loss = 4.299\n",
      "Epoch  24 Batch 1448/2303   train_loss = 3.454\n",
      "Epoch  24 Batch 1468/2303   train_loss = 3.613\n",
      "Epoch  24 Batch 1488/2303   train_loss = 4.074\n",
      "Epoch  24 Batch 1508/2303   train_loss = 3.950\n",
      "Epoch  24 Batch 1528/2303   train_loss = 4.399\n",
      "Epoch  24 Batch 1548/2303   train_loss = 4.577\n",
      "Epoch  24 Batch 1568/2303   train_loss = 4.378\n",
      "Epoch  24 Batch 1588/2303   train_loss = 3.906\n",
      "Epoch  24 Batch 1608/2303   train_loss = 3.702\n",
      "Epoch  24 Batch 1628/2303   train_loss = 4.607\n",
      "Epoch  24 Batch 1648/2303   train_loss = 3.922\n",
      "Epoch  24 Batch 1668/2303   train_loss = 3.925\n",
      "Epoch  24 Batch 1688/2303   train_loss = 3.851\n",
      "Epoch  24 Batch 1708/2303   train_loss = 4.583\n",
      "Epoch  24 Batch 1728/2303   train_loss = 3.606\n",
      "Epoch  24 Batch 1748/2303   train_loss = 3.479\n",
      "Epoch  24 Batch 1768/2303   train_loss = 4.131\n",
      "Epoch  24 Batch 1788/2303   train_loss = 4.227\n",
      "Epoch  24 Batch 1808/2303   train_loss = 3.645\n",
      "Epoch  24 Batch 1828/2303   train_loss = 3.327\n",
      "Epoch  24 Batch 1848/2303   train_loss = 4.056\n",
      "Epoch  24 Batch 1868/2303   train_loss = 3.115\n",
      "Epoch  24 Batch 1888/2303   train_loss = 4.613\n",
      "Epoch  24 Batch 1908/2303   train_loss = 4.247\n",
      "Epoch  24 Batch 1928/2303   train_loss = 3.749\n",
      "Epoch  24 Batch 1948/2303   train_loss = 3.805\n",
      "Epoch  24 Batch 1968/2303   train_loss = 3.650\n",
      "Epoch  24 Batch 1988/2303   train_loss = 4.467\n",
      "Epoch  24 Batch 2008/2303   train_loss = 3.830\n",
      "Epoch  24 Batch 2028/2303   train_loss = 3.853\n",
      "Epoch  24 Batch 2048/2303   train_loss = 4.059\n",
      "Epoch  24 Batch 2068/2303   train_loss = 3.241\n",
      "Epoch  24 Batch 2088/2303   train_loss = 4.225\n",
      "Epoch  24 Batch 2108/2303   train_loss = 3.537\n",
      "Epoch  24 Batch 2128/2303   train_loss = 3.526\n",
      "Epoch  24 Batch 2148/2303   train_loss = 3.650\n",
      "Epoch  24 Batch 2168/2303   train_loss = 3.004\n",
      "Epoch  24 Batch 2188/2303   train_loss = 3.675\n",
      "Epoch  24 Batch 2208/2303   train_loss = 4.284\n",
      "Epoch  24 Batch 2228/2303   train_loss = 4.092\n",
      "Epoch  24 Batch 2248/2303   train_loss = 3.405\n",
      "Epoch  24 Batch 2268/2303   train_loss = 3.523\n",
      "Epoch  24 Batch 2288/2303   train_loss = 3.872\n",
      "Epoch  25 Batch    5/2303   train_loss = 3.262\n",
      "Epoch  25 Batch   25/2303   train_loss = 4.033\n",
      "Epoch  25 Batch   45/2303   train_loss = 3.259\n",
      "Epoch  25 Batch   65/2303   train_loss = 3.033\n",
      "Epoch  25 Batch   85/2303   train_loss = 3.572\n",
      "Epoch  25 Batch  105/2303   train_loss = 4.018\n",
      "Epoch  25 Batch  125/2303   train_loss = 3.271\n",
      "Epoch  25 Batch  145/2303   train_loss = 4.043\n",
      "Epoch  25 Batch  165/2303   train_loss = 4.456\n",
      "Epoch  25 Batch  185/2303   train_loss = 3.784\n",
      "Epoch  25 Batch  205/2303   train_loss = 4.005\n",
      "Epoch  25 Batch  225/2303   train_loss = 3.834\n",
      "Epoch  25 Batch  245/2303   train_loss = 3.346\n",
      "Epoch  25 Batch  265/2303   train_loss = 3.878\n",
      "Epoch  25 Batch  285/2303   train_loss = 3.560\n",
      "Epoch  25 Batch  305/2303   train_loss = 4.136\n",
      "Epoch  25 Batch  325/2303   train_loss = 3.261\n",
      "Epoch  25 Batch  345/2303   train_loss = 3.501\n",
      "Epoch  25 Batch  365/2303   train_loss = 3.874\n",
      "Epoch  25 Batch  385/2303   train_loss = 4.353\n",
      "Epoch  25 Batch  405/2303   train_loss = 3.366\n",
      "Epoch  25 Batch  425/2303   train_loss = 3.454\n",
      "Epoch  25 Batch  445/2303   train_loss = 3.593\n",
      "Epoch  25 Batch  465/2303   train_loss = 3.983\n",
      "Epoch  25 Batch  485/2303   train_loss = 3.829\n",
      "Epoch  25 Batch  505/2303   train_loss = 3.927\n",
      "Epoch  25 Batch  525/2303   train_loss = 2.781\n",
      "Epoch  25 Batch  545/2303   train_loss = 4.780\n",
      "Epoch  25 Batch  565/2303   train_loss = 4.001\n",
      "Epoch  25 Batch  585/2303   train_loss = 4.499\n",
      "Epoch  25 Batch  605/2303   train_loss = 4.302\n",
      "Epoch  25 Batch  625/2303   train_loss = 4.639\n",
      "Epoch  25 Batch  645/2303   train_loss = 4.263\n",
      "Epoch  25 Batch  665/2303   train_loss = 4.149\n",
      "Epoch  25 Batch  685/2303   train_loss = 3.619\n",
      "Epoch  25 Batch  705/2303   train_loss = 3.160\n",
      "Epoch  25 Batch  725/2303   train_loss = 3.409\n",
      "Epoch  25 Batch  745/2303   train_loss = 2.830\n",
      "Epoch  25 Batch  765/2303   train_loss = 3.628\n",
      "Epoch  25 Batch  785/2303   train_loss = 4.872\n",
      "Epoch  25 Batch  805/2303   train_loss = 4.308\n",
      "Epoch  25 Batch  825/2303   train_loss = 3.527\n",
      "Epoch  25 Batch  845/2303   train_loss = 4.410\n",
      "Epoch  25 Batch  865/2303   train_loss = 3.574\n",
      "Epoch  25 Batch  885/2303   train_loss = 3.949\n",
      "Epoch  25 Batch  905/2303   train_loss = 3.046\n",
      "Epoch  25 Batch  925/2303   train_loss = 3.956\n",
      "Epoch  25 Batch  945/2303   train_loss = 3.655\n",
      "Epoch  25 Batch  965/2303   train_loss = 4.371\n",
      "Epoch  25 Batch  985/2303   train_loss = 3.774\n",
      "Epoch  25 Batch 1005/2303   train_loss = 3.162\n",
      "Epoch  25 Batch 1025/2303   train_loss = 3.977\n",
      "Epoch  25 Batch 1045/2303   train_loss = 4.073\n",
      "Epoch  25 Batch 1065/2303   train_loss = 3.625\n",
      "Epoch  25 Batch 1085/2303   train_loss = 3.605\n",
      "Epoch  25 Batch 1105/2303   train_loss = 4.125\n",
      "Epoch  25 Batch 1125/2303   train_loss = 3.401\n",
      "Epoch  25 Batch 1145/2303   train_loss = 3.599\n",
      "Epoch  25 Batch 1165/2303   train_loss = 3.652\n",
      "Epoch  25 Batch 1185/2303   train_loss = 4.304\n",
      "Epoch  25 Batch 1205/2303   train_loss = 3.306\n",
      "Epoch  25 Batch 1225/2303   train_loss = 3.802\n",
      "Epoch  25 Batch 1245/2303   train_loss = 3.515\n",
      "Epoch  25 Batch 1265/2303   train_loss = 3.714\n",
      "Epoch  25 Batch 1285/2303   train_loss = 4.086\n",
      "Epoch  25 Batch 1305/2303   train_loss = 3.671\n",
      "Epoch  25 Batch 1325/2303   train_loss = 4.558\n",
      "Epoch  25 Batch 1345/2303   train_loss = 4.329\n",
      "Epoch  25 Batch 1365/2303   train_loss = 3.916\n",
      "Epoch  25 Batch 1385/2303   train_loss = 4.189\n",
      "Epoch  25 Batch 1405/2303   train_loss = 3.870\n",
      "Epoch  25 Batch 1425/2303   train_loss = 4.324\n",
      "Epoch  25 Batch 1445/2303   train_loss = 3.574\n",
      "Epoch  25 Batch 1465/2303   train_loss = 3.777\n",
      "Epoch  25 Batch 1485/2303   train_loss = 4.136\n",
      "Epoch  25 Batch 1505/2303   train_loss = 3.389\n",
      "Epoch  25 Batch 1525/2303   train_loss = 4.009\n",
      "Epoch  25 Batch 1545/2303   train_loss = 4.389\n",
      "Epoch  25 Batch 1565/2303   train_loss = 4.032\n",
      "Epoch  25 Batch 1585/2303   train_loss = 3.998\n",
      "Epoch  25 Batch 1605/2303   train_loss = 3.419\n",
      "Epoch  25 Batch 1625/2303   train_loss = 3.768\n",
      "Epoch  25 Batch 1645/2303   train_loss = 4.312\n",
      "Epoch  25 Batch 1665/2303   train_loss = 4.267\n",
      "Epoch  25 Batch 1685/2303   train_loss = 3.687\n",
      "Epoch  25 Batch 1705/2303   train_loss = 4.010\n",
      "Epoch  25 Batch 1725/2303   train_loss = 3.901\n",
      "Epoch  25 Batch 1745/2303   train_loss = 3.428\n",
      "Epoch  25 Batch 1765/2303   train_loss = 3.481\n",
      "Epoch  25 Batch 1785/2303   train_loss = 4.005\n",
      "Epoch  25 Batch 1805/2303   train_loss = 3.390\n",
      "Epoch  25 Batch 1825/2303   train_loss = 4.682\n",
      "Epoch  25 Batch 1845/2303   train_loss = 3.755\n",
      "Epoch  25 Batch 1865/2303   train_loss = 3.731\n",
      "Epoch  25 Batch 1885/2303   train_loss = 3.722\n",
      "Epoch  25 Batch 1905/2303   train_loss = 3.914\n",
      "Epoch  25 Batch 1925/2303   train_loss = 3.773\n",
      "Epoch  25 Batch 1945/2303   train_loss = 3.896\n",
      "Epoch  25 Batch 1965/2303   train_loss = 3.115\n",
      "Epoch  25 Batch 1985/2303   train_loss = 4.951\n",
      "Epoch  25 Batch 2005/2303   train_loss = 3.996\n",
      "Epoch  25 Batch 2025/2303   train_loss = 3.718\n",
      "Epoch  25 Batch 2045/2303   train_loss = 3.778\n",
      "Epoch  25 Batch 2065/2303   train_loss = 4.604\n",
      "Epoch  25 Batch 2085/2303   train_loss = 3.381\n",
      "Epoch  25 Batch 2105/2303   train_loss = 3.391\n",
      "Epoch  25 Batch 2125/2303   train_loss = 4.310\n",
      "Epoch  25 Batch 2145/2303   train_loss = 4.297\n",
      "Epoch  25 Batch 2165/2303   train_loss = 4.754\n",
      "Epoch  25 Batch 2185/2303   train_loss = 3.778\n",
      "Epoch  25 Batch 2205/2303   train_loss = 3.144\n",
      "Epoch  25 Batch 2225/2303   train_loss = 3.727\n",
      "Epoch  25 Batch 2245/2303   train_loss = 3.753\n",
      "Epoch  25 Batch 2265/2303   train_loss = 3.025\n",
      "Epoch  25 Batch 2285/2303   train_loss = 3.526\n",
      "Epoch  26 Batch    2/2303   train_loss = 3.455\n",
      "Epoch  26 Batch   22/2303   train_loss = 4.564\n",
      "Epoch  26 Batch   42/2303   train_loss = 4.175\n",
      "Epoch  26 Batch   62/2303   train_loss = 3.685\n",
      "Epoch  26 Batch   82/2303   train_loss = 3.093\n",
      "Epoch  26 Batch  102/2303   train_loss = 3.154\n",
      "Epoch  26 Batch  122/2303   train_loss = 4.856\n",
      "Epoch  26 Batch  142/2303   train_loss = 3.354\n",
      "Epoch  26 Batch  162/2303   train_loss = 3.219\n",
      "Epoch  26 Batch  182/2303   train_loss = 3.720\n",
      "Epoch  26 Batch  202/2303   train_loss = 2.999\n",
      "Epoch  26 Batch  222/2303   train_loss = 3.765\n",
      "Epoch  26 Batch  242/2303   train_loss = 3.540\n",
      "Epoch  26 Batch  262/2303   train_loss = 3.321\n",
      "Epoch  26 Batch  282/2303   train_loss = 3.186\n",
      "Epoch  26 Batch  302/2303   train_loss = 3.638\n",
      "Epoch  26 Batch  322/2303   train_loss = 4.082\n",
      "Epoch  26 Batch  342/2303   train_loss = 3.782\n",
      "Epoch  26 Batch  362/2303   train_loss = 3.318\n",
      "Epoch  26 Batch  382/2303   train_loss = 4.177\n",
      "Epoch  26 Batch  402/2303   train_loss = 4.069\n",
      "Epoch  26 Batch  422/2303   train_loss = 4.365\n",
      "Epoch  26 Batch  442/2303   train_loss = 3.214\n",
      "Epoch  26 Batch  462/2303   train_loss = 3.410\n",
      "Epoch  26 Batch  482/2303   train_loss = 3.856\n",
      "Epoch  26 Batch  502/2303   train_loss = 4.570\n",
      "Epoch  26 Batch  522/2303   train_loss = 3.890\n",
      "Epoch  26 Batch  542/2303   train_loss = 3.871\n",
      "Epoch  26 Batch  562/2303   train_loss = 3.454\n",
      "Epoch  26 Batch  582/2303   train_loss = 3.954\n",
      "Epoch  26 Batch  602/2303   train_loss = 4.301\n",
      "Epoch  26 Batch  622/2303   train_loss = 3.750\n",
      "Epoch  26 Batch  642/2303   train_loss = 3.964\n",
      "Epoch  26 Batch  662/2303   train_loss = 4.614\n",
      "Epoch  26 Batch  682/2303   train_loss = 3.608\n",
      "Epoch  26 Batch  702/2303   train_loss = 3.876\n",
      "Epoch  26 Batch  722/2303   train_loss = 4.216\n",
      "Epoch  26 Batch  742/2303   train_loss = 3.536\n",
      "Epoch  26 Batch  762/2303   train_loss = 4.280\n",
      "Epoch  26 Batch  782/2303   train_loss = 3.166\n",
      "Epoch  26 Batch  802/2303   train_loss = 3.885\n",
      "Epoch  26 Batch  822/2303   train_loss = 3.222\n",
      "Epoch  26 Batch  842/2303   train_loss = 3.630\n",
      "Epoch  26 Batch  862/2303   train_loss = 3.152\n",
      "Epoch  26 Batch  882/2303   train_loss = 3.871\n",
      "Epoch  26 Batch  902/2303   train_loss = 3.396\n",
      "Epoch  26 Batch  922/2303   train_loss = 3.420\n",
      "Epoch  26 Batch  942/2303   train_loss = 4.554\n",
      "Epoch  26 Batch  962/2303   train_loss = 4.477\n",
      "Epoch  26 Batch  982/2303   train_loss = 4.143\n",
      "Epoch  26 Batch 1002/2303   train_loss = 4.270\n",
      "Epoch  26 Batch 1022/2303   train_loss = 3.501\n",
      "Epoch  26 Batch 1042/2303   train_loss = 3.507\n",
      "Epoch  26 Batch 1062/2303   train_loss = 3.057\n",
      "Epoch  26 Batch 1082/2303   train_loss = 5.398\n",
      "Epoch  26 Batch 1102/2303   train_loss = 3.898\n",
      "Epoch  26 Batch 1122/2303   train_loss = 3.209\n",
      "Epoch  26 Batch 1142/2303   train_loss = 2.629\n",
      "Epoch  26 Batch 1162/2303   train_loss = 3.631\n",
      "Epoch  26 Batch 1182/2303   train_loss = 3.625\n",
      "Epoch  26 Batch 1202/2303   train_loss = 3.710\n",
      "Epoch  26 Batch 1222/2303   train_loss = 3.321\n",
      "Epoch  26 Batch 1242/2303   train_loss = 4.291\n",
      "Epoch  26 Batch 1262/2303   train_loss = 3.601\n",
      "Epoch  26 Batch 1282/2303   train_loss = 3.758\n",
      "Epoch  26 Batch 1302/2303   train_loss = 3.886\n",
      "Epoch  26 Batch 1322/2303   train_loss = 4.902\n",
      "Epoch  26 Batch 1342/2303   train_loss = 4.268\n",
      "Epoch  26 Batch 1362/2303   train_loss = 3.787\n",
      "Epoch  26 Batch 1382/2303   train_loss = 2.885\n",
      "Epoch  26 Batch 1402/2303   train_loss = 4.336\n",
      "Epoch  26 Batch 1422/2303   train_loss = 4.491\n",
      "Epoch  26 Batch 1442/2303   train_loss = 3.285\n",
      "Epoch  26 Batch 1462/2303   train_loss = 3.900\n",
      "Epoch  26 Batch 1482/2303   train_loss = 3.640\n",
      "Epoch  26 Batch 1502/2303   train_loss = 4.164\n",
      "Epoch  26 Batch 1522/2303   train_loss = 3.752\n",
      "Epoch  26 Batch 1542/2303   train_loss = 3.973\n",
      "Epoch  26 Batch 1562/2303   train_loss = 3.854\n",
      "Epoch  26 Batch 1582/2303   train_loss = 3.963\n",
      "Epoch  26 Batch 1602/2303   train_loss = 3.584\n",
      "Epoch  26 Batch 1622/2303   train_loss = 3.906\n",
      "Epoch  26 Batch 1642/2303   train_loss = 3.333\n",
      "Epoch  26 Batch 1662/2303   train_loss = 4.356\n",
      "Epoch  26 Batch 1682/2303   train_loss = 3.914\n",
      "Epoch  26 Batch 1702/2303   train_loss = 3.560\n",
      "Epoch  26 Batch 1722/2303   train_loss = 3.965\n",
      "Epoch  26 Batch 1742/2303   train_loss = 3.237\n",
      "Epoch  26 Batch 1762/2303   train_loss = 3.697\n",
      "Epoch  26 Batch 1782/2303   train_loss = 4.039\n",
      "Epoch  26 Batch 1802/2303   train_loss = 4.205\n",
      "Epoch  26 Batch 1822/2303   train_loss = 4.197\n",
      "Epoch  26 Batch 1842/2303   train_loss = 2.771\n",
      "Epoch  26 Batch 1862/2303   train_loss = 3.546\n",
      "Epoch  26 Batch 1882/2303   train_loss = 3.470\n",
      "Epoch  26 Batch 1902/2303   train_loss = 3.310\n",
      "Epoch  26 Batch 1922/2303   train_loss = 4.155\n",
      "Epoch  26 Batch 1942/2303   train_loss = 4.563\n",
      "Epoch  26 Batch 1962/2303   train_loss = 3.845\n",
      "Epoch  26 Batch 1982/2303   train_loss = 3.352\n",
      "Epoch  26 Batch 2002/2303   train_loss = 3.961\n",
      "Epoch  26 Batch 2022/2303   train_loss = 4.154\n",
      "Epoch  26 Batch 2042/2303   train_loss = 3.075\n",
      "Epoch  26 Batch 2062/2303   train_loss = 3.784\n",
      "Epoch  26 Batch 2082/2303   train_loss = 3.029\n",
      "Epoch  26 Batch 2102/2303   train_loss = 4.026\n",
      "Epoch  26 Batch 2122/2303   train_loss = 4.474\n",
      "Epoch  26 Batch 2142/2303   train_loss = 3.984\n",
      "Epoch  26 Batch 2162/2303   train_loss = 3.922\n",
      "Epoch  26 Batch 2182/2303   train_loss = 2.913\n",
      "Epoch  26 Batch 2202/2303   train_loss = 4.255\n",
      "Epoch  26 Batch 2222/2303   train_loss = 3.727\n",
      "Epoch  26 Batch 2242/2303   train_loss = 3.460\n",
      "Epoch  26 Batch 2262/2303   train_loss = 3.140\n",
      "Epoch  26 Batch 2282/2303   train_loss = 3.773\n",
      "Epoch  26 Batch 2302/2303   train_loss = 3.233\n",
      "Epoch  27 Batch   19/2303   train_loss = 2.908\n",
      "Epoch  27 Batch   39/2303   train_loss = 3.311\n",
      "Epoch  27 Batch   59/2303   train_loss = 4.142\n",
      "Epoch  27 Batch   79/2303   train_loss = 3.851\n",
      "Epoch  27 Batch   99/2303   train_loss = 3.241\n",
      "Epoch  27 Batch  119/2303   train_loss = 3.335\n",
      "Epoch  27 Batch  139/2303   train_loss = 3.162\n",
      "Epoch  27 Batch  159/2303   train_loss = 3.203\n",
      "Epoch  27 Batch  179/2303   train_loss = 3.414\n",
      "Epoch  27 Batch  199/2303   train_loss = 4.100\n",
      "Epoch  27 Batch  219/2303   train_loss = 3.892\n",
      "Epoch  27 Batch  239/2303   train_loss = 3.921\n",
      "Epoch  27 Batch  259/2303   train_loss = 4.349\n",
      "Epoch  27 Batch  279/2303   train_loss = 3.736\n",
      "Epoch  27 Batch  299/2303   train_loss = 4.425\n",
      "Epoch  27 Batch  319/2303   train_loss = 3.979\n",
      "Epoch  27 Batch  339/2303   train_loss = 3.711\n",
      "Epoch  27 Batch  359/2303   train_loss = 3.729\n",
      "Epoch  27 Batch  379/2303   train_loss = 3.325\n",
      "Epoch  27 Batch  399/2303   train_loss = 2.845\n",
      "Epoch  27 Batch  419/2303   train_loss = 3.904\n",
      "Epoch  27 Batch  439/2303   train_loss = 3.407\n",
      "Epoch  27 Batch  459/2303   train_loss = 4.417\n",
      "Epoch  27 Batch  479/2303   train_loss = 3.939\n",
      "Epoch  27 Batch  499/2303   train_loss = 3.348\n",
      "Epoch  27 Batch  519/2303   train_loss = 3.492\n",
      "Epoch  27 Batch  539/2303   train_loss = 3.698\n",
      "Epoch  27 Batch  559/2303   train_loss = 3.701\n",
      "Epoch  27 Batch  579/2303   train_loss = 3.933\n",
      "Epoch  27 Batch  599/2303   train_loss = 4.051\n",
      "Epoch  27 Batch  619/2303   train_loss = 4.213\n",
      "Epoch  27 Batch  639/2303   train_loss = 3.932\n",
      "Epoch  27 Batch  659/2303   train_loss = 4.414\n",
      "Epoch  27 Batch  679/2303   train_loss = 3.935\n",
      "Epoch  27 Batch  699/2303   train_loss = 3.325\n",
      "Epoch  27 Batch  719/2303   train_loss = 3.547\n",
      "Epoch  27 Batch  739/2303   train_loss = 2.626\n",
      "Epoch  27 Batch  759/2303   train_loss = 4.065\n",
      "Epoch  27 Batch  779/2303   train_loss = 3.438\n",
      "Epoch  27 Batch  799/2303   train_loss = 3.291\n",
      "Epoch  27 Batch  819/2303   train_loss = 3.488\n",
      "Epoch  27 Batch  839/2303   train_loss = 4.222\n",
      "Epoch  27 Batch  859/2303   train_loss = 3.568\n",
      "Epoch  27 Batch  879/2303   train_loss = 2.891\n",
      "Epoch  27 Batch  899/2303   train_loss = 3.930\n",
      "Epoch  27 Batch  919/2303   train_loss = 2.984\n",
      "Epoch  27 Batch  939/2303   train_loss = 3.790\n",
      "Epoch  27 Batch  959/2303   train_loss = 3.185\n",
      "Epoch  27 Batch  979/2303   train_loss = 3.426\n",
      "Epoch  27 Batch  999/2303   train_loss = 3.440\n",
      "Epoch  27 Batch 1019/2303   train_loss = 3.618\n",
      "Epoch  27 Batch 1039/2303   train_loss = 3.239\n",
      "Epoch  27 Batch 1059/2303   train_loss = 2.522\n",
      "Epoch  27 Batch 1079/2303   train_loss = 3.829\n",
      "Epoch  27 Batch 1099/2303   train_loss = 4.303\n",
      "Epoch  27 Batch 1119/2303   train_loss = 4.493\n",
      "Epoch  27 Batch 1139/2303   train_loss = 4.132\n",
      "Epoch  27 Batch 1159/2303   train_loss = 3.978\n",
      "Epoch  27 Batch 1179/2303   train_loss = 3.817\n",
      "Epoch  27 Batch 1199/2303   train_loss = 3.752\n",
      "Epoch  27 Batch 1219/2303   train_loss = 3.583\n",
      "Epoch  27 Batch 1239/2303   train_loss = 5.082\n",
      "Epoch  27 Batch 1259/2303   train_loss = 4.885\n",
      "Epoch  27 Batch 1279/2303   train_loss = 4.309\n",
      "Epoch  27 Batch 1299/2303   train_loss = 4.419\n",
      "Epoch  27 Batch 1319/2303   train_loss = 3.820\n",
      "Epoch  27 Batch 1339/2303   train_loss = 4.019\n",
      "Epoch  27 Batch 1359/2303   train_loss = 4.409\n",
      "Epoch  27 Batch 1379/2303   train_loss = 3.616\n",
      "Epoch  27 Batch 1399/2303   train_loss = 3.679\n",
      "Epoch  27 Batch 1419/2303   train_loss = 3.478\n",
      "Epoch  27 Batch 1439/2303   train_loss = 4.257\n",
      "Epoch  27 Batch 1459/2303   train_loss = 3.242\n",
      "Epoch  27 Batch 1479/2303   train_loss = 4.550\n",
      "Epoch  27 Batch 1499/2303   train_loss = 3.691\n",
      "Epoch  27 Batch 1519/2303   train_loss = 3.683\n",
      "Epoch  27 Batch 1539/2303   train_loss = 2.893\n",
      "Epoch  27 Batch 1559/2303   train_loss = 4.142\n",
      "Epoch  27 Batch 1579/2303   train_loss = 3.790\n",
      "Epoch  27 Batch 1599/2303   train_loss = 4.269\n",
      "Epoch  27 Batch 1619/2303   train_loss = 4.289\n",
      "Epoch  27 Batch 1639/2303   train_loss = 3.550\n",
      "Epoch  27 Batch 1659/2303   train_loss = 3.917\n",
      "Epoch  27 Batch 1679/2303   train_loss = 3.191\n",
      "Epoch  27 Batch 1699/2303   train_loss = 3.806\n",
      "Epoch  27 Batch 1719/2303   train_loss = 4.455\n",
      "Epoch  27 Batch 1739/2303   train_loss = 4.584\n",
      "Epoch  27 Batch 1759/2303   train_loss = 4.466\n",
      "Epoch  27 Batch 1779/2303   train_loss = 3.269\n",
      "Epoch  27 Batch 1799/2303   train_loss = 2.971\n",
      "Epoch  27 Batch 1819/2303   train_loss = 4.113\n",
      "Epoch  27 Batch 1839/2303   train_loss = 3.706\n",
      "Epoch  27 Batch 1859/2303   train_loss = 3.985\n",
      "Epoch  27 Batch 1879/2303   train_loss = 4.055\n",
      "Epoch  27 Batch 1899/2303   train_loss = 3.831\n",
      "Epoch  27 Batch 1919/2303   train_loss = 2.900\n",
      "Epoch  27 Batch 1939/2303   train_loss = 3.293\n",
      "Epoch  27 Batch 1959/2303   train_loss = 3.908\n",
      "Epoch  27 Batch 1979/2303   train_loss = 4.030\n",
      "Epoch  27 Batch 1999/2303   train_loss = 3.053\n",
      "Epoch  27 Batch 2019/2303   train_loss = 3.479\n",
      "Epoch  27 Batch 2039/2303   train_loss = 3.344\n",
      "Epoch  27 Batch 2059/2303   train_loss = 4.420\n",
      "Epoch  27 Batch 2079/2303   train_loss = 3.702\n",
      "Epoch  27 Batch 2099/2303   train_loss = 2.816\n",
      "Epoch  27 Batch 2119/2303   train_loss = 3.669\n",
      "Epoch  27 Batch 2139/2303   train_loss = 3.516\n",
      "Epoch  27 Batch 2159/2303   train_loss = 3.185\n",
      "Epoch  27 Batch 2179/2303   train_loss = 3.587\n",
      "Epoch  27 Batch 2199/2303   train_loss = 3.343\n",
      "Epoch  27 Batch 2219/2303   train_loss = 3.230\n",
      "Epoch  27 Batch 2239/2303   train_loss = 4.101\n",
      "Epoch  27 Batch 2259/2303   train_loss = 3.916\n",
      "Epoch  27 Batch 2279/2303   train_loss = 4.115\n",
      "Epoch  27 Batch 2299/2303   train_loss = 4.957\n",
      "Epoch  28 Batch   16/2303   train_loss = 3.825\n",
      "Epoch  28 Batch   36/2303   train_loss = 3.977\n",
      "Epoch  28 Batch   56/2303   train_loss = 3.548\n",
      "Epoch  28 Batch   76/2303   train_loss = 3.685\n",
      "Epoch  28 Batch   96/2303   train_loss = 4.256\n",
      "Epoch  28 Batch  116/2303   train_loss = 3.565\n",
      "Epoch  28 Batch  136/2303   train_loss = 4.227\n",
      "Epoch  28 Batch  156/2303   train_loss = 3.208\n",
      "Epoch  28 Batch  176/2303   train_loss = 4.091\n",
      "Epoch  28 Batch  196/2303   train_loss = 4.700\n",
      "Epoch  28 Batch  216/2303   train_loss = 3.500\n",
      "Epoch  28 Batch  236/2303   train_loss = 3.665\n",
      "Epoch  28 Batch  256/2303   train_loss = 4.000\n",
      "Epoch  28 Batch  276/2303   train_loss = 4.562\n",
      "Epoch  28 Batch  296/2303   train_loss = 3.626\n",
      "Epoch  28 Batch  316/2303   train_loss = 3.662\n",
      "Epoch  28 Batch  336/2303   train_loss = 4.052\n",
      "Epoch  28 Batch  356/2303   train_loss = 3.362\n",
      "Epoch  28 Batch  376/2303   train_loss = 3.304\n",
      "Epoch  28 Batch  396/2303   train_loss = 3.597\n",
      "Epoch  28 Batch  416/2303   train_loss = 3.787\n",
      "Epoch  28 Batch  436/2303   train_loss = 3.994\n",
      "Epoch  28 Batch  456/2303   train_loss = 4.453\n",
      "Epoch  28 Batch  476/2303   train_loss = 4.744\n",
      "Epoch  28 Batch  496/2303   train_loss = 4.215\n",
      "Epoch  28 Batch  516/2303   train_loss = 3.827\n",
      "Epoch  28 Batch  536/2303   train_loss = 4.740\n",
      "Epoch  28 Batch  556/2303   train_loss = 4.034\n",
      "Epoch  28 Batch  576/2303   train_loss = 3.554\n",
      "Epoch  28 Batch  596/2303   train_loss = 3.903\n",
      "Epoch  28 Batch  616/2303   train_loss = 2.869\n",
      "Epoch  28 Batch  636/2303   train_loss = 4.475\n",
      "Epoch  28 Batch  656/2303   train_loss = 3.561\n",
      "Epoch  28 Batch  676/2303   train_loss = 3.648\n",
      "Epoch  28 Batch  696/2303   train_loss = 4.000\n",
      "Epoch  28 Batch  716/2303   train_loss = 4.002\n",
      "Epoch  28 Batch  736/2303   train_loss = 3.012\n",
      "Epoch  28 Batch  756/2303   train_loss = 3.343\n",
      "Epoch  28 Batch  776/2303   train_loss = 3.182\n",
      "Epoch  28 Batch  796/2303   train_loss = 3.531\n",
      "Epoch  28 Batch  816/2303   train_loss = 3.433\n",
      "Epoch  28 Batch  836/2303   train_loss = 4.181\n",
      "Epoch  28 Batch  856/2303   train_loss = 4.279\n",
      "Epoch  28 Batch  876/2303   train_loss = 3.780\n",
      "Epoch  28 Batch  896/2303   train_loss = 3.629\n",
      "Epoch  28 Batch  916/2303   train_loss = 2.924\n",
      "Epoch  28 Batch  936/2303   train_loss = 4.498\n",
      "Epoch  28 Batch  956/2303   train_loss = 3.694\n",
      "Epoch  28 Batch  976/2303   train_loss = 3.295\n",
      "Epoch  28 Batch  996/2303   train_loss = 3.673\n",
      "Epoch  28 Batch 1016/2303   train_loss = 3.069\n",
      "Epoch  28 Batch 1036/2303   train_loss = 3.440\n",
      "Epoch  28 Batch 1056/2303   train_loss = 3.294\n",
      "Epoch  28 Batch 1076/2303   train_loss = 3.694\n",
      "Epoch  28 Batch 1096/2303   train_loss = 3.535\n",
      "Epoch  28 Batch 1116/2303   train_loss = 3.602\n",
      "Epoch  28 Batch 1136/2303   train_loss = 3.745\n",
      "Epoch  28 Batch 1156/2303   train_loss = 3.794\n",
      "Epoch  28 Batch 1176/2303   train_loss = 4.048\n",
      "Epoch  28 Batch 1196/2303   train_loss = 3.690\n",
      "Epoch  28 Batch 1216/2303   train_loss = 3.689\n",
      "Epoch  28 Batch 1236/2303   train_loss = 3.563\n",
      "Epoch  28 Batch 1256/2303   train_loss = 3.874\n",
      "Epoch  28 Batch 1276/2303   train_loss = 3.693\n",
      "Epoch  28 Batch 1296/2303   train_loss = 3.635\n",
      "Epoch  28 Batch 1316/2303   train_loss = 4.521\n",
      "Epoch  28 Batch 1336/2303   train_loss = 3.976\n",
      "Epoch  28 Batch 1356/2303   train_loss = 3.444\n",
      "Epoch  28 Batch 1376/2303   train_loss = 4.573\n",
      "Epoch  28 Batch 1396/2303   train_loss = 3.320\n",
      "Epoch  28 Batch 1416/2303   train_loss = 2.930\n",
      "Epoch  28 Batch 1436/2303   train_loss = 3.543\n",
      "Epoch  28 Batch 1456/2303   train_loss = 4.167\n",
      "Epoch  28 Batch 1476/2303   train_loss = 3.093\n",
      "Epoch  28 Batch 1496/2303   train_loss = 3.174\n",
      "Epoch  28 Batch 1516/2303   train_loss = 4.643\n",
      "Epoch  28 Batch 1536/2303   train_loss = 3.851\n",
      "Epoch  28 Batch 1556/2303   train_loss = 4.164\n",
      "Epoch  28 Batch 1576/2303   train_loss = 3.649\n",
      "Epoch  28 Batch 1596/2303   train_loss = 4.016\n",
      "Epoch  28 Batch 1616/2303   train_loss = 4.018\n",
      "Epoch  28 Batch 1636/2303   train_loss = 4.388\n",
      "Epoch  28 Batch 1656/2303   train_loss = 4.267\n",
      "Epoch  28 Batch 1676/2303   train_loss = 4.457\n",
      "Epoch  28 Batch 1696/2303   train_loss = 4.385\n",
      "Epoch  28 Batch 1716/2303   train_loss = 3.554\n",
      "Epoch  28 Batch 1736/2303   train_loss = 3.735\n",
      "Epoch  28 Batch 1756/2303   train_loss = 3.380\n",
      "Epoch  28 Batch 1776/2303   train_loss = 4.275\n",
      "Epoch  28 Batch 1796/2303   train_loss = 3.856\n",
      "Epoch  28 Batch 1816/2303   train_loss = 4.415\n",
      "Epoch  28 Batch 1836/2303   train_loss = 3.650\n",
      "Epoch  28 Batch 1856/2303   train_loss = 3.717\n",
      "Epoch  28 Batch 1876/2303   train_loss = 4.230\n",
      "Epoch  28 Batch 1896/2303   train_loss = 3.929\n",
      "Epoch  28 Batch 1916/2303   train_loss = 3.638\n",
      "Epoch  28 Batch 1936/2303   train_loss = 3.619\n",
      "Epoch  28 Batch 1956/2303   train_loss = 3.780\n",
      "Epoch  28 Batch 1976/2303   train_loss = 3.535\n",
      "Epoch  28 Batch 1996/2303   train_loss = 3.421\n",
      "Epoch  28 Batch 2016/2303   train_loss = 2.901\n",
      "Epoch  28 Batch 2036/2303   train_loss = 3.765\n",
      "Epoch  28 Batch 2056/2303   train_loss = 3.245\n",
      "Epoch  28 Batch 2076/2303   train_loss = 3.609\n",
      "Epoch  28 Batch 2096/2303   train_loss = 2.613\n",
      "Epoch  28 Batch 2116/2303   train_loss = 3.827\n",
      "Epoch  28 Batch 2136/2303   train_loss = 3.324\n",
      "Epoch  28 Batch 2156/2303   train_loss = 3.624\n",
      "Epoch  28 Batch 2176/2303   train_loss = 4.233\n",
      "Epoch  28 Batch 2196/2303   train_loss = 4.872\n",
      "Epoch  28 Batch 2216/2303   train_loss = 4.030\n",
      "Epoch  28 Batch 2236/2303   train_loss = 3.808\n",
      "Epoch  28 Batch 2256/2303   train_loss = 3.845\n",
      "Epoch  28 Batch 2276/2303   train_loss = 4.164\n",
      "Epoch  28 Batch 2296/2303   train_loss = 3.152\n",
      "Epoch  29 Batch   13/2303   train_loss = 2.479\n",
      "Epoch  29 Batch   33/2303   train_loss = 4.819\n",
      "Epoch  29 Batch   53/2303   train_loss = 3.703\n",
      "Epoch  29 Batch   73/2303   train_loss = 4.903\n",
      "Epoch  29 Batch   93/2303   train_loss = 4.029\n",
      "Epoch  29 Batch  113/2303   train_loss = 3.109\n",
      "Epoch  29 Batch  133/2303   train_loss = 4.965\n",
      "Epoch  29 Batch  153/2303   train_loss = 4.162\n",
      "Epoch  29 Batch  173/2303   train_loss = 3.177\n",
      "Epoch  29 Batch  193/2303   train_loss = 2.947\n",
      "Epoch  29 Batch  213/2303   train_loss = 4.317\n",
      "Epoch  29 Batch  233/2303   train_loss = 3.634\n",
      "Epoch  29 Batch  253/2303   train_loss = 3.263\n",
      "Epoch  29 Batch  273/2303   train_loss = 4.025\n",
      "Epoch  29 Batch  293/2303   train_loss = 4.629\n",
      "Epoch  29 Batch  313/2303   train_loss = 3.489\n",
      "Epoch  29 Batch  333/2303   train_loss = 3.963\n",
      "Epoch  29 Batch  353/2303   train_loss = 3.341\n",
      "Epoch  29 Batch  373/2303   train_loss = 4.135\n",
      "Epoch  29 Batch  393/2303   train_loss = 3.971\n",
      "Epoch  29 Batch  413/2303   train_loss = 3.769\n",
      "Epoch  29 Batch  433/2303   train_loss = 4.063\n",
      "Epoch  29 Batch  453/2303   train_loss = 3.999\n",
      "Epoch  29 Batch  473/2303   train_loss = 3.234\n",
      "Epoch  29 Batch  493/2303   train_loss = 4.382\n",
      "Epoch  29 Batch  513/2303   train_loss = 3.523\n",
      "Epoch  29 Batch  533/2303   train_loss = 3.194\n",
      "Epoch  29 Batch  553/2303   train_loss = 3.711\n",
      "Epoch  29 Batch  573/2303   train_loss = 3.910\n",
      "Epoch  29 Batch  593/2303   train_loss = 3.302\n",
      "Epoch  29 Batch  613/2303   train_loss = 3.527\n",
      "Epoch  29 Batch  633/2303   train_loss = 3.851\n",
      "Epoch  29 Batch  653/2303   train_loss = 3.476\n",
      "Epoch  29 Batch  673/2303   train_loss = 4.033\n",
      "Epoch  29 Batch  693/2303   train_loss = 3.747\n",
      "Epoch  29 Batch  713/2303   train_loss = 4.050\n",
      "Epoch  29 Batch  733/2303   train_loss = 3.137\n",
      "Epoch  29 Batch  753/2303   train_loss = 3.788\n",
      "Epoch  29 Batch  773/2303   train_loss = 4.255\n",
      "Epoch  29 Batch  793/2303   train_loss = 4.132\n",
      "Epoch  29 Batch  813/2303   train_loss = 3.739\n",
      "Epoch  29 Batch  833/2303   train_loss = 3.677\n",
      "Epoch  29 Batch  853/2303   train_loss = 5.514\n",
      "Epoch  29 Batch  873/2303   train_loss = 3.849\n",
      "Epoch  29 Batch  893/2303   train_loss = 3.964\n",
      "Epoch  29 Batch  913/2303   train_loss = 3.662\n",
      "Epoch  29 Batch  933/2303   train_loss = 3.713\n",
      "Epoch  29 Batch  953/2303   train_loss = 3.331\n",
      "Epoch  29 Batch  973/2303   train_loss = 4.249\n",
      "Epoch  29 Batch  993/2303   train_loss = 3.353\n",
      "Epoch  29 Batch 1013/2303   train_loss = 3.521\n",
      "Epoch  29 Batch 1033/2303   train_loss = 3.900\n",
      "Epoch  29 Batch 1053/2303   train_loss = 4.022\n",
      "Epoch  29 Batch 1073/2303   train_loss = 3.811\n",
      "Epoch  29 Batch 1093/2303   train_loss = 3.713\n",
      "Epoch  29 Batch 1113/2303   train_loss = 3.953\n",
      "Epoch  29 Batch 1133/2303   train_loss = 3.875\n",
      "Epoch  29 Batch 1153/2303   train_loss = 3.215\n",
      "Epoch  29 Batch 1173/2303   train_loss = 3.902\n",
      "Epoch  29 Batch 1193/2303   train_loss = 4.218\n",
      "Epoch  29 Batch 1213/2303   train_loss = 3.573\n",
      "Epoch  29 Batch 1233/2303   train_loss = 3.662\n",
      "Epoch  29 Batch 1253/2303   train_loss = 4.239\n",
      "Epoch  29 Batch 1273/2303   train_loss = 3.843\n",
      "Epoch  29 Batch 1293/2303   train_loss = 3.807\n",
      "Epoch  29 Batch 1313/2303   train_loss = 3.087\n",
      "Epoch  29 Batch 1333/2303   train_loss = 3.829\n",
      "Epoch  29 Batch 1353/2303   train_loss = 3.417\n",
      "Epoch  29 Batch 1373/2303   train_loss = 3.363\n",
      "Epoch  29 Batch 1393/2303   train_loss = 3.327\n",
      "Epoch  29 Batch 1413/2303   train_loss = 4.247\n",
      "Epoch  29 Batch 1433/2303   train_loss = 4.691\n",
      "Epoch  29 Batch 1453/2303   train_loss = 3.551\n",
      "Epoch  29 Batch 1473/2303   train_loss = 4.909\n",
      "Epoch  29 Batch 1493/2303   train_loss = 3.887\n",
      "Epoch  29 Batch 1513/2303   train_loss = 3.481\n",
      "Epoch  29 Batch 1533/2303   train_loss = 4.527\n",
      "Epoch  29 Batch 1553/2303   train_loss = 3.672\n",
      "Epoch  29 Batch 1573/2303   train_loss = 4.188\n",
      "Epoch  29 Batch 1593/2303   train_loss = 3.979\n",
      "Epoch  29 Batch 1613/2303   train_loss = 3.407\n",
      "Epoch  29 Batch 1633/2303   train_loss = 3.583\n",
      "Epoch  29 Batch 1653/2303   train_loss = 2.767\n",
      "Epoch  29 Batch 1673/2303   train_loss = 3.734\n",
      "Epoch  29 Batch 1693/2303   train_loss = 3.271\n",
      "Epoch  29 Batch 1713/2303   train_loss = 3.667\n",
      "Epoch  29 Batch 1733/2303   train_loss = 4.106\n",
      "Epoch  29 Batch 1753/2303   train_loss = 3.597\n",
      "Epoch  29 Batch 1773/2303   train_loss = 2.831\n",
      "Epoch  29 Batch 1793/2303   train_loss = 3.713\n",
      "Epoch  29 Batch 1813/2303   train_loss = 2.930\n",
      "Epoch  29 Batch 1833/2303   train_loss = 3.909\n",
      "Epoch  29 Batch 1853/2303   train_loss = 3.289\n",
      "Epoch  29 Batch 1873/2303   train_loss = 4.067\n",
      "Epoch  29 Batch 1893/2303   train_loss = 4.091\n",
      "Epoch  29 Batch 1913/2303   train_loss = 4.564\n",
      "Epoch  29 Batch 1933/2303   train_loss = 3.961\n",
      "Epoch  29 Batch 1953/2303   train_loss = 4.888\n",
      "Epoch  29 Batch 1973/2303   train_loss = 3.887\n",
      "Epoch  29 Batch 1993/2303   train_loss = 3.841\n",
      "Epoch  29 Batch 2013/2303   train_loss = 4.116\n",
      "Epoch  29 Batch 2033/2303   train_loss = 4.609\n",
      "Epoch  29 Batch 2053/2303   train_loss = 3.966\n",
      "Epoch  29 Batch 2073/2303   train_loss = 4.101\n",
      "Epoch  29 Batch 2093/2303   train_loss = 4.061\n",
      "Epoch  29 Batch 2113/2303   train_loss = 3.799\n",
      "Epoch  29 Batch 2133/2303   train_loss = 3.756\n",
      "Epoch  29 Batch 2153/2303   train_loss = 3.796\n",
      "Epoch  29 Batch 2173/2303   train_loss = 3.973\n",
      "Epoch  29 Batch 2193/2303   train_loss = 3.777\n",
      "Epoch  29 Batch 2213/2303   train_loss = 3.954\n",
      "Epoch  29 Batch 2233/2303   train_loss = 3.500\n",
      "Epoch  29 Batch 2253/2303   train_loss = 3.787\n",
      "Epoch  29 Batch 2273/2303   train_loss = 4.271\n",
      "Epoch  29 Batch 2293/2303   train_loss = 3.454\n",
      "Epoch  30 Batch   10/2303   train_loss = 3.831\n",
      "Epoch  30 Batch   30/2303   train_loss = 3.662\n",
      "Epoch  30 Batch   50/2303   train_loss = 3.762\n",
      "Epoch  30 Batch   70/2303   train_loss = 3.940\n",
      "Epoch  30 Batch   90/2303   train_loss = 4.316\n",
      "Epoch  30 Batch  110/2303   train_loss = 4.175\n",
      "Epoch  30 Batch  130/2303   train_loss = 3.696\n",
      "Epoch  30 Batch  150/2303   train_loss = 3.752\n",
      "Epoch  30 Batch  170/2303   train_loss = 4.007\n",
      "Epoch  30 Batch  190/2303   train_loss = 4.705\n",
      "Epoch  30 Batch  210/2303   train_loss = 3.150\n",
      "Epoch  30 Batch  230/2303   train_loss = 3.245\n",
      "Epoch  30 Batch  250/2303   train_loss = 2.836\n",
      "Epoch  30 Batch  270/2303   train_loss = 4.181\n",
      "Epoch  30 Batch  290/2303   train_loss = 3.347\n",
      "Epoch  30 Batch  310/2303   train_loss = 4.482\n",
      "Epoch  30 Batch  330/2303   train_loss = 4.513\n",
      "Epoch  30 Batch  350/2303   train_loss = 4.399\n",
      "Epoch  30 Batch  370/2303   train_loss = 3.825\n",
      "Epoch  30 Batch  390/2303   train_loss = 4.088\n",
      "Epoch  30 Batch  410/2303   train_loss = 3.952\n",
      "Epoch  30 Batch  430/2303   train_loss = 2.978\n",
      "Epoch  30 Batch  450/2303   train_loss = 3.195\n",
      "Epoch  30 Batch  470/2303   train_loss = 3.818\n",
      "Epoch  30 Batch  490/2303   train_loss = 3.868\n",
      "Epoch  30 Batch  510/2303   train_loss = 3.645\n",
      "Epoch  30 Batch  530/2303   train_loss = 4.316\n",
      "Epoch  30 Batch  550/2303   train_loss = 3.774\n",
      "Epoch  30 Batch  570/2303   train_loss = 3.869\n",
      "Epoch  30 Batch  590/2303   train_loss = 3.806\n",
      "Epoch  30 Batch  610/2303   train_loss = 3.501\n",
      "Epoch  30 Batch  630/2303   train_loss = 4.100\n",
      "Epoch  30 Batch  650/2303   train_loss = 2.886\n",
      "Epoch  30 Batch  670/2303   train_loss = 3.619\n",
      "Epoch  30 Batch  690/2303   train_loss = 4.096\n",
      "Epoch  30 Batch  710/2303   train_loss = 4.006\n",
      "Epoch  30 Batch  730/2303   train_loss = 3.697\n",
      "Epoch  30 Batch  750/2303   train_loss = 3.795\n",
      "Epoch  30 Batch  770/2303   train_loss = 3.918\n",
      "Epoch  30 Batch  790/2303   train_loss = 2.762\n",
      "Epoch  30 Batch  810/2303   train_loss = 3.734\n",
      "Epoch  30 Batch  830/2303   train_loss = 3.244\n",
      "Epoch  30 Batch  850/2303   train_loss = 3.627\n",
      "Epoch  30 Batch  870/2303   train_loss = 2.998\n",
      "Epoch  30 Batch  890/2303   train_loss = 4.022\n",
      "Epoch  30 Batch  910/2303   train_loss = 3.413\n",
      "Epoch  30 Batch  930/2303   train_loss = 3.463\n",
      "Epoch  30 Batch  950/2303   train_loss = 3.391\n",
      "Epoch  30 Batch  970/2303   train_loss = 3.295\n",
      "Epoch  30 Batch  990/2303   train_loss = 3.514\n",
      "Epoch  30 Batch 1010/2303   train_loss = 3.352\n",
      "Epoch  30 Batch 1030/2303   train_loss = 3.100\n",
      "Epoch  30 Batch 1050/2303   train_loss = 3.976\n",
      "Epoch  30 Batch 1070/2303   train_loss = 4.152\n",
      "Epoch  30 Batch 1090/2303   train_loss = 4.715\n",
      "Epoch  30 Batch 1110/2303   train_loss = 3.027\n",
      "Epoch  30 Batch 1130/2303   train_loss = 3.803\n",
      "Epoch  30 Batch 1150/2303   train_loss = 4.099\n",
      "Epoch  30 Batch 1170/2303   train_loss = 4.395\n",
      "Epoch  30 Batch 1190/2303   train_loss = 3.269\n",
      "Epoch  30 Batch 1210/2303   train_loss = 3.914\n",
      "Epoch  30 Batch 1230/2303   train_loss = 3.457\n",
      "Epoch  30 Batch 1250/2303   train_loss = 3.445\n",
      "Epoch  30 Batch 1270/2303   train_loss = 3.488\n",
      "Epoch  30 Batch 1290/2303   train_loss = 3.308\n",
      "Epoch  30 Batch 1310/2303   train_loss = 4.352\n",
      "Epoch  30 Batch 1330/2303   train_loss = 4.572\n",
      "Epoch  30 Batch 1350/2303   train_loss = 3.407\n",
      "Epoch  30 Batch 1370/2303   train_loss = 3.905\n",
      "Epoch  30 Batch 1390/2303   train_loss = 2.969\n",
      "Epoch  30 Batch 1410/2303   train_loss = 4.466\n",
      "Epoch  30 Batch 1430/2303   train_loss = 4.128\n",
      "Epoch  30 Batch 1450/2303   train_loss = 3.742\n",
      "Epoch  30 Batch 1470/2303   train_loss = 3.271\n",
      "Epoch  30 Batch 1490/2303   train_loss = 3.966\n",
      "Epoch  30 Batch 1510/2303   train_loss = 4.056\n",
      "Epoch  30 Batch 1530/2303   train_loss = 4.370\n",
      "Epoch  30 Batch 1550/2303   train_loss = 4.609\n",
      "Epoch  30 Batch 1570/2303   train_loss = 3.549\n",
      "Epoch  30 Batch 1590/2303   train_loss = 3.760\n",
      "Epoch  30 Batch 1610/2303   train_loss = 2.892\n",
      "Epoch  30 Batch 1630/2303   train_loss = 3.475\n",
      "Epoch  30 Batch 1650/2303   train_loss = 4.192\n",
      "Epoch  30 Batch 1670/2303   train_loss = 3.775\n",
      "Epoch  30 Batch 1690/2303   train_loss = 4.503\n",
      "Epoch  30 Batch 1710/2303   train_loss = 3.274\n",
      "Epoch  30 Batch 1730/2303   train_loss = 4.012\n",
      "Epoch  30 Batch 1750/2303   train_loss = 2.910\n",
      "Epoch  30 Batch 1770/2303   train_loss = 4.000\n",
      "Epoch  30 Batch 1790/2303   train_loss = 4.147\n",
      "Epoch  30 Batch 1810/2303   train_loss = 3.931\n",
      "Epoch  30 Batch 1830/2303   train_loss = 3.478\n",
      "Epoch  30 Batch 1850/2303   train_loss = 4.473\n",
      "Epoch  30 Batch 1870/2303   train_loss = 3.159\n",
      "Epoch  30 Batch 1890/2303   train_loss = 4.208\n",
      "Epoch  30 Batch 1910/2303   train_loss = 4.395\n",
      "Epoch  30 Batch 1930/2303   train_loss = 3.375\n",
      "Epoch  30 Batch 1950/2303   train_loss = 3.692\n",
      "Epoch  30 Batch 1970/2303   train_loss = 4.075\n",
      "Epoch  30 Batch 1990/2303   train_loss = 3.942\n",
      "Epoch  30 Batch 2010/2303   train_loss = 3.465\n",
      "Epoch  30 Batch 2030/2303   train_loss = 4.061\n",
      "Epoch  30 Batch 2050/2303   train_loss = 4.106\n",
      "Epoch  30 Batch 2070/2303   train_loss = 3.475\n",
      "Epoch  30 Batch 2090/2303   train_loss = 3.523\n",
      "Epoch  30 Batch 2110/2303   train_loss = 4.413\n",
      "Epoch  30 Batch 2130/2303   train_loss = 3.316\n",
      "Epoch  30 Batch 2150/2303   train_loss = 3.426\n",
      "Epoch  30 Batch 2170/2303   train_loss = 4.164\n",
      "Epoch  30 Batch 2190/2303   train_loss = 3.969\n",
      "Epoch  30 Batch 2210/2303   train_loss = 3.359\n",
      "Epoch  30 Batch 2230/2303   train_loss = 4.432\n",
      "Epoch  30 Batch 2250/2303   train_loss = 2.725\n",
      "Epoch  30 Batch 2270/2303   train_loss = 3.320\n",
      "Epoch  30 Batch 2290/2303   train_loss = 2.961\n",
      "Epoch  31 Batch    7/2303   train_loss = 3.972\n",
      "Epoch  31 Batch   27/2303   train_loss = 3.699\n",
      "Epoch  31 Batch   47/2303   train_loss = 4.042\n",
      "Epoch  31 Batch   67/2303   train_loss = 4.203\n",
      "Epoch  31 Batch   87/2303   train_loss = 4.418\n",
      "Epoch  31 Batch  107/2303   train_loss = 4.117\n",
      "Epoch  31 Batch  127/2303   train_loss = 3.968\n",
      "Epoch  31 Batch  147/2303   train_loss = 3.122\n",
      "Epoch  31 Batch  167/2303   train_loss = 3.955\n",
      "Epoch  31 Batch  187/2303   train_loss = 3.072\n",
      "Epoch  31 Batch  207/2303   train_loss = 3.944\n",
      "Epoch  31 Batch  227/2303   train_loss = 3.710\n",
      "Epoch  31 Batch  247/2303   train_loss = 4.348\n",
      "Epoch  31 Batch  267/2303   train_loss = 3.413\n",
      "Epoch  31 Batch  287/2303   train_loss = 2.896\n",
      "Epoch  31 Batch  307/2303   train_loss = 3.044\n",
      "Epoch  31 Batch  327/2303   train_loss = 3.290\n",
      "Epoch  31 Batch  347/2303   train_loss = 4.358\n",
      "Epoch  31 Batch  367/2303   train_loss = 3.175\n",
      "Epoch  31 Batch  387/2303   train_loss = 3.076\n",
      "Epoch  31 Batch  407/2303   train_loss = 3.434\n",
      "Epoch  31 Batch  427/2303   train_loss = 2.949\n",
      "Epoch  31 Batch  447/2303   train_loss = 3.565\n",
      "Epoch  31 Batch  467/2303   train_loss = 2.861\n",
      "Epoch  31 Batch  487/2303   train_loss = 4.043\n",
      "Epoch  31 Batch  507/2303   train_loss = 3.363\n",
      "Epoch  31 Batch  527/2303   train_loss = 3.546\n",
      "Epoch  31 Batch  547/2303   train_loss = 3.904\n",
      "Epoch  31 Batch  567/2303   train_loss = 2.970\n",
      "Epoch  31 Batch  587/2303   train_loss = 3.745\n",
      "Epoch  31 Batch  607/2303   train_loss = 3.101\n",
      "Epoch  31 Batch  627/2303   train_loss = 3.329\n",
      "Epoch  31 Batch  647/2303   train_loss = 3.827\n",
      "Epoch  31 Batch  667/2303   train_loss = 4.815\n",
      "Epoch  31 Batch  687/2303   train_loss = 3.349\n",
      "Epoch  31 Batch  707/2303   train_loss = 3.695\n",
      "Epoch  31 Batch  727/2303   train_loss = 3.716\n",
      "Epoch  31 Batch  747/2303   train_loss = 2.958\n",
      "Epoch  31 Batch  767/2303   train_loss = 3.941\n",
      "Epoch  31 Batch  787/2303   train_loss = 3.964\n",
      "Epoch  31 Batch  807/2303   train_loss = 3.487\n",
      "Epoch  31 Batch  827/2303   train_loss = 3.836\n",
      "Epoch  31 Batch  847/2303   train_loss = 4.012\n",
      "Epoch  31 Batch  867/2303   train_loss = 3.522\n",
      "Epoch  31 Batch  887/2303   train_loss = 3.700\n",
      "Epoch  31 Batch  907/2303   train_loss = 3.869\n",
      "Epoch  31 Batch  927/2303   train_loss = 3.921\n",
      "Epoch  31 Batch  947/2303   train_loss = 3.713\n",
      "Epoch  31 Batch  967/2303   train_loss = 2.968\n",
      "Epoch  31 Batch  987/2303   train_loss = 4.379\n",
      "Epoch  31 Batch 1007/2303   train_loss = 3.427\n",
      "Epoch  31 Batch 1027/2303   train_loss = 3.294\n",
      "Epoch  31 Batch 1047/2303   train_loss = 3.328\n",
      "Epoch  31 Batch 1067/2303   train_loss = 3.288\n",
      "Epoch  31 Batch 1087/2303   train_loss = 2.998\n",
      "Epoch  31 Batch 1107/2303   train_loss = 4.479\n",
      "Epoch  31 Batch 1127/2303   train_loss = 3.366\n",
      "Epoch  31 Batch 1147/2303   train_loss = 4.342\n",
      "Epoch  31 Batch 1167/2303   train_loss = 3.500\n",
      "Epoch  31 Batch 1187/2303   train_loss = 3.303\n",
      "Epoch  31 Batch 1207/2303   train_loss = 3.865\n",
      "Epoch  31 Batch 1227/2303   train_loss = 4.499\n",
      "Epoch  31 Batch 1247/2303   train_loss = 4.363\n",
      "Epoch  31 Batch 1267/2303   train_loss = 3.438\n",
      "Epoch  31 Batch 1287/2303   train_loss = 3.875\n",
      "Epoch  31 Batch 1307/2303   train_loss = 3.557\n",
      "Epoch  31 Batch 1327/2303   train_loss = 4.032\n",
      "Epoch  31 Batch 1347/2303   train_loss = 3.842\n",
      "Epoch  31 Batch 1367/2303   train_loss = 4.913\n",
      "Epoch  31 Batch 1387/2303   train_loss = 3.746\n",
      "Epoch  31 Batch 1407/2303   train_loss = 3.754\n",
      "Epoch  31 Batch 1427/2303   train_loss = 3.376\n",
      "Epoch  31 Batch 1447/2303   train_loss = 3.788\n",
      "Epoch  31 Batch 1467/2303   train_loss = 3.595\n",
      "Epoch  31 Batch 1487/2303   train_loss = 3.918\n",
      "Epoch  31 Batch 1507/2303   train_loss = 4.119\n",
      "Epoch  31 Batch 1527/2303   train_loss = 4.133\n",
      "Epoch  31 Batch 1547/2303   train_loss = 2.733\n",
      "Epoch  31 Batch 1567/2303   train_loss = 3.892\n",
      "Epoch  31 Batch 1587/2303   train_loss = 2.960\n",
      "Epoch  31 Batch 1607/2303   train_loss = 3.489\n",
      "Epoch  31 Batch 1627/2303   train_loss = 3.521\n",
      "Epoch  31 Batch 1647/2303   train_loss = 4.341\n",
      "Epoch  31 Batch 1667/2303   train_loss = 3.808\n",
      "Epoch  31 Batch 1687/2303   train_loss = 4.679\n",
      "Epoch  31 Batch 1707/2303   train_loss = 4.136\n",
      "Epoch  31 Batch 1727/2303   train_loss = 3.898\n",
      "Epoch  31 Batch 1747/2303   train_loss = 3.330\n",
      "Epoch  31 Batch 1767/2303   train_loss = 2.644\n",
      "Epoch  31 Batch 1787/2303   train_loss = 3.695\n",
      "Epoch  31 Batch 1807/2303   train_loss = 3.788\n",
      "Epoch  31 Batch 1827/2303   train_loss = 3.777\n",
      "Epoch  31 Batch 1847/2303   train_loss = 3.754\n",
      "Epoch  31 Batch 1867/2303   train_loss = 3.706\n",
      "Epoch  31 Batch 1887/2303   train_loss = 3.457\n",
      "Epoch  31 Batch 1907/2303   train_loss = 3.444\n",
      "Epoch  31 Batch 1927/2303   train_loss = 4.043\n",
      "Epoch  31 Batch 1947/2303   train_loss = 3.853\n",
      "Epoch  31 Batch 1967/2303   train_loss = 3.473\n",
      "Epoch  31 Batch 1987/2303   train_loss = 3.601\n",
      "Epoch  31 Batch 2007/2303   train_loss = 3.770\n",
      "Epoch  31 Batch 2027/2303   train_loss = 3.706\n",
      "Epoch  31 Batch 2047/2303   train_loss = 4.318\n",
      "Epoch  31 Batch 2067/2303   train_loss = 3.897\n",
      "Epoch  31 Batch 2087/2303   train_loss = 3.708\n",
      "Epoch  31 Batch 2107/2303   train_loss = 3.696\n",
      "Epoch  31 Batch 2127/2303   train_loss = 3.542\n",
      "Epoch  31 Batch 2147/2303   train_loss = 3.044\n",
      "Epoch  31 Batch 2167/2303   train_loss = 4.174\n",
      "Epoch  31 Batch 2187/2303   train_loss = 3.904\n",
      "Epoch  31 Batch 2207/2303   train_loss = 3.504\n",
      "Epoch  31 Batch 2227/2303   train_loss = 3.929\n",
      "Epoch  31 Batch 2247/2303   train_loss = 3.133\n",
      "Epoch  31 Batch 2267/2303   train_loss = 5.048\n",
      "Epoch  31 Batch 2287/2303   train_loss = 4.481\n",
      "Epoch  32 Batch    4/2303   train_loss = 3.829\n",
      "Epoch  32 Batch   24/2303   train_loss = 3.389\n",
      "Epoch  32 Batch   44/2303   train_loss = 3.778\n",
      "Epoch  32 Batch   64/2303   train_loss = 3.919\n",
      "Epoch  32 Batch   84/2303   train_loss = 3.634\n",
      "Epoch  32 Batch  104/2303   train_loss = 3.793\n",
      "Epoch  32 Batch  124/2303   train_loss = 4.371\n",
      "Epoch  32 Batch  144/2303   train_loss = 3.745\n",
      "Epoch  32 Batch  164/2303   train_loss = 4.178\n",
      "Epoch  32 Batch  184/2303   train_loss = 3.659\n",
      "Epoch  32 Batch  204/2303   train_loss = 3.152\n",
      "Epoch  32 Batch  224/2303   train_loss = 2.861\n",
      "Epoch  32 Batch  244/2303   train_loss = 3.983\n",
      "Epoch  32 Batch  264/2303   train_loss = 4.218\n",
      "Epoch  32 Batch  284/2303   train_loss = 4.033\n",
      "Epoch  32 Batch  304/2303   train_loss = 4.096\n",
      "Epoch  32 Batch  324/2303   train_loss = 3.318\n",
      "Epoch  32 Batch  344/2303   train_loss = 3.210\n",
      "Epoch  32 Batch  364/2303   train_loss = 3.798\n",
      "Epoch  32 Batch  384/2303   train_loss = 4.072\n",
      "Epoch  32 Batch  404/2303   train_loss = 3.836\n",
      "Epoch  32 Batch  424/2303   train_loss = 4.004\n",
      "Epoch  32 Batch  444/2303   train_loss = 4.207\n",
      "Epoch  32 Batch  464/2303   train_loss = 3.665\n",
      "Epoch  32 Batch  484/2303   train_loss = 4.290\n",
      "Epoch  32 Batch  504/2303   train_loss = 2.877\n",
      "Epoch  32 Batch  524/2303   train_loss = 3.472\n",
      "Epoch  32 Batch  544/2303   train_loss = 4.242\n",
      "Epoch  32 Batch  564/2303   train_loss = 4.256\n",
      "Epoch  32 Batch  584/2303   train_loss = 3.553\n",
      "Epoch  32 Batch  604/2303   train_loss = 3.547\n",
      "Epoch  32 Batch  624/2303   train_loss = 2.927\n",
      "Epoch  32 Batch  644/2303   train_loss = 4.076\n",
      "Epoch  32 Batch  664/2303   train_loss = 3.541\n",
      "Epoch  32 Batch  684/2303   train_loss = 3.239\n",
      "Epoch  32 Batch  704/2303   train_loss = 3.304\n",
      "Epoch  32 Batch  724/2303   train_loss = 3.973\n",
      "Epoch  32 Batch  744/2303   train_loss = 4.283\n",
      "Epoch  32 Batch  764/2303   train_loss = 4.837\n",
      "Epoch  32 Batch  784/2303   train_loss = 3.034\n",
      "Epoch  32 Batch  804/2303   train_loss = 4.303\n",
      "Epoch  32 Batch  824/2303   train_loss = 3.379\n",
      "Epoch  32 Batch  844/2303   train_loss = 3.731\n",
      "Epoch  32 Batch  864/2303   train_loss = 3.289\n",
      "Epoch  32 Batch  884/2303   train_loss = 3.789\n",
      "Epoch  32 Batch  904/2303   train_loss = 4.158\n",
      "Epoch  32 Batch  924/2303   train_loss = 3.512\n",
      "Epoch  32 Batch  944/2303   train_loss = 3.186\n",
      "Epoch  32 Batch  964/2303   train_loss = 3.113\n",
      "Epoch  32 Batch  984/2303   train_loss = 3.595\n",
      "Epoch  32 Batch 1004/2303   train_loss = 4.099\n",
      "Epoch  32 Batch 1024/2303   train_loss = 3.247\n",
      "Epoch  32 Batch 1044/2303   train_loss = 3.951\n",
      "Epoch  32 Batch 1064/2303   train_loss = 3.271\n",
      "Epoch  32 Batch 1084/2303   train_loss = 3.502\n",
      "Epoch  32 Batch 1104/2303   train_loss = 4.739\n",
      "Epoch  32 Batch 1124/2303   train_loss = 3.576\n",
      "Epoch  32 Batch 1144/2303   train_loss = 3.246\n",
      "Epoch  32 Batch 1164/2303   train_loss = 3.534\n",
      "Epoch  32 Batch 1184/2303   train_loss = 3.915\n",
      "Epoch  32 Batch 1204/2303   train_loss = 3.687\n",
      "Epoch  32 Batch 1224/2303   train_loss = 3.688\n",
      "Epoch  32 Batch 1244/2303   train_loss = 3.643\n",
      "Epoch  32 Batch 1264/2303   train_loss = 3.835\n",
      "Epoch  32 Batch 1284/2303   train_loss = 3.643\n",
      "Epoch  32 Batch 1304/2303   train_loss = 3.414\n",
      "Epoch  32 Batch 1324/2303   train_loss = 3.882\n",
      "Epoch  32 Batch 1344/2303   train_loss = 4.430\n",
      "Epoch  32 Batch 1364/2303   train_loss = 3.475\n",
      "Epoch  32 Batch 1384/2303   train_loss = 4.062\n",
      "Epoch  32 Batch 1404/2303   train_loss = 4.691\n",
      "Epoch  32 Batch 1424/2303   train_loss = 4.136\n",
      "Epoch  32 Batch 1444/2303   train_loss = 4.270\n",
      "Epoch  32 Batch 1464/2303   train_loss = 3.929\n",
      "Epoch  32 Batch 1484/2303   train_loss = 3.495\n",
      "Epoch  32 Batch 1504/2303   train_loss = 4.350\n",
      "Epoch  32 Batch 1524/2303   train_loss = 2.785\n",
      "Epoch  32 Batch 1544/2303   train_loss = 3.410\n",
      "Epoch  32 Batch 1564/2303   train_loss = 3.350\n",
      "Epoch  32 Batch 1584/2303   train_loss = 4.011\n",
      "Epoch  32 Batch 1604/2303   train_loss = 4.391\n",
      "Epoch  32 Batch 1624/2303   train_loss = 4.292\n",
      "Epoch  32 Batch 1644/2303   train_loss = 4.469\n",
      "Epoch  32 Batch 1664/2303   train_loss = 4.035\n",
      "Epoch  32 Batch 1684/2303   train_loss = 3.797\n",
      "Epoch  32 Batch 1704/2303   train_loss = 4.567\n",
      "Epoch  32 Batch 1724/2303   train_loss = 3.840\n",
      "Epoch  32 Batch 1744/2303   train_loss = 4.211\n",
      "Epoch  32 Batch 1764/2303   train_loss = 4.189\n",
      "Epoch  32 Batch 1784/2303   train_loss = 3.011\n",
      "Epoch  32 Batch 1804/2303   train_loss = 3.614\n",
      "Epoch  32 Batch 1824/2303   train_loss = 3.692\n",
      "Epoch  32 Batch 1844/2303   train_loss = 3.260\n",
      "Epoch  32 Batch 1864/2303   train_loss = 3.984\n",
      "Epoch  32 Batch 1884/2303   train_loss = 3.583\n",
      "Epoch  32 Batch 1904/2303   train_loss = 3.735\n",
      "Epoch  32 Batch 1924/2303   train_loss = 4.326\n",
      "Epoch  32 Batch 1944/2303   train_loss = 3.637\n",
      "Epoch  32 Batch 1964/2303   train_loss = 2.908\n",
      "Epoch  32 Batch 1984/2303   train_loss = 3.295\n",
      "Epoch  32 Batch 2004/2303   train_loss = 4.320\n",
      "Epoch  32 Batch 2024/2303   train_loss = 3.951\n",
      "Epoch  32 Batch 2044/2303   train_loss = 4.019\n",
      "Epoch  32 Batch 2064/2303   train_loss = 3.710\n",
      "Epoch  32 Batch 2084/2303   train_loss = 4.229\n",
      "Epoch  32 Batch 2104/2303   train_loss = 4.265\n",
      "Epoch  32 Batch 2124/2303   train_loss = 5.105\n",
      "Epoch  32 Batch 2144/2303   train_loss = 3.656\n",
      "Epoch  32 Batch 2164/2303   train_loss = 4.312\n",
      "Epoch  32 Batch 2184/2303   train_loss = 3.460\n",
      "Epoch  32 Batch 2204/2303   train_loss = 3.687\n",
      "Epoch  32 Batch 2224/2303   train_loss = 3.695\n",
      "Epoch  32 Batch 2244/2303   train_loss = 4.196\n",
      "Epoch  32 Batch 2264/2303   train_loss = 3.108\n",
      "Epoch  32 Batch 2284/2303   train_loss = 3.365\n",
      "Epoch  33 Batch    1/2303   train_loss = 3.119\n",
      "Epoch  33 Batch   21/2303   train_loss = 3.140\n",
      "Epoch  33 Batch   41/2303   train_loss = 3.647\n",
      "Epoch  33 Batch   61/2303   train_loss = 3.542\n",
      "Epoch  33 Batch   81/2303   train_loss = 3.141\n",
      "Epoch  33 Batch  101/2303   train_loss = 4.499\n",
      "Epoch  33 Batch  121/2303   train_loss = 4.308\n",
      "Epoch  33 Batch  141/2303   train_loss = 2.877\n",
      "Epoch  33 Batch  161/2303   train_loss = 3.365\n",
      "Epoch  33 Batch  181/2303   train_loss = 3.640\n",
      "Epoch  33 Batch  201/2303   train_loss = 4.149\n",
      "Epoch  33 Batch  221/2303   train_loss = 3.550\n",
      "Epoch  33 Batch  241/2303   train_loss = 3.977\n",
      "Epoch  33 Batch  261/2303   train_loss = 3.666\n",
      "Epoch  33 Batch  281/2303   train_loss = 3.233\n",
      "Epoch  33 Batch  301/2303   train_loss = 3.354\n",
      "Epoch  33 Batch  321/2303   train_loss = 4.560\n",
      "Epoch  33 Batch  341/2303   train_loss = 4.300\n",
      "Epoch  33 Batch  361/2303   train_loss = 3.541\n",
      "Epoch  33 Batch  381/2303   train_loss = 3.934\n",
      "Epoch  33 Batch  401/2303   train_loss = 3.613\n",
      "Epoch  33 Batch  421/2303   train_loss = 3.576\n",
      "Epoch  33 Batch  441/2303   train_loss = 2.488\n",
      "Epoch  33 Batch  461/2303   train_loss = 3.974\n",
      "Epoch  33 Batch  481/2303   train_loss = 3.504\n",
      "Epoch  33 Batch  501/2303   train_loss = 3.984\n",
      "Epoch  33 Batch  521/2303   train_loss = 4.203\n",
      "Epoch  33 Batch  541/2303   train_loss = 3.008\n",
      "Epoch  33 Batch  561/2303   train_loss = 3.636\n",
      "Epoch  33 Batch  581/2303   train_loss = 3.029\n",
      "Epoch  33 Batch  601/2303   train_loss = 4.302\n",
      "Epoch  33 Batch  621/2303   train_loss = 3.846\n",
      "Epoch  33 Batch  641/2303   train_loss = 3.826\n",
      "Epoch  33 Batch  661/2303   train_loss = 3.915\n",
      "Epoch  33 Batch  681/2303   train_loss = 4.141\n",
      "Epoch  33 Batch  701/2303   train_loss = 3.961\n",
      "Epoch  33 Batch  721/2303   train_loss = 3.557\n",
      "Epoch  33 Batch  741/2303   train_loss = 3.761\n",
      "Epoch  33 Batch  761/2303   train_loss = 3.942\n",
      "Epoch  33 Batch  781/2303   train_loss = 3.269\n",
      "Epoch  33 Batch  801/2303   train_loss = 3.859\n",
      "Epoch  33 Batch  821/2303   train_loss = 3.799\n",
      "Epoch  33 Batch  841/2303   train_loss = 4.160\n",
      "Epoch  33 Batch  861/2303   train_loss = 3.656\n",
      "Epoch  33 Batch  881/2303   train_loss = 3.455\n",
      "Epoch  33 Batch  901/2303   train_loss = 3.321\n",
      "Epoch  33 Batch  921/2303   train_loss = 3.341\n",
      "Epoch  33 Batch  941/2303   train_loss = 3.100\n",
      "Epoch  33 Batch  961/2303   train_loss = 3.317\n",
      "Epoch  33 Batch  981/2303   train_loss = 3.710\n",
      "Epoch  33 Batch 1001/2303   train_loss = 3.841\n",
      "Epoch  33 Batch 1021/2303   train_loss = 4.110\n",
      "Epoch  33 Batch 1041/2303   train_loss = 4.177\n",
      "Epoch  33 Batch 1061/2303   train_loss = 3.508\n",
      "Epoch  33 Batch 1081/2303   train_loss = 3.498\n",
      "Epoch  33 Batch 1101/2303   train_loss = 3.429\n",
      "Epoch  33 Batch 1121/2303   train_loss = 3.724\n",
      "Epoch  33 Batch 1141/2303   train_loss = 3.766\n",
      "Epoch  33 Batch 1161/2303   train_loss = 3.581\n",
      "Epoch  33 Batch 1181/2303   train_loss = 3.792\n",
      "Epoch  33 Batch 1201/2303   train_loss = 3.960\n",
      "Epoch  33 Batch 1221/2303   train_loss = 3.304\n",
      "Epoch  33 Batch 1241/2303   train_loss = 4.418\n",
      "Epoch  33 Batch 1261/2303   train_loss = 4.126\n",
      "Epoch  33 Batch 1281/2303   train_loss = 3.995\n",
      "Epoch  33 Batch 1301/2303   train_loss = 3.659\n",
      "Epoch  33 Batch 1321/2303   train_loss = 3.850\n",
      "Epoch  33 Batch 1341/2303   train_loss = 3.938\n",
      "Epoch  33 Batch 1361/2303   train_loss = 3.828\n",
      "Epoch  33 Batch 1381/2303   train_loss = 4.401\n",
      "Epoch  33 Batch 1401/2303   train_loss = 3.276\n",
      "Epoch  33 Batch 1421/2303   train_loss = 3.709\n",
      "Epoch  33 Batch 1441/2303   train_loss = 3.383\n",
      "Epoch  33 Batch 1461/2303   train_loss = 2.918\n",
      "Epoch  33 Batch 1481/2303   train_loss = 3.697\n",
      "Epoch  33 Batch 1501/2303   train_loss = 3.842\n",
      "Epoch  33 Batch 1521/2303   train_loss = 3.938\n",
      "Epoch  33 Batch 1541/2303   train_loss = 4.189\n",
      "Epoch  33 Batch 1561/2303   train_loss = 4.033\n",
      "Epoch  33 Batch 1581/2303   train_loss = 3.910\n",
      "Epoch  33 Batch 1601/2303   train_loss = 3.910\n",
      "Epoch  33 Batch 1621/2303   train_loss = 3.727\n",
      "Epoch  33 Batch 1641/2303   train_loss = 3.301\n",
      "Epoch  33 Batch 1661/2303   train_loss = 4.161\n",
      "Epoch  33 Batch 1681/2303   train_loss = 3.321\n",
      "Epoch  33 Batch 1701/2303   train_loss = 4.145\n",
      "Epoch  33 Batch 1721/2303   train_loss = 3.716\n",
      "Epoch  33 Batch 1741/2303   train_loss = 3.691\n",
      "Epoch  33 Batch 1761/2303   train_loss = 4.009\n",
      "Epoch  33 Batch 1781/2303   train_loss = 3.039\n",
      "Epoch  33 Batch 1801/2303   train_loss = 3.851\n",
      "Epoch  33 Batch 1821/2303   train_loss = 2.600\n",
      "Epoch  33 Batch 1841/2303   train_loss = 3.285\n",
      "Epoch  33 Batch 1861/2303   train_loss = 3.413\n",
      "Epoch  33 Batch 1881/2303   train_loss = 4.422\n",
      "Epoch  33 Batch 1901/2303   train_loss = 3.378\n",
      "Epoch  33 Batch 1921/2303   train_loss = 3.251\n",
      "Epoch  33 Batch 1941/2303   train_loss = 3.986\n",
      "Epoch  33 Batch 1961/2303   train_loss = 4.127\n",
      "Epoch  33 Batch 1981/2303   train_loss = 3.132\n",
      "Epoch  33 Batch 2001/2303   train_loss = 5.103\n",
      "Epoch  33 Batch 2021/2303   train_loss = 3.408\n",
      "Epoch  33 Batch 2041/2303   train_loss = 3.212\n",
      "Epoch  33 Batch 2061/2303   train_loss = 3.456\n",
      "Epoch  33 Batch 2081/2303   train_loss = 3.545\n",
      "Epoch  33 Batch 2101/2303   train_loss = 3.818\n",
      "Epoch  33 Batch 2121/2303   train_loss = 4.509\n",
      "Epoch  33 Batch 2141/2303   train_loss = 4.260\n",
      "Epoch  33 Batch 2161/2303   train_loss = 3.588\n",
      "Epoch  33 Batch 2181/2303   train_loss = 4.367\n",
      "Epoch  33 Batch 2201/2303   train_loss = 3.768\n",
      "Epoch  33 Batch 2221/2303   train_loss = 2.658\n",
      "Epoch  33 Batch 2241/2303   train_loss = 3.167\n",
      "Epoch  33 Batch 2261/2303   train_loss = 3.068\n",
      "Epoch  33 Batch 2281/2303   train_loss = 4.263\n",
      "Epoch  33 Batch 2301/2303   train_loss = 3.709\n",
      "Epoch  34 Batch   18/2303   train_loss = 3.633\n",
      "Epoch  34 Batch   38/2303   train_loss = 3.714\n",
      "Epoch  34 Batch   58/2303   train_loss = 4.035\n",
      "Epoch  34 Batch   78/2303   train_loss = 3.749\n",
      "Epoch  34 Batch   98/2303   train_loss = 3.715\n",
      "Epoch  34 Batch  118/2303   train_loss = 3.579\n",
      "Epoch  34 Batch  138/2303   train_loss = 3.587\n",
      "Epoch  34 Batch  158/2303   train_loss = 3.178\n",
      "Epoch  34 Batch  178/2303   train_loss = 3.421\n",
      "Epoch  34 Batch  198/2303   train_loss = 3.734\n",
      "Epoch  34 Batch  218/2303   train_loss = 3.582\n",
      "Epoch  34 Batch  238/2303   train_loss = 3.790\n",
      "Epoch  34 Batch  258/2303   train_loss = 3.238\n",
      "Epoch  34 Batch  278/2303   train_loss = 3.618\n",
      "Epoch  34 Batch  298/2303   train_loss = 3.500\n",
      "Epoch  34 Batch  318/2303   train_loss = 4.297\n",
      "Epoch  34 Batch  338/2303   train_loss = 3.955\n",
      "Epoch  34 Batch  358/2303   train_loss = 3.694\n",
      "Epoch  34 Batch  378/2303   train_loss = 4.052\n",
      "Epoch  34 Batch  398/2303   train_loss = 4.136\n",
      "Epoch  34 Batch  418/2303   train_loss = 4.201\n",
      "Epoch  34 Batch  438/2303   train_loss = 3.803\n",
      "Epoch  34 Batch  458/2303   train_loss = 3.723\n",
      "Epoch  34 Batch  478/2303   train_loss = 3.761\n",
      "Epoch  34 Batch  498/2303   train_loss = 3.296\n",
      "Epoch  34 Batch  518/2303   train_loss = 4.662\n",
      "Epoch  34 Batch  538/2303   train_loss = 3.579\n",
      "Epoch  34 Batch  558/2303   train_loss = 4.008\n",
      "Epoch  34 Batch  578/2303   train_loss = 3.590\n",
      "Epoch  34 Batch  598/2303   train_loss = 4.059\n",
      "Epoch  34 Batch  618/2303   train_loss = 4.120\n",
      "Epoch  34 Batch  638/2303   train_loss = 2.643\n",
      "Epoch  34 Batch  658/2303   train_loss = 3.888\n",
      "Epoch  34 Batch  678/2303   train_loss = 3.621\n",
      "Epoch  34 Batch  698/2303   train_loss = 3.798\n",
      "Epoch  34 Batch  718/2303   train_loss = 3.328\n",
      "Epoch  34 Batch  738/2303   train_loss = 3.579\n",
      "Epoch  34 Batch  758/2303   train_loss = 3.588\n",
      "Epoch  34 Batch  778/2303   train_loss = 5.485\n",
      "Epoch  34 Batch  798/2303   train_loss = 3.011\n",
      "Epoch  34 Batch  818/2303   train_loss = 3.644\n",
      "Epoch  34 Batch  838/2303   train_loss = 3.643\n",
      "Epoch  34 Batch  858/2303   train_loss = 3.405\n",
      "Epoch  34 Batch  878/2303   train_loss = 2.982\n",
      "Epoch  34 Batch  898/2303   train_loss = 2.876\n",
      "Epoch  34 Batch  918/2303   train_loss = 3.508\n",
      "Epoch  34 Batch  938/2303   train_loss = 3.455\n",
      "Epoch  34 Batch  958/2303   train_loss = 3.495\n",
      "Epoch  34 Batch  978/2303   train_loss = 4.006\n",
      "Epoch  34 Batch  998/2303   train_loss = 4.166\n",
      "Epoch  34 Batch 1018/2303   train_loss = 4.092\n",
      "Epoch  34 Batch 1038/2303   train_loss = 3.342\n",
      "Epoch  34 Batch 1058/2303   train_loss = 4.282\n",
      "Epoch  34 Batch 1078/2303   train_loss = 3.994\n",
      "Epoch  34 Batch 1098/2303   train_loss = 3.611\n",
      "Epoch  34 Batch 1118/2303   train_loss = 2.925\n",
      "Epoch  34 Batch 1138/2303   train_loss = 4.325\n",
      "Epoch  34 Batch 1158/2303   train_loss = 3.419\n",
      "Epoch  34 Batch 1178/2303   train_loss = 3.568\n",
      "Epoch  34 Batch 1198/2303   train_loss = 3.068\n",
      "Epoch  34 Batch 1218/2303   train_loss = 3.600\n",
      "Epoch  34 Batch 1238/2303   train_loss = 3.734\n",
      "Epoch  34 Batch 1258/2303   train_loss = 3.930\n",
      "Epoch  34 Batch 1278/2303   train_loss = 3.273\n",
      "Epoch  34 Batch 1298/2303   train_loss = 4.516\n",
      "Epoch  34 Batch 1318/2303   train_loss = 3.994\n",
      "Epoch  34 Batch 1338/2303   train_loss = 3.083\n",
      "Epoch  34 Batch 1358/2303   train_loss = 3.616\n",
      "Epoch  34 Batch 1378/2303   train_loss = 3.968\n",
      "Epoch  34 Batch 1398/2303   train_loss = 3.493\n",
      "Epoch  34 Batch 1418/2303   train_loss = 3.863\n",
      "Epoch  34 Batch 1438/2303   train_loss = 3.728\n",
      "Epoch  34 Batch 1458/2303   train_loss = 3.483\n",
      "Epoch  34 Batch 1478/2303   train_loss = 3.894\n",
      "Epoch  34 Batch 1498/2303   train_loss = 3.325\n",
      "Epoch  34 Batch 1518/2303   train_loss = 3.714\n",
      "Epoch  34 Batch 1538/2303   train_loss = 4.121\n",
      "Epoch  34 Batch 1558/2303   train_loss = 4.105\n",
      "Epoch  34 Batch 1578/2303   train_loss = 4.041\n",
      "Epoch  34 Batch 1598/2303   train_loss = 3.698\n",
      "Epoch  34 Batch 1618/2303   train_loss = 3.214\n",
      "Epoch  34 Batch 1638/2303   train_loss = 3.579\n",
      "Epoch  34 Batch 1658/2303   train_loss = 3.116\n",
      "Epoch  34 Batch 1678/2303   train_loss = 4.437\n",
      "Epoch  34 Batch 1698/2303   train_loss = 4.325\n",
      "Epoch  34 Batch 1718/2303   train_loss = 3.512\n",
      "Epoch  34 Batch 1738/2303   train_loss = 3.415\n",
      "Epoch  34 Batch 1758/2303   train_loss = 3.830\n",
      "Epoch  34 Batch 1778/2303   train_loss = 3.930\n",
      "Epoch  34 Batch 1798/2303   train_loss = 3.385\n",
      "Epoch  34 Batch 1818/2303   train_loss = 4.253\n",
      "Epoch  34 Batch 1838/2303   train_loss = 3.091\n",
      "Epoch  34 Batch 1858/2303   train_loss = 4.056\n",
      "Epoch  34 Batch 1878/2303   train_loss = 3.809\n",
      "Epoch  34 Batch 1898/2303   train_loss = 4.245\n",
      "Epoch  34 Batch 1918/2303   train_loss = 4.243\n",
      "Epoch  34 Batch 1938/2303   train_loss = 4.756\n",
      "Epoch  34 Batch 1958/2303   train_loss = 4.521\n",
      "Epoch  34 Batch 1978/2303   train_loss = 4.026\n",
      "Epoch  34 Batch 1998/2303   train_loss = 3.928\n",
      "Epoch  34 Batch 2018/2303   train_loss = 3.585\n",
      "Epoch  34 Batch 2038/2303   train_loss = 4.513\n",
      "Epoch  34 Batch 2058/2303   train_loss = 4.119\n",
      "Epoch  34 Batch 2078/2303   train_loss = 3.666\n",
      "Epoch  34 Batch 2098/2303   train_loss = 2.876\n",
      "Epoch  34 Batch 2118/2303   train_loss = 4.576\n",
      "Epoch  34 Batch 2138/2303   train_loss = 2.989\n",
      "Epoch  34 Batch 2158/2303   train_loss = 3.980\n",
      "Epoch  34 Batch 2178/2303   train_loss = 3.576\n",
      "Epoch  34 Batch 2198/2303   train_loss = 4.109\n",
      "Epoch  34 Batch 2218/2303   train_loss = 3.491\n",
      "Epoch  34 Batch 2238/2303   train_loss = 3.648\n",
      "Epoch  34 Batch 2258/2303   train_loss = 3.378\n",
      "Epoch  34 Batch 2278/2303   train_loss = 3.990\n",
      "Epoch  34 Batch 2298/2303   train_loss = 3.374\n",
      "Epoch  35 Batch   15/2303   train_loss = 3.167\n",
      "Epoch  35 Batch   35/2303   train_loss = 2.989\n",
      "Epoch  35 Batch   55/2303   train_loss = 3.921\n",
      "Epoch  35 Batch   75/2303   train_loss = 4.003\n",
      "Epoch  35 Batch   95/2303   train_loss = 3.144\n",
      "Epoch  35 Batch  115/2303   train_loss = 3.337\n",
      "Epoch  35 Batch  135/2303   train_loss = 3.621\n",
      "Epoch  35 Batch  155/2303   train_loss = 3.628\n",
      "Epoch  35 Batch  175/2303   train_loss = 4.254\n",
      "Epoch  35 Batch  195/2303   train_loss = 3.559\n",
      "Epoch  35 Batch  215/2303   train_loss = 2.617\n",
      "Epoch  35 Batch  235/2303   train_loss = 3.321\n",
      "Epoch  35 Batch  255/2303   train_loss = 4.059\n",
      "Epoch  35 Batch  275/2303   train_loss = 3.291\n",
      "Epoch  35 Batch  295/2303   train_loss = 4.296\n",
      "Epoch  35 Batch  315/2303   train_loss = 4.541\n",
      "Epoch  35 Batch  335/2303   train_loss = 4.718\n",
      "Epoch  35 Batch  355/2303   train_loss = 3.731\n",
      "Epoch  35 Batch  375/2303   train_loss = 4.490\n",
      "Epoch  35 Batch  395/2303   train_loss = 2.752\n",
      "Epoch  35 Batch  415/2303   train_loss = 3.734\n",
      "Epoch  35 Batch  435/2303   train_loss = 5.137\n",
      "Epoch  35 Batch  455/2303   train_loss = 4.147\n",
      "Epoch  35 Batch  475/2303   train_loss = 3.806\n",
      "Epoch  35 Batch  495/2303   train_loss = 3.194\n",
      "Epoch  35 Batch  515/2303   train_loss = 3.707\n",
      "Epoch  35 Batch  535/2303   train_loss = 4.315\n",
      "Epoch  35 Batch  555/2303   train_loss = 4.198\n",
      "Epoch  35 Batch  575/2303   train_loss = 3.520\n",
      "Epoch  35 Batch  595/2303   train_loss = 3.359\n",
      "Epoch  35 Batch  615/2303   train_loss = 3.047\n",
      "Epoch  35 Batch  635/2303   train_loss = 4.196\n",
      "Epoch  35 Batch  655/2303   train_loss = 3.195\n",
      "Epoch  35 Batch  675/2303   train_loss = 3.345\n",
      "Epoch  35 Batch  695/2303   train_loss = 3.910\n",
      "Epoch  35 Batch  715/2303   train_loss = 3.851\n",
      "Epoch  35 Batch  735/2303   train_loss = 3.054\n",
      "Epoch  35 Batch  755/2303   train_loss = 3.837\n",
      "Epoch  35 Batch  775/2303   train_loss = 3.669\n",
      "Epoch  35 Batch  795/2303   train_loss = 3.883\n",
      "Epoch  35 Batch  815/2303   train_loss = 3.954\n",
      "Epoch  35 Batch  835/2303   train_loss = 3.731\n",
      "Epoch  35 Batch  855/2303   train_loss = 3.676\n",
      "Epoch  35 Batch  875/2303   train_loss = 4.410\n",
      "Epoch  35 Batch  895/2303   train_loss = 4.601\n",
      "Epoch  35 Batch  915/2303   train_loss = 2.667\n",
      "Epoch  35 Batch  935/2303   train_loss = 3.112\n",
      "Epoch  35 Batch  955/2303   train_loss = 3.447\n",
      "Epoch  35 Batch  975/2303   train_loss = 3.939\n",
      "Epoch  35 Batch  995/2303   train_loss = 3.356\n",
      "Epoch  35 Batch 1015/2303   train_loss = 3.710\n",
      "Epoch  35 Batch 1035/2303   train_loss = 4.118\n",
      "Epoch  35 Batch 1055/2303   train_loss = 5.053\n",
      "Epoch  35 Batch 1075/2303   train_loss = 4.022\n",
      "Epoch  35 Batch 1095/2303   train_loss = 3.541\n",
      "Epoch  35 Batch 1115/2303   train_loss = 4.388\n",
      "Epoch  35 Batch 1135/2303   train_loss = 3.840\n",
      "Epoch  35 Batch 1155/2303   train_loss = 4.692\n",
      "Epoch  35 Batch 1175/2303   train_loss = 4.185\n",
      "Epoch  35 Batch 1195/2303   train_loss = 3.779\n",
      "Epoch  35 Batch 1215/2303   train_loss = 4.608\n",
      "Epoch  35 Batch 1235/2303   train_loss = 4.057\n",
      "Epoch  35 Batch 1255/2303   train_loss = 3.667\n",
      "Epoch  35 Batch 1275/2303   train_loss = 2.771\n",
      "Epoch  35 Batch 1295/2303   train_loss = 4.240\n",
      "Epoch  35 Batch 1315/2303   train_loss = 3.437\n",
      "Epoch  35 Batch 1335/2303   train_loss = 4.392\n",
      "Epoch  35 Batch 1355/2303   train_loss = 3.812\n",
      "Epoch  35 Batch 1375/2303   train_loss = 3.978\n",
      "Epoch  35 Batch 1395/2303   train_loss = 3.014\n",
      "Epoch  35 Batch 1415/2303   train_loss = 3.166\n",
      "Epoch  35 Batch 1435/2303   train_loss = 3.357\n",
      "Epoch  35 Batch 1455/2303   train_loss = 3.738\n",
      "Epoch  35 Batch 1475/2303   train_loss = 3.966\n",
      "Epoch  35 Batch 1495/2303   train_loss = 3.604\n",
      "Epoch  35 Batch 1515/2303   train_loss = 3.901\n",
      "Epoch  35 Batch 1535/2303   train_loss = 3.497\n",
      "Epoch  35 Batch 1555/2303   train_loss = 3.867\n",
      "Epoch  35 Batch 1575/2303   train_loss = 4.276\n",
      "Epoch  35 Batch 1595/2303   train_loss = 4.230\n",
      "Epoch  35 Batch 1615/2303   train_loss = 4.129\n",
      "Epoch  35 Batch 1635/2303   train_loss = 3.345\n",
      "Epoch  35 Batch 1655/2303   train_loss = 3.649\n",
      "Epoch  35 Batch 1675/2303   train_loss = 3.828\n",
      "Epoch  35 Batch 1695/2303   train_loss = 3.865\n",
      "Epoch  35 Batch 1715/2303   train_loss = 3.881\n",
      "Epoch  35 Batch 1735/2303   train_loss = 5.234\n",
      "Epoch  35 Batch 1755/2303   train_loss = 3.662\n",
      "Epoch  35 Batch 1775/2303   train_loss = 3.640\n",
      "Epoch  35 Batch 1795/2303   train_loss = 4.525\n",
      "Epoch  35 Batch 1815/2303   train_loss = 4.107\n",
      "Epoch  35 Batch 1835/2303   train_loss = 3.688\n",
      "Epoch  35 Batch 1855/2303   train_loss = 3.510\n",
      "Epoch  35 Batch 1875/2303   train_loss = 3.719\n",
      "Epoch  35 Batch 1895/2303   train_loss = 3.831\n",
      "Epoch  35 Batch 1915/2303   train_loss = 4.040\n",
      "Epoch  35 Batch 1935/2303   train_loss = 3.196\n",
      "Epoch  35 Batch 1955/2303   train_loss = 3.738\n",
      "Epoch  35 Batch 1975/2303   train_loss = 4.219\n",
      "Epoch  35 Batch 1995/2303   train_loss = 3.195\n",
      "Epoch  35 Batch 2015/2303   train_loss = 3.784\n",
      "Epoch  35 Batch 2035/2303   train_loss = 3.223\n",
      "Epoch  35 Batch 2055/2303   train_loss = 3.230\n",
      "Epoch  35 Batch 2075/2303   train_loss = 4.170\n",
      "Epoch  35 Batch 2095/2303   train_loss = 3.190\n",
      "Epoch  35 Batch 2115/2303   train_loss = 3.689\n",
      "Epoch  35 Batch 2135/2303   train_loss = 3.750\n",
      "Epoch  35 Batch 2155/2303   train_loss = 3.419\n",
      "Epoch  35 Batch 2175/2303   train_loss = 3.355\n",
      "Epoch  35 Batch 2195/2303   train_loss = 3.654\n",
      "Epoch  35 Batch 2215/2303   train_loss = 4.489\n",
      "Epoch  35 Batch 2235/2303   train_loss = 3.335\n",
      "Epoch  35 Batch 2255/2303   train_loss = 4.099\n",
      "Epoch  35 Batch 2275/2303   train_loss = 4.678\n",
      "Epoch  35 Batch 2295/2303   train_loss = 3.415\n",
      "Epoch  36 Batch   12/2303   train_loss = 3.974\n",
      "Epoch  36 Batch   32/2303   train_loss = 2.529\n",
      "Epoch  36 Batch   52/2303   train_loss = 4.150\n",
      "Epoch  36 Batch   72/2303   train_loss = 3.398\n",
      "Epoch  36 Batch   92/2303   train_loss = 3.569\n",
      "Epoch  36 Batch  112/2303   train_loss = 2.835\n",
      "Epoch  36 Batch  132/2303   train_loss = 3.786\n",
      "Epoch  36 Batch  152/2303   train_loss = 3.490\n",
      "Epoch  36 Batch  172/2303   train_loss = 3.235\n",
      "Epoch  36 Batch  192/2303   train_loss = 3.295\n",
      "Epoch  36 Batch  212/2303   train_loss = 2.028\n",
      "Epoch  36 Batch  232/2303   train_loss = 3.774\n",
      "Epoch  36 Batch  252/2303   train_loss = 3.686\n",
      "Epoch  36 Batch  272/2303   train_loss = 2.974\n",
      "Epoch  36 Batch  292/2303   train_loss = 4.154\n",
      "Epoch  36 Batch  312/2303   train_loss = 3.054\n",
      "Epoch  36 Batch  332/2303   train_loss = 4.112\n",
      "Epoch  36 Batch  352/2303   train_loss = 4.089\n",
      "Epoch  36 Batch  372/2303   train_loss = 3.863\n",
      "Epoch  36 Batch  392/2303   train_loss = 3.661\n",
      "Epoch  36 Batch  412/2303   train_loss = 3.399\n",
      "Epoch  36 Batch  432/2303   train_loss = 3.863\n",
      "Epoch  36 Batch  452/2303   train_loss = 4.420\n",
      "Epoch  36 Batch  472/2303   train_loss = 4.738\n",
      "Epoch  36 Batch  492/2303   train_loss = 4.055\n",
      "Epoch  36 Batch  512/2303   train_loss = 4.126\n",
      "Epoch  36 Batch  532/2303   train_loss = 3.972\n",
      "Epoch  36 Batch  552/2303   train_loss = 4.259\n",
      "Epoch  36 Batch  572/2303   train_loss = 3.691\n",
      "Epoch  36 Batch  592/2303   train_loss = 3.616\n",
      "Epoch  36 Batch  612/2303   train_loss = 3.155\n",
      "Epoch  36 Batch  632/2303   train_loss = 3.801\n",
      "Epoch  36 Batch  652/2303   train_loss = 3.963\n",
      "Epoch  36 Batch  672/2303   train_loss = 3.820\n",
      "Epoch  36 Batch  692/2303   train_loss = 4.442\n",
      "Epoch  36 Batch  712/2303   train_loss = 4.378\n",
      "Epoch  36 Batch  732/2303   train_loss = 3.227\n",
      "Epoch  36 Batch  752/2303   train_loss = 4.125\n",
      "Epoch  36 Batch  772/2303   train_loss = 2.703\n",
      "Epoch  36 Batch  792/2303   train_loss = 4.755\n",
      "Epoch  36 Batch  812/2303   train_loss = 4.244\n",
      "Epoch  36 Batch  832/2303   train_loss = 3.672\n",
      "Epoch  36 Batch  852/2303   train_loss = 3.459\n",
      "Epoch  36 Batch  872/2303   train_loss = 3.882\n",
      "Epoch  36 Batch  892/2303   train_loss = 4.096\n",
      "Epoch  36 Batch  912/2303   train_loss = 4.144\n",
      "Epoch  36 Batch  932/2303   train_loss = 3.778\n",
      "Epoch  36 Batch  952/2303   train_loss = 3.864\n",
      "Epoch  36 Batch  972/2303   train_loss = 3.350\n",
      "Epoch  36 Batch  992/2303   train_loss = 4.621\n",
      "Epoch  36 Batch 1012/2303   train_loss = 3.312\n",
      "Epoch  36 Batch 1032/2303   train_loss = 3.991\n",
      "Epoch  36 Batch 1052/2303   train_loss = 3.644\n",
      "Epoch  36 Batch 1072/2303   train_loss = 3.928\n",
      "Epoch  36 Batch 1092/2303   train_loss = 4.289\n",
      "Epoch  36 Batch 1112/2303   train_loss = 3.622\n",
      "Epoch  36 Batch 1132/2303   train_loss = 4.445\n",
      "Epoch  36 Batch 1152/2303   train_loss = 3.819\n",
      "Epoch  36 Batch 1172/2303   train_loss = 4.501\n",
      "Epoch  36 Batch 1192/2303   train_loss = 3.868\n",
      "Epoch  36 Batch 1212/2303   train_loss = 3.908\n",
      "Epoch  36 Batch 1232/2303   train_loss = 4.199\n",
      "Epoch  36 Batch 1252/2303   train_loss = 4.337\n",
      "Epoch  36 Batch 1272/2303   train_loss = 3.209\n",
      "Epoch  36 Batch 1292/2303   train_loss = 4.394\n",
      "Epoch  36 Batch 1312/2303   train_loss = 3.039\n",
      "Epoch  36 Batch 1332/2303   train_loss = 3.479\n",
      "Epoch  36 Batch 1352/2303   train_loss = 3.734\n",
      "Epoch  36 Batch 1372/2303   train_loss = 4.175\n",
      "Epoch  36 Batch 1392/2303   train_loss = 3.955\n",
      "Epoch  36 Batch 1412/2303   train_loss = 4.051\n",
      "Epoch  36 Batch 1432/2303   train_loss = 4.036\n",
      "Epoch  36 Batch 1452/2303   train_loss = 3.524\n",
      "Epoch  36 Batch 1472/2303   train_loss = 3.941\n",
      "Epoch  36 Batch 1492/2303   train_loss = 3.973\n",
      "Epoch  36 Batch 1512/2303   train_loss = 3.558\n",
      "Epoch  36 Batch 1532/2303   train_loss = 3.790\n",
      "Epoch  36 Batch 1552/2303   train_loss = 4.312\n",
      "Epoch  36 Batch 1572/2303   train_loss = 3.653\n",
      "Epoch  36 Batch 1592/2303   train_loss = 3.946\n",
      "Epoch  36 Batch 1612/2303   train_loss = 4.088\n",
      "Epoch  36 Batch 1632/2303   train_loss = 3.661\n",
      "Epoch  36 Batch 1652/2303   train_loss = 2.801\n",
      "Epoch  36 Batch 1672/2303   train_loss = 3.758\n",
      "Epoch  36 Batch 1692/2303   train_loss = 4.546\n",
      "Epoch  36 Batch 1712/2303   train_loss = 4.687\n",
      "Epoch  36 Batch 1732/2303   train_loss = 4.174\n",
      "Epoch  36 Batch 1752/2303   train_loss = 3.892\n",
      "Epoch  36 Batch 1772/2303   train_loss = 3.159\n",
      "Epoch  36 Batch 1792/2303   train_loss = 4.621\n",
      "Epoch  36 Batch 1812/2303   train_loss = 4.098\n",
      "Epoch  36 Batch 1832/2303   train_loss = 3.887\n",
      "Epoch  36 Batch 1852/2303   train_loss = 3.905\n",
      "Epoch  36 Batch 1872/2303   train_loss = 3.633\n",
      "Epoch  36 Batch 1892/2303   train_loss = 4.180\n",
      "Epoch  36 Batch 1912/2303   train_loss = 4.349\n",
      "Epoch  36 Batch 1932/2303   train_loss = 4.472\n",
      "Epoch  36 Batch 1952/2303   train_loss = 3.424\n",
      "Epoch  36 Batch 1972/2303   train_loss = 3.653\n",
      "Epoch  36 Batch 1992/2303   train_loss = 3.797\n",
      "Epoch  36 Batch 2012/2303   train_loss = 3.912\n",
      "Epoch  36 Batch 2032/2303   train_loss = 3.772\n",
      "Epoch  36 Batch 2052/2303   train_loss = 3.344\n",
      "Epoch  36 Batch 2072/2303   train_loss = 3.931\n",
      "Epoch  36 Batch 2092/2303   train_loss = 3.208\n",
      "Epoch  36 Batch 2112/2303   train_loss = 3.441\n",
      "Epoch  36 Batch 2132/2303   train_loss = 3.714\n",
      "Epoch  36 Batch 2152/2303   train_loss = 4.138\n",
      "Epoch  36 Batch 2172/2303   train_loss = 3.841\n",
      "Epoch  36 Batch 2192/2303   train_loss = 4.008\n",
      "Epoch  36 Batch 2212/2303   train_loss = 3.930\n",
      "Epoch  36 Batch 2232/2303   train_loss = 3.854\n",
      "Epoch  36 Batch 2252/2303   train_loss = 3.392\n",
      "Epoch  36 Batch 2272/2303   train_loss = 4.003\n",
      "Epoch  36 Batch 2292/2303   train_loss = 3.851\n",
      "Epoch  37 Batch    9/2303   train_loss = 3.401\n",
      "Epoch  37 Batch   29/2303   train_loss = 3.628\n",
      "Epoch  37 Batch   49/2303   train_loss = 3.032\n",
      "Epoch  37 Batch   69/2303   train_loss = 3.050\n",
      "Epoch  37 Batch   89/2303   train_loss = 4.039\n",
      "Epoch  37 Batch  109/2303   train_loss = 2.970\n",
      "Epoch  37 Batch  129/2303   train_loss = 4.496\n",
      "Epoch  37 Batch  149/2303   train_loss = 3.129\n",
      "Epoch  37 Batch  169/2303   train_loss = 3.227\n",
      "Epoch  37 Batch  189/2303   train_loss = 3.948\n",
      "Epoch  37 Batch  209/2303   train_loss = 3.682\n",
      "Epoch  37 Batch  229/2303   train_loss = 4.759\n",
      "Epoch  37 Batch  249/2303   train_loss = 3.641\n",
      "Epoch  37 Batch  269/2303   train_loss = 4.202\n",
      "Epoch  37 Batch  289/2303   train_loss = 4.023\n",
      "Epoch  37 Batch  309/2303   train_loss = 4.083\n",
      "Epoch  37 Batch  329/2303   train_loss = 3.209\n",
      "Epoch  37 Batch  349/2303   train_loss = 3.857\n",
      "Epoch  37 Batch  369/2303   train_loss = 4.273\n",
      "Epoch  37 Batch  389/2303   train_loss = 3.876\n",
      "Epoch  37 Batch  409/2303   train_loss = 3.082\n",
      "Epoch  37 Batch  429/2303   train_loss = 3.619\n",
      "Epoch  37 Batch  449/2303   train_loss = 3.836\n",
      "Epoch  37 Batch  469/2303   train_loss = 4.408\n",
      "Epoch  37 Batch  489/2303   train_loss = 3.693\n",
      "Epoch  37 Batch  509/2303   train_loss = 4.426\n",
      "Epoch  37 Batch  529/2303   train_loss = 3.644\n",
      "Epoch  37 Batch  549/2303   train_loss = 4.409\n",
      "Epoch  37 Batch  569/2303   train_loss = 3.075\n",
      "Epoch  37 Batch  589/2303   train_loss = 3.462\n",
      "Epoch  37 Batch  609/2303   train_loss = 3.990\n",
      "Epoch  37 Batch  629/2303   train_loss = 2.973\n",
      "Epoch  37 Batch  649/2303   train_loss = 4.105\n",
      "Epoch  37 Batch  669/2303   train_loss = 4.076\n",
      "Epoch  37 Batch  689/2303   train_loss = 3.746\n",
      "Epoch  37 Batch  709/2303   train_loss = 3.170\n",
      "Epoch  37 Batch  729/2303   train_loss = 2.500\n",
      "Epoch  37 Batch  749/2303   train_loss = 3.882\n",
      "Epoch  37 Batch  769/2303   train_loss = 4.072\n",
      "Epoch  37 Batch  789/2303   train_loss = 3.525\n",
      "Epoch  37 Batch  809/2303   train_loss = 3.955\n",
      "Epoch  37 Batch  829/2303   train_loss = 3.603\n",
      "Epoch  37 Batch  849/2303   train_loss = 3.702\n",
      "Epoch  37 Batch  869/2303   train_loss = 3.250\n",
      "Epoch  37 Batch  889/2303   train_loss = 3.889\n",
      "Epoch  37 Batch  909/2303   train_loss = 4.471\n",
      "Epoch  37 Batch  929/2303   train_loss = 4.144\n",
      "Epoch  37 Batch  949/2303   train_loss = 3.171\n",
      "Epoch  37 Batch  969/2303   train_loss = 3.929\n",
      "Epoch  37 Batch  989/2303   train_loss = 4.240\n",
      "Epoch  37 Batch 1009/2303   train_loss = 3.613\n",
      "Epoch  37 Batch 1029/2303   train_loss = 3.027\n",
      "Epoch  37 Batch 1049/2303   train_loss = 3.572\n",
      "Epoch  37 Batch 1069/2303   train_loss = 3.697\n",
      "Epoch  37 Batch 1089/2303   train_loss = 4.176\n",
      "Epoch  37 Batch 1109/2303   train_loss = 1.892\n",
      "Epoch  37 Batch 1129/2303   train_loss = 4.059\n",
      "Epoch  37 Batch 1149/2303   train_loss = 3.634\n",
      "Epoch  37 Batch 1169/2303   train_loss = 4.147\n",
      "Epoch  37 Batch 1189/2303   train_loss = 3.433\n",
      "Epoch  37 Batch 1209/2303   train_loss = 3.665\n",
      "Epoch  37 Batch 1229/2303   train_loss = 3.586\n",
      "Epoch  37 Batch 1249/2303   train_loss = 3.887\n",
      "Epoch  37 Batch 1269/2303   train_loss = 2.915\n",
      "Epoch  37 Batch 1289/2303   train_loss = 4.329\n",
      "Epoch  37 Batch 1309/2303   train_loss = 3.730\n",
      "Epoch  37 Batch 1329/2303   train_loss = 4.398\n",
      "Epoch  37 Batch 1349/2303   train_loss = 3.919\n",
      "Epoch  37 Batch 1369/2303   train_loss = 3.327\n",
      "Epoch  37 Batch 1389/2303   train_loss = 3.216\n",
      "Epoch  37 Batch 1409/2303   train_loss = 3.789\n",
      "Epoch  37 Batch 1429/2303   train_loss = 4.075\n",
      "Epoch  37 Batch 1449/2303   train_loss = 3.662\n",
      "Epoch  37 Batch 1469/2303   train_loss = 3.423\n",
      "Epoch  37 Batch 1489/2303   train_loss = 3.532\n",
      "Epoch  37 Batch 1509/2303   train_loss = 4.737\n",
      "Epoch  37 Batch 1529/2303   train_loss = 3.962\n",
      "Epoch  37 Batch 1549/2303   train_loss = 4.505\n",
      "Epoch  37 Batch 1569/2303   train_loss = 3.454\n",
      "Epoch  37 Batch 1589/2303   train_loss = 3.615\n",
      "Epoch  37 Batch 1609/2303   train_loss = 3.591\n",
      "Epoch  37 Batch 1629/2303   train_loss = 4.037\n",
      "Epoch  37 Batch 1649/2303   train_loss = 4.537\n",
      "Epoch  37 Batch 1669/2303   train_loss = 3.833\n",
      "Epoch  37 Batch 1689/2303   train_loss = 3.590\n",
      "Epoch  37 Batch 1709/2303   train_loss = 3.151\n",
      "Epoch  37 Batch 1729/2303   train_loss = 4.316\n",
      "Epoch  37 Batch 1749/2303   train_loss = 4.410\n",
      "Epoch  37 Batch 1769/2303   train_loss = 4.211\n",
      "Epoch  37 Batch 1789/2303   train_loss = 4.459\n",
      "Epoch  37 Batch 1809/2303   train_loss = 3.794\n",
      "Epoch  37 Batch 1829/2303   train_loss = 3.009\n",
      "Epoch  37 Batch 1849/2303   train_loss = 3.650\n",
      "Epoch  37 Batch 1869/2303   train_loss = 3.424\n",
      "Epoch  37 Batch 1889/2303   train_loss = 4.535\n",
      "Epoch  37 Batch 1909/2303   train_loss = 4.520\n",
      "Epoch  37 Batch 1929/2303   train_loss = 3.657\n",
      "Epoch  37 Batch 1949/2303   train_loss = 4.195\n",
      "Epoch  37 Batch 1969/2303   train_loss = 4.238\n",
      "Epoch  37 Batch 1989/2303   train_loss = 4.359\n",
      "Epoch  37 Batch 2009/2303   train_loss = 3.790\n",
      "Epoch  37 Batch 2029/2303   train_loss = 3.803\n",
      "Epoch  37 Batch 2049/2303   train_loss = 2.862\n",
      "Epoch  37 Batch 2069/2303   train_loss = 3.548\n",
      "Epoch  37 Batch 2089/2303   train_loss = 3.657\n",
      "Epoch  37 Batch 2109/2303   train_loss = 3.803\n",
      "Epoch  37 Batch 2129/2303   train_loss = 3.903\n",
      "Epoch  37 Batch 2149/2303   train_loss = 4.084\n",
      "Epoch  37 Batch 2169/2303   train_loss = 4.159\n",
      "Epoch  37 Batch 2189/2303   train_loss = 4.561\n",
      "Epoch  37 Batch 2209/2303   train_loss = 4.059\n",
      "Epoch  37 Batch 2229/2303   train_loss = 3.705\n",
      "Epoch  37 Batch 2249/2303   train_loss = 3.425\n",
      "Epoch  37 Batch 2269/2303   train_loss = 3.148\n",
      "Epoch  37 Batch 2289/2303   train_loss = 3.364\n",
      "Epoch  38 Batch    6/2303   train_loss = 3.701\n",
      "Epoch  38 Batch   26/2303   train_loss = 3.570\n",
      "Epoch  38 Batch   46/2303   train_loss = 3.105\n",
      "Epoch  38 Batch   66/2303   train_loss = 3.361\n",
      "Epoch  38 Batch   86/2303   train_loss = 4.026\n",
      "Epoch  38 Batch  106/2303   train_loss = 3.129\n",
      "Epoch  38 Batch  126/2303   train_loss = 4.576\n",
      "Epoch  38 Batch  146/2303   train_loss = 4.535\n",
      "Epoch  38 Batch  166/2303   train_loss = 4.373\n",
      "Epoch  38 Batch  186/2303   train_loss = 3.917\n",
      "Epoch  38 Batch  206/2303   train_loss = 3.693\n",
      "Epoch  38 Batch  226/2303   train_loss = 4.351\n",
      "Epoch  38 Batch  246/2303   train_loss = 4.188\n",
      "Epoch  38 Batch  266/2303   train_loss = 3.501\n",
      "Epoch  38 Batch  286/2303   train_loss = 4.011\n",
      "Epoch  38 Batch  306/2303   train_loss = 3.877\n",
      "Epoch  38 Batch  326/2303   train_loss = 3.286\n",
      "Epoch  38 Batch  346/2303   train_loss = 4.805\n",
      "Epoch  38 Batch  366/2303   train_loss = 4.311\n",
      "Epoch  38 Batch  386/2303   train_loss = 4.280\n",
      "Epoch  38 Batch  406/2303   train_loss = 3.142\n",
      "Epoch  38 Batch  426/2303   train_loss = 3.113\n",
      "Epoch  38 Batch  446/2303   train_loss = 3.936\n",
      "Epoch  38 Batch  466/2303   train_loss = 4.423\n",
      "Epoch  38 Batch  486/2303   train_loss = 3.790\n",
      "Epoch  38 Batch  506/2303   train_loss = 3.964\n",
      "Epoch  38 Batch  526/2303   train_loss = 2.260\n",
      "Epoch  38 Batch  546/2303   train_loss = 3.913\n",
      "Epoch  38 Batch  566/2303   train_loss = 3.395\n",
      "Epoch  38 Batch  586/2303   train_loss = 3.429\n",
      "Epoch  38 Batch  606/2303   train_loss = 4.018\n",
      "Epoch  38 Batch  626/2303   train_loss = 4.042\n",
      "Epoch  38 Batch  646/2303   train_loss = 3.544\n",
      "Epoch  38 Batch  666/2303   train_loss = 3.736\n",
      "Epoch  38 Batch  686/2303   train_loss = 4.274\n",
      "Epoch  38 Batch  706/2303   train_loss = 4.315\n",
      "Epoch  38 Batch  726/2303   train_loss = 4.011\n",
      "Epoch  38 Batch  746/2303   train_loss = 3.632\n",
      "Epoch  38 Batch  766/2303   train_loss = 3.630\n",
      "Epoch  38 Batch  786/2303   train_loss = 3.702\n",
      "Epoch  38 Batch  806/2303   train_loss = 3.993\n",
      "Epoch  38 Batch  826/2303   train_loss = 3.301\n",
      "Epoch  38 Batch  846/2303   train_loss = 4.597\n",
      "Epoch  38 Batch  866/2303   train_loss = 3.548\n",
      "Epoch  38 Batch  886/2303   train_loss = 3.443\n",
      "Epoch  38 Batch  906/2303   train_loss = 3.990\n",
      "Epoch  38 Batch  926/2303   train_loss = 3.472\n",
      "Epoch  38 Batch  946/2303   train_loss = 3.446\n",
      "Epoch  38 Batch  966/2303   train_loss = 3.395\n",
      "Epoch  38 Batch  986/2303   train_loss = 3.099\n",
      "Epoch  38 Batch 1006/2303   train_loss = 2.627\n",
      "Epoch  38 Batch 1026/2303   train_loss = 3.037\n",
      "Epoch  38 Batch 1046/2303   train_loss = 4.894\n",
      "Epoch  38 Batch 1066/2303   train_loss = 3.431\n",
      "Epoch  38 Batch 1086/2303   train_loss = 3.115\n",
      "Epoch  38 Batch 1106/2303   train_loss = 4.644\n",
      "Epoch  38 Batch 1126/2303   train_loss = 4.389\n",
      "Epoch  38 Batch 1146/2303   train_loss = 4.182\n",
      "Epoch  38 Batch 1166/2303   train_loss = 3.585\n",
      "Epoch  38 Batch 1186/2303   train_loss = 4.148\n",
      "Epoch  38 Batch 1206/2303   train_loss = 3.684\n",
      "Epoch  38 Batch 1226/2303   train_loss = 3.621\n",
      "Epoch  38 Batch 1246/2303   train_loss = 4.313\n",
      "Epoch  38 Batch 1266/2303   train_loss = 3.502\n",
      "Epoch  38 Batch 1286/2303   train_loss = 4.228\n",
      "Epoch  38 Batch 1306/2303   train_loss = 3.395\n",
      "Epoch  38 Batch 1326/2303   train_loss = 4.109\n",
      "Epoch  38 Batch 1346/2303   train_loss = 3.284\n",
      "Epoch  38 Batch 1366/2303   train_loss = 3.125\n",
      "Epoch  38 Batch 1386/2303   train_loss = 5.105\n",
      "Epoch  38 Batch 1406/2303   train_loss = 3.671\n",
      "Epoch  38 Batch 1426/2303   train_loss = 4.316\n",
      "Epoch  38 Batch 1446/2303   train_loss = 3.716\n",
      "Epoch  38 Batch 1466/2303   train_loss = 3.503\n",
      "Epoch  38 Batch 1486/2303   train_loss = 3.485\n",
      "Epoch  38 Batch 1506/2303   train_loss = 4.159\n",
      "Epoch  38 Batch 1526/2303   train_loss = 3.464\n",
      "Epoch  38 Batch 1546/2303   train_loss = 3.189\n",
      "Epoch  38 Batch 1566/2303   train_loss = 4.408\n",
      "Epoch  38 Batch 1586/2303   train_loss = 2.188\n",
      "Epoch  38 Batch 1606/2303   train_loss = 3.687\n",
      "Epoch  38 Batch 1626/2303   train_loss = 4.081\n",
      "Epoch  38 Batch 1646/2303   train_loss = 3.763\n",
      "Epoch  38 Batch 1666/2303   train_loss = 3.748\n",
      "Epoch  38 Batch 1686/2303   train_loss = 4.186\n",
      "Epoch  38 Batch 1706/2303   train_loss = 3.291\n",
      "Epoch  38 Batch 1726/2303   train_loss = 3.274\n",
      "Epoch  38 Batch 1746/2303   train_loss = 4.523\n",
      "Epoch  38 Batch 1766/2303   train_loss = 3.391\n",
      "Epoch  38 Batch 1786/2303   train_loss = 3.195\n",
      "Epoch  38 Batch 1806/2303   train_loss = 4.132\n",
      "Epoch  38 Batch 1826/2303   train_loss = 4.336\n",
      "Epoch  38 Batch 1846/2303   train_loss = 3.011\n",
      "Epoch  38 Batch 1866/2303   train_loss = 3.963\n",
      "Epoch  38 Batch 1886/2303   train_loss = 3.450\n",
      "Epoch  38 Batch 1906/2303   train_loss = 3.551\n",
      "Epoch  38 Batch 1926/2303   train_loss = 4.321\n",
      "Epoch  38 Batch 1946/2303   train_loss = 3.449\n",
      "Epoch  38 Batch 1966/2303   train_loss = 4.254\n",
      "Epoch  38 Batch 1986/2303   train_loss = 3.521\n",
      "Epoch  38 Batch 2006/2303   train_loss = 3.850\n",
      "Epoch  38 Batch 2026/2303   train_loss = 4.361\n",
      "Epoch  38 Batch 2046/2303   train_loss = 3.692\n",
      "Epoch  38 Batch 2066/2303   train_loss = 5.046\n",
      "Epoch  38 Batch 2086/2303   train_loss = 3.821\n",
      "Epoch  38 Batch 2106/2303   train_loss = 2.983\n",
      "Epoch  38 Batch 2126/2303   train_loss = 3.339\n",
      "Epoch  38 Batch 2146/2303   train_loss = 3.734\n",
      "Epoch  38 Batch 2166/2303   train_loss = 3.614\n",
      "Epoch  38 Batch 2186/2303   train_loss = 3.832\n",
      "Epoch  38 Batch 2206/2303   train_loss = 3.913\n",
      "Epoch  38 Batch 2226/2303   train_loss = 3.002\n",
      "Epoch  38 Batch 2246/2303   train_loss = 5.074\n",
      "Epoch  38 Batch 2266/2303   train_loss = 3.100\n",
      "Epoch  38 Batch 2286/2303   train_loss = 3.229\n",
      "Epoch  39 Batch    3/2303   train_loss = 3.783\n",
      "Epoch  39 Batch   23/2303   train_loss = 3.658\n",
      "Epoch  39 Batch   43/2303   train_loss = 3.280\n",
      "Epoch  39 Batch   63/2303   train_loss = 3.824\n",
      "Epoch  39 Batch   83/2303   train_loss = 3.252\n",
      "Epoch  39 Batch  103/2303   train_loss = 3.026\n",
      "Epoch  39 Batch  123/2303   train_loss = 4.127\n",
      "Epoch  39 Batch  143/2303   train_loss = 3.028\n",
      "Epoch  39 Batch  163/2303   train_loss = 4.416\n",
      "Epoch  39 Batch  183/2303   train_loss = 4.200\n",
      "Epoch  39 Batch  203/2303   train_loss = 3.991\n",
      "Epoch  39 Batch  223/2303   train_loss = 4.282\n",
      "Epoch  39 Batch  243/2303   train_loss = 3.346\n",
      "Epoch  39 Batch  263/2303   train_loss = 3.306\n",
      "Epoch  39 Batch  283/2303   train_loss = 4.112\n",
      "Epoch  39 Batch  303/2303   train_loss = 3.833\n",
      "Epoch  39 Batch  323/2303   train_loss = 3.987\n",
      "Epoch  39 Batch  343/2303   train_loss = 3.580\n",
      "Epoch  39 Batch  363/2303   train_loss = 3.565\n",
      "Epoch  39 Batch  383/2303   train_loss = 3.764\n",
      "Epoch  39 Batch  403/2303   train_loss = 3.497\n",
      "Epoch  39 Batch  423/2303   train_loss = 3.297\n",
      "Epoch  39 Batch  443/2303   train_loss = 3.498\n",
      "Epoch  39 Batch  463/2303   train_loss = 2.819\n",
      "Epoch  39 Batch  483/2303   train_loss = 4.394\n",
      "Epoch  39 Batch  503/2303   train_loss = 3.346\n",
      "Epoch  39 Batch  523/2303   train_loss = 3.932\n",
      "Epoch  39 Batch  543/2303   train_loss = 3.394\n",
      "Epoch  39 Batch  563/2303   train_loss = 3.770\n",
      "Epoch  39 Batch  583/2303   train_loss = 3.690\n",
      "Epoch  39 Batch  603/2303   train_loss = 4.294\n",
      "Epoch  39 Batch  623/2303   train_loss = 5.039\n",
      "Epoch  39 Batch  643/2303   train_loss = 3.105\n",
      "Epoch  39 Batch  663/2303   train_loss = 3.641\n",
      "Epoch  39 Batch  683/2303   train_loss = 3.401\n",
      "Epoch  39 Batch  703/2303   train_loss = 3.381\n",
      "Epoch  39 Batch  723/2303   train_loss = 3.725\n",
      "Epoch  39 Batch  743/2303   train_loss = 3.524\n",
      "Epoch  39 Batch  763/2303   train_loss = 3.224\n",
      "Epoch  39 Batch  783/2303   train_loss = 3.609\n",
      "Epoch  39 Batch  803/2303   train_loss = 2.803\n",
      "Epoch  39 Batch  823/2303   train_loss = 4.248\n",
      "Epoch  39 Batch  843/2303   train_loss = 4.061\n",
      "Epoch  39 Batch  863/2303   train_loss = 3.986\n",
      "Epoch  39 Batch  883/2303   train_loss = 3.073\n",
      "Epoch  39 Batch  903/2303   train_loss = 3.162\n",
      "Epoch  39 Batch  923/2303   train_loss = 3.011\n",
      "Epoch  39 Batch  943/2303   train_loss = 3.336\n",
      "Epoch  39 Batch  963/2303   train_loss = 2.856\n",
      "Epoch  39 Batch  983/2303   train_loss = 3.628\n",
      "Epoch  39 Batch 1003/2303   train_loss = 3.830\n",
      "Epoch  39 Batch 1023/2303   train_loss = 3.455\n",
      "Epoch  39 Batch 1043/2303   train_loss = 3.117\n",
      "Epoch  39 Batch 1063/2303   train_loss = 3.647\n",
      "Epoch  39 Batch 1083/2303   train_loss = 4.205\n",
      "Epoch  39 Batch 1103/2303   train_loss = 3.719\n",
      "Epoch  39 Batch 1123/2303   train_loss = 4.116\n",
      "Epoch  39 Batch 1143/2303   train_loss = 4.313\n",
      "Epoch  39 Batch 1163/2303   train_loss = 3.366\n",
      "Epoch  39 Batch 1183/2303   train_loss = 3.958\n",
      "Epoch  39 Batch 1203/2303   train_loss = 3.141\n",
      "Epoch  39 Batch 1223/2303   train_loss = 3.768\n",
      "Epoch  39 Batch 1243/2303   train_loss = 4.364\n",
      "Epoch  39 Batch 1263/2303   train_loss = 4.068\n",
      "Epoch  39 Batch 1283/2303   train_loss = 3.499\n",
      "Epoch  39 Batch 1303/2303   train_loss = 3.307\n",
      "Epoch  39 Batch 1323/2303   train_loss = 3.737\n",
      "Epoch  39 Batch 1343/2303   train_loss = 3.448\n",
      "Epoch  39 Batch 1363/2303   train_loss = 3.304\n",
      "Epoch  39 Batch 1383/2303   train_loss = 3.999\n",
      "Epoch  39 Batch 1403/2303   train_loss = 4.062\n",
      "Epoch  39 Batch 1423/2303   train_loss = 3.807\n",
      "Epoch  39 Batch 1443/2303   train_loss = 3.316\n",
      "Epoch  39 Batch 1463/2303   train_loss = 3.293\n",
      "Epoch  39 Batch 1483/2303   train_loss = 3.780\n",
      "Epoch  39 Batch 1503/2303   train_loss = 3.525\n",
      "Epoch  39 Batch 1523/2303   train_loss = 3.408\n",
      "Epoch  39 Batch 1543/2303   train_loss = 3.931\n",
      "Epoch  39 Batch 1563/2303   train_loss = 4.644\n",
      "Epoch  39 Batch 1583/2303   train_loss = 3.389\n",
      "Epoch  39 Batch 1603/2303   train_loss = 4.475\n",
      "Epoch  39 Batch 1623/2303   train_loss = 4.012\n",
      "Epoch  39 Batch 1643/2303   train_loss = 3.977\n",
      "Epoch  39 Batch 1663/2303   train_loss = 2.989\n",
      "Epoch  39 Batch 1683/2303   train_loss = 3.673\n",
      "Epoch  39 Batch 1703/2303   train_loss = 3.455\n",
      "Epoch  39 Batch 1723/2303   train_loss = 4.391\n",
      "Epoch  39 Batch 1743/2303   train_loss = 3.940\n",
      "Epoch  39 Batch 1763/2303   train_loss = 4.850\n",
      "Epoch  39 Batch 1783/2303   train_loss = 3.657\n",
      "Epoch  39 Batch 1803/2303   train_loss = 3.215\n",
      "Epoch  39 Batch 1823/2303   train_loss = 3.709\n",
      "Epoch  39 Batch 1843/2303   train_loss = 3.717\n",
      "Epoch  39 Batch 1863/2303   train_loss = 4.441\n",
      "Epoch  39 Batch 1883/2303   train_loss = 4.619\n",
      "Epoch  39 Batch 1903/2303   train_loss = 4.112\n",
      "Epoch  39 Batch 1923/2303   train_loss = 3.561\n",
      "Epoch  39 Batch 1943/2303   train_loss = 3.703\n",
      "Epoch  39 Batch 1963/2303   train_loss = 3.782\n",
      "Epoch  39 Batch 1983/2303   train_loss = 4.740\n",
      "Epoch  39 Batch 2003/2303   train_loss = 3.624\n",
      "Epoch  39 Batch 2023/2303   train_loss = 3.529\n",
      "Epoch  39 Batch 2043/2303   train_loss = 3.995\n",
      "Epoch  39 Batch 2063/2303   train_loss = 3.166\n",
      "Epoch  39 Batch 2083/2303   train_loss = 3.544\n",
      "Epoch  39 Batch 2103/2303   train_loss = 4.443\n",
      "Epoch  39 Batch 2123/2303   train_loss = 3.640\n",
      "Epoch  39 Batch 2143/2303   train_loss = 2.497\n",
      "Epoch  39 Batch 2163/2303   train_loss = 3.390\n",
      "Epoch  39 Batch 2183/2303   train_loss = 4.111\n",
      "Epoch  39 Batch 2203/2303   train_loss = 3.710\n",
      "Epoch  39 Batch 2223/2303   train_loss = 3.668\n",
      "Epoch  39 Batch 2243/2303   train_loss = 3.504\n",
      "Epoch  39 Batch 2263/2303   train_loss = 3.124\n",
      "Epoch  39 Batch 2283/2303   train_loss = 3.134\n",
      "Epoch  40 Batch    0/2303   train_loss = 3.863\n",
      "Epoch  40 Batch   20/2303   train_loss = 4.429\n",
      "Epoch  40 Batch   40/2303   train_loss = 3.390\n",
      "Epoch  40 Batch   60/2303   train_loss = 4.337\n",
      "Epoch  40 Batch   80/2303   train_loss = 3.595\n",
      "Epoch  40 Batch  100/2303   train_loss = 3.701\n",
      "Epoch  40 Batch  120/2303   train_loss = 2.633\n",
      "Epoch  40 Batch  140/2303   train_loss = 3.554\n",
      "Epoch  40 Batch  160/2303   train_loss = 2.970\n",
      "Epoch  40 Batch  180/2303   train_loss = 3.706\n",
      "Epoch  40 Batch  200/2303   train_loss = 3.629\n",
      "Epoch  40 Batch  220/2303   train_loss = 3.928\n",
      "Epoch  40 Batch  240/2303   train_loss = 3.382\n",
      "Epoch  40 Batch  260/2303   train_loss = 4.139\n",
      "Epoch  40 Batch  280/2303   train_loss = 4.331\n",
      "Epoch  40 Batch  300/2303   train_loss = 4.069\n",
      "Epoch  40 Batch  320/2303   train_loss = 3.909\n",
      "Epoch  40 Batch  340/2303   train_loss = 4.313\n",
      "Epoch  40 Batch  360/2303   train_loss = 2.889\n",
      "Epoch  40 Batch  380/2303   train_loss = 3.619\n",
      "Epoch  40 Batch  400/2303   train_loss = 3.692\n",
      "Epoch  40 Batch  420/2303   train_loss = 3.999\n",
      "Epoch  40 Batch  440/2303   train_loss = 2.301\n",
      "Epoch  40 Batch  460/2303   train_loss = 3.920\n",
      "Epoch  40 Batch  480/2303   train_loss = 3.861\n",
      "Epoch  40 Batch  500/2303   train_loss = 3.353\n",
      "Epoch  40 Batch  520/2303   train_loss = 5.237\n",
      "Epoch  40 Batch  540/2303   train_loss = 3.577\n",
      "Epoch  40 Batch  560/2303   train_loss = 4.142\n",
      "Epoch  40 Batch  580/2303   train_loss = 4.378\n",
      "Epoch  40 Batch  600/2303   train_loss = 4.153\n",
      "Epoch  40 Batch  620/2303   train_loss = 3.796\n",
      "Epoch  40 Batch  640/2303   train_loss = 3.745\n",
      "Epoch  40 Batch  660/2303   train_loss = 3.809\n",
      "Epoch  40 Batch  680/2303   train_loss = 4.070\n",
      "Epoch  40 Batch  700/2303   train_loss = 3.687\n",
      "Epoch  40 Batch  720/2303   train_loss = 3.555\n",
      "Epoch  40 Batch  740/2303   train_loss = 3.748\n",
      "Epoch  40 Batch  760/2303   train_loss = 3.725\n",
      "Epoch  40 Batch  780/2303   train_loss = 3.339\n",
      "Epoch  40 Batch  800/2303   train_loss = 4.238\n",
      "Epoch  40 Batch  820/2303   train_loss = 3.083\n",
      "Epoch  40 Batch  840/2303   train_loss = 3.598\n",
      "Epoch  40 Batch  860/2303   train_loss = 3.184\n",
      "Epoch  40 Batch  880/2303   train_loss = 2.969\n",
      "Epoch  40 Batch  900/2303   train_loss = 3.688\n",
      "Epoch  40 Batch  920/2303   train_loss = 3.988\n",
      "Epoch  40 Batch  940/2303   train_loss = 3.641\n",
      "Epoch  40 Batch  960/2303   train_loss = 3.800\n",
      "Epoch  40 Batch  980/2303   train_loss = 3.382\n",
      "Epoch  40 Batch 1000/2303   train_loss = 3.595\n",
      "Epoch  40 Batch 1020/2303   train_loss = 3.832\n",
      "Epoch  40 Batch 1040/2303   train_loss = 4.182\n",
      "Epoch  40 Batch 1060/2303   train_loss = 3.211\n",
      "Epoch  40 Batch 1080/2303   train_loss = 4.713\n",
      "Epoch  40 Batch 1100/2303   train_loss = 4.065\n",
      "Epoch  40 Batch 1120/2303   train_loss = 3.557\n",
      "Epoch  40 Batch 1140/2303   train_loss = 3.912\n",
      "Epoch  40 Batch 1160/2303   train_loss = 3.829\n",
      "Epoch  40 Batch 1180/2303   train_loss = 3.272\n",
      "Epoch  40 Batch 1200/2303   train_loss = 5.331\n",
      "Epoch  40 Batch 1220/2303   train_loss = 4.692\n",
      "Epoch  40 Batch 1240/2303   train_loss = 4.082\n",
      "Epoch  40 Batch 1260/2303   train_loss = 4.322\n",
      "Epoch  40 Batch 1280/2303   train_loss = 3.727\n",
      "Epoch  40 Batch 1300/2303   train_loss = 4.118\n",
      "Epoch  40 Batch 1320/2303   train_loss = 3.628\n",
      "Epoch  40 Batch 1340/2303   train_loss = 3.141\n",
      "Epoch  40 Batch 1360/2303   train_loss = 3.429\n",
      "Epoch  40 Batch 1380/2303   train_loss = 3.599\n",
      "Epoch  40 Batch 1400/2303   train_loss = 3.589\n",
      "Epoch  40 Batch 1420/2303   train_loss = 3.461\n",
      "Epoch  40 Batch 1440/2303   train_loss = 4.056\n",
      "Epoch  40 Batch 1460/2303   train_loss = 4.298\n",
      "Epoch  40 Batch 1480/2303   train_loss = 3.613\n",
      "Epoch  40 Batch 1500/2303   train_loss = 3.951\n",
      "Epoch  40 Batch 1520/2303   train_loss = 3.361\n",
      "Epoch  40 Batch 1540/2303   train_loss = 3.031\n",
      "Epoch  40 Batch 1560/2303   train_loss = 3.169\n",
      "Epoch  40 Batch 1580/2303   train_loss = 3.083\n",
      "Epoch  40 Batch 1600/2303   train_loss = 5.344\n",
      "Epoch  40 Batch 1620/2303   train_loss = 3.844\n",
      "Epoch  40 Batch 1640/2303   train_loss = 3.682\n",
      "Epoch  40 Batch 1660/2303   train_loss = 4.340\n",
      "Epoch  40 Batch 1680/2303   train_loss = 3.283\n",
      "Epoch  40 Batch 1700/2303   train_loss = 3.462\n",
      "Epoch  40 Batch 1720/2303   train_loss = 4.065\n",
      "Epoch  40 Batch 1740/2303   train_loss = 3.920\n",
      "Epoch  40 Batch 1760/2303   train_loss = 3.669\n",
      "Epoch  40 Batch 1780/2303   train_loss = 2.813\n",
      "Epoch  40 Batch 1800/2303   train_loss = 3.630\n",
      "Epoch  40 Batch 1820/2303   train_loss = 3.456\n",
      "Epoch  40 Batch 1840/2303   train_loss = 3.817\n",
      "Epoch  40 Batch 1860/2303   train_loss = 3.141\n",
      "Epoch  40 Batch 1880/2303   train_loss = 4.361\n",
      "Epoch  40 Batch 1900/2303   train_loss = 3.962\n",
      "Epoch  40 Batch 1920/2303   train_loss = 3.802\n",
      "Epoch  40 Batch 1940/2303   train_loss = 3.271\n",
      "Epoch  40 Batch 1960/2303   train_loss = 4.486\n",
      "Epoch  40 Batch 1980/2303   train_loss = 3.874\n",
      "Epoch  40 Batch 2000/2303   train_loss = 3.920\n",
      "Epoch  40 Batch 2020/2303   train_loss = 3.822\n",
      "Epoch  40 Batch 2040/2303   train_loss = 3.535\n",
      "Epoch  40 Batch 2060/2303   train_loss = 3.833\n",
      "Epoch  40 Batch 2080/2303   train_loss = 3.565\n",
      "Epoch  40 Batch 2100/2303   train_loss = 5.063\n",
      "Epoch  40 Batch 2120/2303   train_loss = 3.087\n",
      "Epoch  40 Batch 2140/2303   train_loss = 3.196\n",
      "Epoch  40 Batch 2160/2303   train_loss = 3.808\n",
      "Epoch  40 Batch 2180/2303   train_loss = 4.001\n",
      "Epoch  40 Batch 2200/2303   train_loss = 2.709\n",
      "Epoch  40 Batch 2220/2303   train_loss = 2.535\n",
      "Epoch  40 Batch 2240/2303   train_loss = 3.677\n",
      "Epoch  40 Batch 2260/2303   train_loss = 3.473\n",
      "Epoch  40 Batch 2280/2303   train_loss = 4.000\n",
      "Epoch  40 Batch 2300/2303   train_loss = 4.626\n",
      "Epoch  41 Batch   17/2303   train_loss = 3.545\n",
      "Epoch  41 Batch   37/2303   train_loss = 4.702\n",
      "Epoch  41 Batch   57/2303   train_loss = 3.331\n",
      "Epoch  41 Batch   77/2303   train_loss = 3.924\n",
      "Epoch  41 Batch   97/2303   train_loss = 4.360\n",
      "Epoch  41 Batch  117/2303   train_loss = 3.599\n",
      "Epoch  41 Batch  137/2303   train_loss = 3.418\n",
      "Epoch  41 Batch  157/2303   train_loss = 3.159\n",
      "Epoch  41 Batch  177/2303   train_loss = 4.425\n",
      "Epoch  41 Batch  197/2303   train_loss = 2.829\n",
      "Epoch  41 Batch  217/2303   train_loss = 3.599\n",
      "Epoch  41 Batch  237/2303   train_loss = 3.613\n",
      "Epoch  41 Batch  257/2303   train_loss = 3.264\n",
      "Epoch  41 Batch  277/2303   train_loss = 3.082\n",
      "Epoch  41 Batch  297/2303   train_loss = 3.765\n",
      "Epoch  41 Batch  317/2303   train_loss = 3.336\n",
      "Epoch  41 Batch  337/2303   train_loss = 4.320\n",
      "Epoch  41 Batch  357/2303   train_loss = 3.666\n",
      "Epoch  41 Batch  377/2303   train_loss = 4.032\n",
      "Epoch  41 Batch  397/2303   train_loss = 3.905\n",
      "Epoch  41 Batch  417/2303   train_loss = 4.698\n",
      "Epoch  41 Batch  437/2303   train_loss = 4.316\n",
      "Epoch  41 Batch  457/2303   train_loss = 4.463\n",
      "Epoch  41 Batch  477/2303   train_loss = 4.019\n",
      "Epoch  41 Batch  497/2303   train_loss = 4.219\n",
      "Epoch  41 Batch  517/2303   train_loss = 3.571\n",
      "Epoch  41 Batch  537/2303   train_loss = 3.632\n",
      "Epoch  41 Batch  557/2303   train_loss = 4.039\n",
      "Epoch  41 Batch  577/2303   train_loss = 3.664\n",
      "Epoch  41 Batch  597/2303   train_loss = 3.797\n",
      "Epoch  41 Batch  617/2303   train_loss = 3.151\n",
      "Epoch  41 Batch  637/2303   train_loss = 3.487\n",
      "Epoch  41 Batch  657/2303   train_loss = 3.387\n",
      "Epoch  41 Batch  677/2303   train_loss = 3.471\n",
      "Epoch  41 Batch  697/2303   train_loss = 3.789\n",
      "Epoch  41 Batch  717/2303   train_loss = 3.159\n",
      "Epoch  41 Batch  737/2303   train_loss = 3.065\n",
      "Epoch  41 Batch  757/2303   train_loss = 3.892\n",
      "Epoch  41 Batch  777/2303   train_loss = 3.547\n",
      "Epoch  41 Batch  797/2303   train_loss = 3.996\n",
      "Epoch  41 Batch  817/2303   train_loss = 4.114\n",
      "Epoch  41 Batch  837/2303   train_loss = 3.551\n",
      "Epoch  41 Batch  857/2303   train_loss = 4.169\n",
      "Epoch  41 Batch  877/2303   train_loss = 3.760\n",
      "Epoch  41 Batch  897/2303   train_loss = 3.805\n",
      "Epoch  41 Batch  917/2303   train_loss = 3.856\n",
      "Epoch  41 Batch  937/2303   train_loss = 3.791\n",
      "Epoch  41 Batch  957/2303   train_loss = 3.503\n",
      "Epoch  41 Batch  977/2303   train_loss = 3.682\n",
      "Epoch  41 Batch  997/2303   train_loss = 3.572\n",
      "Epoch  41 Batch 1017/2303   train_loss = 3.577\n",
      "Epoch  41 Batch 1037/2303   train_loss = 3.566\n",
      "Epoch  41 Batch 1057/2303   train_loss = 3.461\n",
      "Epoch  41 Batch 1077/2303   train_loss = 4.551\n",
      "Epoch  41 Batch 1097/2303   train_loss = 4.115\n",
      "Epoch  41 Batch 1117/2303   train_loss = 6.115\n",
      "Epoch  41 Batch 1137/2303   train_loss = 4.227\n",
      "Epoch  41 Batch 1157/2303   train_loss = 3.760\n",
      "Epoch  41 Batch 1177/2303   train_loss = 4.020\n",
      "Epoch  41 Batch 1197/2303   train_loss = 4.421\n",
      "Epoch  41 Batch 1217/2303   train_loss = 3.910\n",
      "Epoch  41 Batch 1237/2303   train_loss = 3.827\n",
      "Epoch  41 Batch 1257/2303   train_loss = 3.706\n",
      "Epoch  41 Batch 1277/2303   train_loss = 4.037\n",
      "Epoch  41 Batch 1297/2303   train_loss = 3.612\n",
      "Epoch  41 Batch 1317/2303   train_loss = 4.271\n",
      "Epoch  41 Batch 1337/2303   train_loss = 4.393\n",
      "Epoch  41 Batch 1357/2303   train_loss = 3.139\n",
      "Epoch  41 Batch 1377/2303   train_loss = 3.589\n",
      "Epoch  41 Batch 1397/2303   train_loss = 3.462\n",
      "Epoch  41 Batch 1417/2303   train_loss = 2.629\n",
      "Epoch  41 Batch 1437/2303   train_loss = 4.426\n",
      "Epoch  41 Batch 1457/2303   train_loss = 3.760\n",
      "Epoch  41 Batch 1477/2303   train_loss = 4.579\n",
      "Epoch  41 Batch 1497/2303   train_loss = 3.129\n",
      "Epoch  41 Batch 1517/2303   train_loss = 3.857\n",
      "Epoch  41 Batch 1537/2303   train_loss = 4.616\n",
      "Epoch  41 Batch 1557/2303   train_loss = 4.692\n",
      "Epoch  41 Batch 1577/2303   train_loss = 4.052\n",
      "Epoch  41 Batch 1597/2303   train_loss = 4.164\n",
      "Epoch  41 Batch 1617/2303   train_loss = 3.806\n",
      "Epoch  41 Batch 1637/2303   train_loss = 3.520\n",
      "Epoch  41 Batch 1657/2303   train_loss = 3.336\n",
      "Epoch  41 Batch 1677/2303   train_loss = 3.485\n",
      "Epoch  41 Batch 1697/2303   train_loss = 3.281\n",
      "Epoch  41 Batch 1717/2303   train_loss = 3.939\n",
      "Epoch  41 Batch 1737/2303   train_loss = 3.888\n",
      "Epoch  41 Batch 1757/2303   train_loss = 4.168\n",
      "Epoch  41 Batch 1777/2303   train_loss = 4.125\n",
      "Epoch  41 Batch 1797/2303   train_loss = 2.958\n",
      "Epoch  41 Batch 1817/2303   train_loss = 4.477\n",
      "Epoch  41 Batch 1837/2303   train_loss = 4.397\n",
      "Epoch  41 Batch 1857/2303   train_loss = 3.535\n",
      "Epoch  41 Batch 1877/2303   train_loss = 3.802\n",
      "Epoch  41 Batch 1897/2303   train_loss = 4.309\n",
      "Epoch  41 Batch 1917/2303   train_loss = 3.439\n",
      "Epoch  41 Batch 1937/2303   train_loss = 3.647\n",
      "Epoch  41 Batch 1957/2303   train_loss = 4.094\n",
      "Epoch  41 Batch 1977/2303   train_loss = 4.382\n",
      "Epoch  41 Batch 1997/2303   train_loss = 3.669\n",
      "Epoch  41 Batch 2017/2303   train_loss = 3.264\n",
      "Epoch  41 Batch 2037/2303   train_loss = 3.949\n",
      "Epoch  41 Batch 2057/2303   train_loss = 3.967\n",
      "Epoch  41 Batch 2077/2303   train_loss = 3.679\n",
      "Epoch  41 Batch 2097/2303   train_loss = 2.838\n",
      "Epoch  41 Batch 2117/2303   train_loss = 4.777\n",
      "Epoch  41 Batch 2137/2303   train_loss = 3.821\n",
      "Epoch  41 Batch 2157/2303   train_loss = 3.676\n",
      "Epoch  41 Batch 2177/2303   train_loss = 2.800\n",
      "Epoch  41 Batch 2197/2303   train_loss = 3.476\n",
      "Epoch  41 Batch 2217/2303   train_loss = 3.671\n",
      "Epoch  41 Batch 2237/2303   train_loss = 3.843\n",
      "Epoch  41 Batch 2257/2303   train_loss = 3.813\n",
      "Epoch  41 Batch 2277/2303   train_loss = 4.193\n",
      "Epoch  41 Batch 2297/2303   train_loss = 3.783\n",
      "Epoch  42 Batch   14/2303   train_loss = 4.362\n",
      "Epoch  42 Batch   34/2303   train_loss = 4.078\n",
      "Epoch  42 Batch   54/2303   train_loss = 3.784\n",
      "Epoch  42 Batch   74/2303   train_loss = 3.623\n",
      "Epoch  42 Batch   94/2303   train_loss = 4.167\n",
      "Epoch  42 Batch  114/2303   train_loss = 3.643\n",
      "Epoch  42 Batch  134/2303   train_loss = 3.527\n",
      "Epoch  42 Batch  154/2303   train_loss = 4.987\n",
      "Epoch  42 Batch  174/2303   train_loss = 3.556\n",
      "Epoch  42 Batch  194/2303   train_loss = 4.214\n",
      "Epoch  42 Batch  214/2303   train_loss = 4.222\n",
      "Epoch  42 Batch  234/2303   train_loss = 3.473\n",
      "Epoch  42 Batch  254/2303   train_loss = 3.467\n",
      "Epoch  42 Batch  274/2303   train_loss = 3.312\n",
      "Epoch  42 Batch  294/2303   train_loss = 3.798\n",
      "Epoch  42 Batch  314/2303   train_loss = 3.165\n",
      "Epoch  42 Batch  334/2303   train_loss = 3.153\n",
      "Epoch  42 Batch  354/2303   train_loss = 2.838\n",
      "Epoch  42 Batch  374/2303   train_loss = 3.494\n",
      "Epoch  42 Batch  394/2303   train_loss = 4.215\n",
      "Epoch  42 Batch  414/2303   train_loss = 4.374\n",
      "Epoch  42 Batch  434/2303   train_loss = 4.075\n",
      "Epoch  42 Batch  454/2303   train_loss = 4.232\n",
      "Epoch  42 Batch  474/2303   train_loss = 3.989\n",
      "Epoch  42 Batch  494/2303   train_loss = 4.912\n",
      "Epoch  42 Batch  514/2303   train_loss = 4.121\n",
      "Epoch  42 Batch  534/2303   train_loss = 4.377\n",
      "Epoch  42 Batch  554/2303   train_loss = 4.933\n",
      "Epoch  42 Batch  574/2303   train_loss = 4.394\n",
      "Epoch  42 Batch  594/2303   train_loss = 3.586\n",
      "Epoch  42 Batch  614/2303   train_loss = 3.564\n",
      "Epoch  42 Batch  634/2303   train_loss = 3.209\n",
      "Epoch  42 Batch  654/2303   train_loss = 3.668\n",
      "Epoch  42 Batch  674/2303   train_loss = 4.114\n",
      "Epoch  42 Batch  694/2303   train_loss = 3.114\n",
      "Epoch  42 Batch  714/2303   train_loss = 3.269\n",
      "Epoch  42 Batch  734/2303   train_loss = 3.775\n",
      "Epoch  42 Batch  754/2303   train_loss = 4.320\n",
      "Epoch  42 Batch  774/2303   train_loss = 4.293\n",
      "Epoch  42 Batch  794/2303   train_loss = 4.455\n",
      "Epoch  42 Batch  814/2303   train_loss = 3.065\n",
      "Epoch  42 Batch  834/2303   train_loss = 3.534\n",
      "Epoch  42 Batch  854/2303   train_loss = 3.259\n",
      "Epoch  42 Batch  874/2303   train_loss = 3.781\n",
      "Epoch  42 Batch  894/2303   train_loss = 4.066\n",
      "Epoch  42 Batch  914/2303   train_loss = 4.787\n",
      "Epoch  42 Batch  934/2303   train_loss = 3.924\n",
      "Epoch  42 Batch  954/2303   train_loss = 3.340\n",
      "Epoch  42 Batch  974/2303   train_loss = 3.193\n",
      "Epoch  42 Batch  994/2303   train_loss = 3.069\n",
      "Epoch  42 Batch 1014/2303   train_loss = 3.861\n",
      "Epoch  42 Batch 1034/2303   train_loss = 3.810\n",
      "Epoch  42 Batch 1054/2303   train_loss = 4.270\n",
      "Epoch  42 Batch 1074/2303   train_loss = 4.073\n",
      "Epoch  42 Batch 1094/2303   train_loss = 4.408\n",
      "Epoch  42 Batch 1114/2303   train_loss = 4.555\n",
      "Epoch  42 Batch 1134/2303   train_loss = 3.606\n",
      "Epoch  42 Batch 1154/2303   train_loss = 3.981\n",
      "Epoch  42 Batch 1174/2303   train_loss = 4.134\n",
      "Epoch  42 Batch 1194/2303   train_loss = 4.204\n",
      "Epoch  42 Batch 1214/2303   train_loss = 3.964\n",
      "Epoch  42 Batch 1234/2303   train_loss = 3.804\n",
      "Epoch  42 Batch 1254/2303   train_loss = 3.929\n",
      "Epoch  42 Batch 1274/2303   train_loss = 4.005\n",
      "Epoch  42 Batch 1294/2303   train_loss = 4.501\n",
      "Epoch  42 Batch 1314/2303   train_loss = 3.473\n",
      "Epoch  42 Batch 1334/2303   train_loss = 3.823\n",
      "Epoch  42 Batch 1354/2303   train_loss = 3.985\n",
      "Epoch  42 Batch 1374/2303   train_loss = 3.949\n",
      "Epoch  42 Batch 1394/2303   train_loss = 3.748\n",
      "Epoch  42 Batch 1414/2303   train_loss = 3.942\n",
      "Epoch  42 Batch 1434/2303   train_loss = 4.326\n",
      "Epoch  42 Batch 1454/2303   train_loss = 4.325\n",
      "Epoch  42 Batch 1474/2303   train_loss = 4.033\n",
      "Epoch  42 Batch 1494/2303   train_loss = 4.055\n",
      "Epoch  42 Batch 1514/2303   train_loss = 3.735\n",
      "Epoch  42 Batch 1534/2303   train_loss = 3.640\n",
      "Epoch  42 Batch 1554/2303   train_loss = 3.400\n",
      "Epoch  42 Batch 1574/2303   train_loss = 4.287\n",
      "Epoch  42 Batch 1594/2303   train_loss = 4.917\n",
      "Epoch  42 Batch 1614/2303   train_loss = 4.356\n",
      "Epoch  42 Batch 1634/2303   train_loss = 3.291\n",
      "Epoch  42 Batch 1654/2303   train_loss = 3.569\n",
      "Epoch  42 Batch 1674/2303   train_loss = 3.989\n",
      "Epoch  42 Batch 1694/2303   train_loss = 3.845\n",
      "Epoch  42 Batch 1714/2303   train_loss = 4.258\n",
      "Epoch  42 Batch 1734/2303   train_loss = 3.771\n",
      "Epoch  42 Batch 1754/2303   train_loss = 3.741\n",
      "Epoch  42 Batch 1774/2303   train_loss = 4.363\n",
      "Epoch  42 Batch 1794/2303   train_loss = 3.714\n",
      "Epoch  42 Batch 1814/2303   train_loss = 3.892\n",
      "Epoch  42 Batch 1834/2303   train_loss = 3.691\n",
      "Epoch  42 Batch 1854/2303   train_loss = 3.026\n",
      "Epoch  42 Batch 1874/2303   train_loss = 4.012\n",
      "Epoch  42 Batch 1894/2303   train_loss = 3.917\n",
      "Epoch  42 Batch 1914/2303   train_loss = 3.985\n",
      "Epoch  42 Batch 1934/2303   train_loss = 4.331\n",
      "Epoch  42 Batch 1954/2303   train_loss = 4.073\n",
      "Epoch  42 Batch 1974/2303   train_loss = 3.155\n",
      "Epoch  42 Batch 1994/2303   train_loss = 3.439\n",
      "Epoch  42 Batch 2014/2303   train_loss = 4.661\n",
      "Epoch  42 Batch 2034/2303   train_loss = 3.939\n",
      "Epoch  42 Batch 2054/2303   train_loss = 4.031\n",
      "Epoch  42 Batch 2074/2303   train_loss = 3.239\n",
      "Epoch  42 Batch 2094/2303   train_loss = 3.569\n",
      "Epoch  42 Batch 2114/2303   train_loss = 3.601\n",
      "Epoch  42 Batch 2134/2303   train_loss = 3.755\n",
      "Epoch  42 Batch 2154/2303   train_loss = 3.794\n",
      "Epoch  42 Batch 2174/2303   train_loss = 3.633\n",
      "Epoch  42 Batch 2194/2303   train_loss = 4.087\n",
      "Epoch  42 Batch 2214/2303   train_loss = 4.199\n",
      "Epoch  42 Batch 2234/2303   train_loss = 3.716\n",
      "Epoch  42 Batch 2254/2303   train_loss = 3.865\n",
      "Epoch  42 Batch 2274/2303   train_loss = 3.198\n",
      "Epoch  42 Batch 2294/2303   train_loss = 3.928\n",
      "Epoch  43 Batch   11/2303   train_loss = 3.929\n",
      "Epoch  43 Batch   31/2303   train_loss = 3.462\n",
      "Epoch  43 Batch   51/2303   train_loss = 4.119\n",
      "Epoch  43 Batch   71/2303   train_loss = 3.496\n",
      "Epoch  43 Batch   91/2303   train_loss = 4.418\n",
      "Epoch  43 Batch  111/2303   train_loss = 3.098\n",
      "Epoch  43 Batch  131/2303   train_loss = 2.876\n",
      "Epoch  43 Batch  151/2303   train_loss = 3.139\n",
      "Epoch  43 Batch  171/2303   train_loss = 3.678\n",
      "Epoch  43 Batch  191/2303   train_loss = 4.419\n",
      "Epoch  43 Batch  211/2303   train_loss = 3.450\n",
      "Epoch  43 Batch  231/2303   train_loss = 3.519\n",
      "Epoch  43 Batch  251/2303   train_loss = 3.377\n",
      "Epoch  43 Batch  271/2303   train_loss = 3.385\n",
      "Epoch  43 Batch  291/2303   train_loss = 4.239\n",
      "Epoch  43 Batch  311/2303   train_loss = 4.159\n",
      "Epoch  43 Batch  331/2303   train_loss = 3.959\n",
      "Epoch  43 Batch  351/2303   train_loss = 4.924\n",
      "Epoch  43 Batch  371/2303   train_loss = 4.288\n",
      "Epoch  43 Batch  391/2303   train_loss = 3.488\n",
      "Epoch  43 Batch  411/2303   train_loss = 3.863\n",
      "Epoch  43 Batch  431/2303   train_loss = 4.471\n",
      "Epoch  43 Batch  451/2303   train_loss = 3.872\n",
      "Epoch  43 Batch  471/2303   train_loss = 2.792\n",
      "Epoch  43 Batch  491/2303   train_loss = 4.758\n",
      "Epoch  43 Batch  511/2303   train_loss = 3.010\n",
      "Epoch  43 Batch  531/2303   train_loss = 4.500\n",
      "Epoch  43 Batch  551/2303   train_loss = 4.289\n",
      "Epoch  43 Batch  571/2303   train_loss = 2.864\n",
      "Epoch  43 Batch  591/2303   train_loss = 4.067\n",
      "Epoch  43 Batch  611/2303   train_loss = 3.560\n",
      "Epoch  43 Batch  631/2303   train_loss = 4.785\n",
      "Epoch  43 Batch  651/2303   train_loss = 4.066\n",
      "Epoch  43 Batch  671/2303   train_loss = 2.904\n",
      "Epoch  43 Batch  691/2303   train_loss = 3.501\n",
      "Epoch  43 Batch  711/2303   train_loss = 3.910\n",
      "Epoch  43 Batch  731/2303   train_loss = 3.791\n",
      "Epoch  43 Batch  751/2303   train_loss = 3.295\n",
      "Epoch  43 Batch  771/2303   train_loss = 4.038\n",
      "Epoch  43 Batch  791/2303   train_loss = 3.965\n",
      "Epoch  43 Batch  811/2303   train_loss = 3.579\n",
      "Epoch  43 Batch  831/2303   train_loss = 4.213\n",
      "Epoch  43 Batch  851/2303   train_loss = 4.006\n",
      "Epoch  43 Batch  871/2303   train_loss = 3.272\n",
      "Epoch  43 Batch  891/2303   train_loss = 4.399\n",
      "Epoch  43 Batch  911/2303   train_loss = 4.470\n",
      "Epoch  43 Batch  931/2303   train_loss = 4.895\n",
      "Epoch  43 Batch  951/2303   train_loss = 3.624\n",
      "Epoch  43 Batch  971/2303   train_loss = 3.895\n",
      "Epoch  43 Batch  991/2303   train_loss = 4.042\n",
      "Epoch  43 Batch 1011/2303   train_loss = 4.187\n",
      "Epoch  43 Batch 1031/2303   train_loss = 3.169\n",
      "Epoch  43 Batch 1051/2303   train_loss = 3.775\n",
      "Epoch  43 Batch 1071/2303   train_loss = 3.989\n",
      "Epoch  43 Batch 1091/2303   train_loss = 3.376\n",
      "Epoch  43 Batch 1111/2303   train_loss = 3.872\n",
      "Epoch  43 Batch 1131/2303   train_loss = 5.300\n",
      "Epoch  43 Batch 1151/2303   train_loss = 3.814\n",
      "Epoch  43 Batch 1171/2303   train_loss = 4.591\n",
      "Epoch  43 Batch 1191/2303   train_loss = 3.952\n",
      "Epoch  43 Batch 1211/2303   train_loss = 4.357\n",
      "Epoch  43 Batch 1231/2303   train_loss = 2.923\n",
      "Epoch  43 Batch 1251/2303   train_loss = 3.565\n",
      "Epoch  43 Batch 1271/2303   train_loss = 3.786\n",
      "Epoch  43 Batch 1291/2303   train_loss = 3.634\n",
      "Epoch  43 Batch 1311/2303   train_loss = 3.991\n",
      "Epoch  43 Batch 1331/2303   train_loss = 3.746\n",
      "Epoch  43 Batch 1351/2303   train_loss = 3.190\n",
      "Epoch  43 Batch 1371/2303   train_loss = 3.668\n",
      "Epoch  43 Batch 1391/2303   train_loss = 3.957\n",
      "Epoch  43 Batch 1411/2303   train_loss = 3.378\n",
      "Epoch  43 Batch 1431/2303   train_loss = 4.007\n",
      "Epoch  43 Batch 1451/2303   train_loss = 4.991\n",
      "Epoch  43 Batch 1471/2303   train_loss = 3.659\n",
      "Epoch  43 Batch 1491/2303   train_loss = 3.354\n",
      "Epoch  43 Batch 1511/2303   train_loss = 4.302\n",
      "Epoch  43 Batch 1531/2303   train_loss = 3.302\n",
      "Epoch  43 Batch 1551/2303   train_loss = 3.262\n",
      "Epoch  43 Batch 1571/2303   train_loss = 3.889\n",
      "Epoch  43 Batch 1591/2303   train_loss = 4.046\n",
      "Epoch  43 Batch 1611/2303   train_loss = 4.028\n",
      "Epoch  43 Batch 1631/2303   train_loss = 4.760\n",
      "Epoch  43 Batch 1651/2303   train_loss = 4.457\n",
      "Epoch  43 Batch 1671/2303   train_loss = 2.708\n",
      "Epoch  43 Batch 1691/2303   train_loss = 3.473\n",
      "Epoch  43 Batch 1711/2303   train_loss = 4.141\n",
      "Epoch  43 Batch 1731/2303   train_loss = 4.050\n",
      "Epoch  43 Batch 1751/2303   train_loss = 4.104\n",
      "Epoch  43 Batch 1771/2303   train_loss = 4.515\n",
      "Epoch  43 Batch 1791/2303   train_loss = 4.064\n",
      "Epoch  43 Batch 1811/2303   train_loss = 3.577\n",
      "Epoch  43 Batch 1831/2303   train_loss = 3.766\n",
      "Epoch  43 Batch 1851/2303   train_loss = 3.786\n",
      "Epoch  43 Batch 1871/2303   train_loss = 3.113\n",
      "Epoch  43 Batch 1891/2303   train_loss = 3.506\n",
      "Epoch  43 Batch 1911/2303   train_loss = 3.594\n",
      "Epoch  43 Batch 1931/2303   train_loss = 4.045\n",
      "Epoch  43 Batch 1951/2303   train_loss = 3.609\n",
      "Epoch  43 Batch 1971/2303   train_loss = 3.426\n",
      "Epoch  43 Batch 1991/2303   train_loss = 4.475\n",
      "Epoch  43 Batch 2011/2303   train_loss = 3.207\n",
      "Epoch  43 Batch 2031/2303   train_loss = 4.147\n",
      "Epoch  43 Batch 2051/2303   train_loss = 4.670\n",
      "Epoch  43 Batch 2071/2303   train_loss = 3.410\n",
      "Epoch  43 Batch 2091/2303   train_loss = 2.942\n",
      "Epoch  43 Batch 2111/2303   train_loss = 3.337\n",
      "Epoch  43 Batch 2131/2303   train_loss = 3.140\n",
      "Epoch  43 Batch 2151/2303   train_loss = 3.806\n",
      "Epoch  43 Batch 2171/2303   train_loss = 3.150\n",
      "Epoch  43 Batch 2191/2303   train_loss = 3.266\n",
      "Epoch  43 Batch 2211/2303   train_loss = 3.923\n",
      "Epoch  43 Batch 2231/2303   train_loss = 3.076\n",
      "Epoch  43 Batch 2251/2303   train_loss = 4.751\n",
      "Epoch  43 Batch 2271/2303   train_loss = 4.898\n",
      "Epoch  43 Batch 2291/2303   train_loss = 3.601\n",
      "Epoch  44 Batch    8/2303   train_loss = 3.211\n",
      "Epoch  44 Batch   28/2303   train_loss = 3.983\n",
      "Epoch  44 Batch   48/2303   train_loss = 3.841\n",
      "Epoch  44 Batch   68/2303   train_loss = 3.632\n",
      "Epoch  44 Batch   88/2303   train_loss = 3.876\n",
      "Epoch  44 Batch  108/2303   train_loss = 3.922\n",
      "Epoch  44 Batch  128/2303   train_loss = 2.912\n",
      "Epoch  44 Batch  148/2303   train_loss = 3.146\n",
      "Epoch  44 Batch  168/2303   train_loss = 4.260\n",
      "Epoch  44 Batch  188/2303   train_loss = 4.327\n",
      "Epoch  44 Batch  208/2303   train_loss = 3.043\n",
      "Epoch  44 Batch  228/2303   train_loss = 4.019\n",
      "Epoch  44 Batch  248/2303   train_loss = 3.341\n",
      "Epoch  44 Batch  268/2303   train_loss = 4.305\n",
      "Epoch  44 Batch  288/2303   train_loss = 3.541\n",
      "Epoch  44 Batch  308/2303   train_loss = 3.815\n",
      "Epoch  44 Batch  328/2303   train_loss = 3.284\n",
      "Epoch  44 Batch  348/2303   train_loss = 3.950\n",
      "Epoch  44 Batch  368/2303   train_loss = 4.852\n",
      "Epoch  44 Batch  388/2303   train_loss = 3.473\n",
      "Epoch  44 Batch  408/2303   train_loss = 3.538\n",
      "Epoch  44 Batch  428/2303   train_loss = 3.192\n",
      "Epoch  44 Batch  448/2303   train_loss = 3.549\n",
      "Epoch  44 Batch  468/2303   train_loss = 3.474\n",
      "Epoch  44 Batch  488/2303   train_loss = 4.360\n",
      "Epoch  44 Batch  508/2303   train_loss = 3.425\n",
      "Epoch  44 Batch  528/2303   train_loss = 3.401\n",
      "Epoch  44 Batch  548/2303   train_loss = 4.152\n",
      "Epoch  44 Batch  568/2303   train_loss = 3.504\n",
      "Epoch  44 Batch  588/2303   train_loss = 4.508\n",
      "Epoch  44 Batch  608/2303   train_loss = 3.230\n",
      "Epoch  44 Batch  628/2303   train_loss = 3.077\n",
      "Epoch  44 Batch  648/2303   train_loss = 3.887\n",
      "Epoch  44 Batch  668/2303   train_loss = 3.838\n",
      "Epoch  44 Batch  688/2303   train_loss = 3.616\n",
      "Epoch  44 Batch  708/2303   train_loss = 4.545\n",
      "Epoch  44 Batch  728/2303   train_loss = 5.070\n",
      "Epoch  44 Batch  748/2303   train_loss = 4.302\n",
      "Epoch  44 Batch  768/2303   train_loss = 3.986\n",
      "Epoch  44 Batch  788/2303   train_loss = 3.462\n",
      "Epoch  44 Batch  808/2303   train_loss = 3.562\n",
      "Epoch  44 Batch  828/2303   train_loss = 3.646\n",
      "Epoch  44 Batch  848/2303   train_loss = 3.474\n",
      "Epoch  44 Batch  868/2303   train_loss = 3.418\n",
      "Epoch  44 Batch  888/2303   train_loss = 3.430\n",
      "Epoch  44 Batch  908/2303   train_loss = 3.918\n",
      "Epoch  44 Batch  928/2303   train_loss = 3.615\n",
      "Epoch  44 Batch  948/2303   train_loss = 4.496\n",
      "Epoch  44 Batch  968/2303   train_loss = 3.182\n",
      "Epoch  44 Batch  988/2303   train_loss = 3.464\n",
      "Epoch  44 Batch 1008/2303   train_loss = 4.174\n",
      "Epoch  44 Batch 1028/2303   train_loss = 3.071\n",
      "Epoch  44 Batch 1048/2303   train_loss = 4.627\n",
      "Epoch  44 Batch 1068/2303   train_loss = 3.678\n",
      "Epoch  44 Batch 1088/2303   train_loss = 4.046\n",
      "Epoch  44 Batch 1108/2303   train_loss = 4.217\n",
      "Epoch  44 Batch 1128/2303   train_loss = 3.234\n",
      "Epoch  44 Batch 1148/2303   train_loss = 4.084\n",
      "Epoch  44 Batch 1168/2303   train_loss = 4.259\n",
      "Epoch  44 Batch 1188/2303   train_loss = 3.918\n",
      "Epoch  44 Batch 1208/2303   train_loss = 3.697\n",
      "Epoch  44 Batch 1228/2303   train_loss = 4.095\n",
      "Epoch  44 Batch 1248/2303   train_loss = 3.954\n",
      "Epoch  44 Batch 1268/2303   train_loss = 4.043\n",
      "Epoch  44 Batch 1288/2303   train_loss = 4.693\n",
      "Epoch  44 Batch 1308/2303   train_loss = 4.021\n",
      "Epoch  44 Batch 1328/2303   train_loss = 4.152\n",
      "Epoch  44 Batch 1348/2303   train_loss = 3.263\n",
      "Epoch  44 Batch 1368/2303   train_loss = 4.185\n",
      "Epoch  44 Batch 1388/2303   train_loss = 4.007\n",
      "Epoch  44 Batch 1408/2303   train_loss = 4.268\n",
      "Epoch  44 Batch 1428/2303   train_loss = 4.546\n",
      "Epoch  44 Batch 1448/2303   train_loss = 3.464\n",
      "Epoch  44 Batch 1468/2303   train_loss = 3.463\n",
      "Epoch  44 Batch 1488/2303   train_loss = 3.887\n",
      "Epoch  44 Batch 1508/2303   train_loss = 3.454\n",
      "Epoch  44 Batch 1528/2303   train_loss = 4.222\n",
      "Epoch  44 Batch 1548/2303   train_loss = 4.473\n",
      "Epoch  44 Batch 1568/2303   train_loss = 4.513\n",
      "Epoch  44 Batch 1588/2303   train_loss = 3.995\n",
      "Epoch  44 Batch 1608/2303   train_loss = 3.679\n",
      "Epoch  44 Batch 1628/2303   train_loss = 4.710\n",
      "Epoch  44 Batch 1648/2303   train_loss = 3.842\n",
      "Epoch  44 Batch 1668/2303   train_loss = 3.745\n",
      "Epoch  44 Batch 1688/2303   train_loss = 4.367\n",
      "Epoch  44 Batch 1708/2303   train_loss = 4.607\n",
      "Epoch  44 Batch 1728/2303   train_loss = 3.744\n",
      "Epoch  44 Batch 1748/2303   train_loss = 3.438\n",
      "Epoch  44 Batch 1768/2303   train_loss = 4.077\n",
      "Epoch  44 Batch 1788/2303   train_loss = 4.439\n",
      "Epoch  44 Batch 1808/2303   train_loss = 3.610\n",
      "Epoch  44 Batch 1828/2303   train_loss = 3.285\n",
      "Epoch  44 Batch 1848/2303   train_loss = 3.940\n",
      "Epoch  44 Batch 1868/2303   train_loss = 2.905\n",
      "Epoch  44 Batch 1888/2303   train_loss = 4.301\n",
      "Epoch  44 Batch 1908/2303   train_loss = 4.457\n",
      "Epoch  44 Batch 1928/2303   train_loss = 3.650\n",
      "Epoch  44 Batch 1948/2303   train_loss = 3.839\n",
      "Epoch  44 Batch 1968/2303   train_loss = 3.671\n",
      "Epoch  44 Batch 1988/2303   train_loss = 3.811\n",
      "Epoch  44 Batch 2008/2303   train_loss = 3.630\n",
      "Epoch  44 Batch 2028/2303   train_loss = 4.103\n",
      "Epoch  44 Batch 2048/2303   train_loss = 3.451\n",
      "Epoch  44 Batch 2068/2303   train_loss = 3.512\n",
      "Epoch  44 Batch 2088/2303   train_loss = 4.549\n",
      "Epoch  44 Batch 2108/2303   train_loss = 3.974\n",
      "Epoch  44 Batch 2128/2303   train_loss = 3.647\n",
      "Epoch  44 Batch 2148/2303   train_loss = 3.451\n",
      "Epoch  44 Batch 2168/2303   train_loss = 2.843\n",
      "Epoch  44 Batch 2188/2303   train_loss = 3.389\n",
      "Epoch  44 Batch 2208/2303   train_loss = 3.838\n",
      "Epoch  44 Batch 2228/2303   train_loss = 4.157\n",
      "Epoch  44 Batch 2248/2303   train_loss = 3.535\n",
      "Epoch  44 Batch 2268/2303   train_loss = 3.348\n",
      "Epoch  44 Batch 2288/2303   train_loss = 3.854\n",
      "Epoch  45 Batch    5/2303   train_loss = 2.951\n",
      "Epoch  45 Batch   25/2303   train_loss = 3.723\n",
      "Epoch  45 Batch   45/2303   train_loss = 3.223\n",
      "Epoch  45 Batch   65/2303   train_loss = 3.505\n",
      "Epoch  45 Batch   85/2303   train_loss = 3.917\n",
      "Epoch  45 Batch  105/2303   train_loss = 3.433\n",
      "Epoch  45 Batch  125/2303   train_loss = 3.313\n",
      "Epoch  45 Batch  145/2303   train_loss = 3.820\n",
      "Epoch  45 Batch  165/2303   train_loss = 3.698\n",
      "Epoch  45 Batch  185/2303   train_loss = 3.666\n",
      "Epoch  45 Batch  205/2303   train_loss = 3.739\n",
      "Epoch  45 Batch  225/2303   train_loss = 3.481\n",
      "Epoch  45 Batch  245/2303   train_loss = 3.201\n",
      "Epoch  45 Batch  265/2303   train_loss = 3.789\n",
      "Epoch  45 Batch  285/2303   train_loss = 3.027\n",
      "Epoch  45 Batch  305/2303   train_loss = 4.497\n",
      "Epoch  45 Batch  325/2303   train_loss = 3.327\n",
      "Epoch  45 Batch  345/2303   train_loss = 3.736\n",
      "Epoch  45 Batch  365/2303   train_loss = 3.966\n",
      "Epoch  45 Batch  385/2303   train_loss = 4.286\n",
      "Epoch  45 Batch  405/2303   train_loss = 3.281\n",
      "Epoch  45 Batch  425/2303   train_loss = 3.569\n",
      "Epoch  45 Batch  445/2303   train_loss = 3.631\n",
      "Epoch  45 Batch  465/2303   train_loss = 4.123\n",
      "Epoch  45 Batch  485/2303   train_loss = 4.025\n",
      "Epoch  45 Batch  505/2303   train_loss = 3.800\n",
      "Epoch  45 Batch  525/2303   train_loss = 2.668\n",
      "Epoch  45 Batch  545/2303   train_loss = 4.121\n",
      "Epoch  45 Batch  565/2303   train_loss = 4.058\n",
      "Epoch  45 Batch  585/2303   train_loss = 3.944\n",
      "Epoch  45 Batch  605/2303   train_loss = 3.969\n",
      "Epoch  45 Batch  625/2303   train_loss = 4.697\n",
      "Epoch  45 Batch  645/2303   train_loss = 3.977\n",
      "Epoch  45 Batch  665/2303   train_loss = 4.115\n",
      "Epoch  45 Batch  685/2303   train_loss = 3.557\n",
      "Epoch  45 Batch  705/2303   train_loss = 3.096\n",
      "Epoch  45 Batch  725/2303   train_loss = 3.820\n",
      "Epoch  45 Batch  745/2303   train_loss = 2.978\n",
      "Epoch  45 Batch  765/2303   train_loss = 3.821\n",
      "Epoch  45 Batch  785/2303   train_loss = 4.972\n",
      "Epoch  45 Batch  805/2303   train_loss = 3.980\n",
      "Epoch  45 Batch  825/2303   train_loss = 3.312\n",
      "Epoch  45 Batch  845/2303   train_loss = 3.893\n",
      "Epoch  45 Batch  865/2303   train_loss = 3.818\n",
      "Epoch  45 Batch  885/2303   train_loss = 3.755\n",
      "Epoch  45 Batch  905/2303   train_loss = 3.006\n",
      "Epoch  45 Batch  925/2303   train_loss = 4.202\n",
      "Epoch  45 Batch  945/2303   train_loss = 3.847\n",
      "Epoch  45 Batch  965/2303   train_loss = 4.055\n",
      "Epoch  45 Batch  985/2303   train_loss = 3.307\n",
      "Epoch  45 Batch 1005/2303   train_loss = 2.824\n",
      "Epoch  45 Batch 1025/2303   train_loss = 3.880\n",
      "Epoch  45 Batch 1045/2303   train_loss = 3.726\n",
      "Epoch  45 Batch 1065/2303   train_loss = 3.566\n",
      "Epoch  45 Batch 1085/2303   train_loss = 3.330\n",
      "Epoch  45 Batch 1105/2303   train_loss = 4.165\n",
      "Epoch  45 Batch 1125/2303   train_loss = 3.208\n",
      "Epoch  45 Batch 1145/2303   train_loss = 3.445\n",
      "Epoch  45 Batch 1165/2303   train_loss = 3.673\n",
      "Epoch  45 Batch 1185/2303   train_loss = 4.187\n",
      "Epoch  45 Batch 1205/2303   train_loss = 3.738\n",
      "Epoch  45 Batch 1225/2303   train_loss = 3.962\n",
      "Epoch  45 Batch 1245/2303   train_loss = 3.379\n",
      "Epoch  45 Batch 1265/2303   train_loss = 3.633\n",
      "Epoch  45 Batch 1285/2303   train_loss = 4.413\n",
      "Epoch  45 Batch 1305/2303   train_loss = 3.574\n",
      "Epoch  45 Batch 1325/2303   train_loss = 3.903\n",
      "Epoch  45 Batch 1345/2303   train_loss = 4.348\n",
      "Epoch  45 Batch 1365/2303   train_loss = 3.411\n",
      "Epoch  45 Batch 1385/2303   train_loss = 4.268\n",
      "Epoch  45 Batch 1405/2303   train_loss = 3.629\n",
      "Epoch  45 Batch 1425/2303   train_loss = 4.277\n",
      "Epoch  45 Batch 1445/2303   train_loss = 3.806\n",
      "Epoch  45 Batch 1465/2303   train_loss = 3.713\n",
      "Epoch  45 Batch 1485/2303   train_loss = 4.603\n",
      "Epoch  45 Batch 1505/2303   train_loss = 3.330\n",
      "Epoch  45 Batch 1525/2303   train_loss = 4.416\n",
      "Epoch  45 Batch 1545/2303   train_loss = 4.111\n",
      "Epoch  45 Batch 1565/2303   train_loss = 3.769\n",
      "Epoch  45 Batch 1585/2303   train_loss = 3.406\n",
      "Epoch  45 Batch 1605/2303   train_loss = 3.263\n",
      "Epoch  45 Batch 1625/2303   train_loss = 3.741\n",
      "Epoch  45 Batch 1645/2303   train_loss = 4.529\n",
      "Epoch  45 Batch 1665/2303   train_loss = 4.320\n",
      "Epoch  45 Batch 1685/2303   train_loss = 3.909\n",
      "Epoch  45 Batch 1705/2303   train_loss = 4.169\n",
      "Epoch  45 Batch 1725/2303   train_loss = 3.876\n",
      "Epoch  45 Batch 1745/2303   train_loss = 3.511\n",
      "Epoch  45 Batch 1765/2303   train_loss = 3.243\n",
      "Epoch  45 Batch 1785/2303   train_loss = 3.934\n",
      "Epoch  45 Batch 1805/2303   train_loss = 3.044\n",
      "Epoch  45 Batch 1825/2303   train_loss = 4.282\n",
      "Epoch  45 Batch 1845/2303   train_loss = 4.045\n",
      "Epoch  45 Batch 1865/2303   train_loss = 3.578\n",
      "Epoch  45 Batch 1885/2303   train_loss = 3.767\n",
      "Epoch  45 Batch 1905/2303   train_loss = 4.104\n",
      "Epoch  45 Batch 1925/2303   train_loss = 3.790\n",
      "Epoch  45 Batch 1945/2303   train_loss = 3.848\n",
      "Epoch  45 Batch 1965/2303   train_loss = 3.506\n",
      "Epoch  45 Batch 1985/2303   train_loss = 4.997\n",
      "Epoch  45 Batch 2005/2303   train_loss = 4.043\n",
      "Epoch  45 Batch 2025/2303   train_loss = 3.610\n",
      "Epoch  45 Batch 2045/2303   train_loss = 3.890\n",
      "Epoch  45 Batch 2065/2303   train_loss = 4.530\n",
      "Epoch  45 Batch 2085/2303   train_loss = 3.711\n",
      "Epoch  45 Batch 2105/2303   train_loss = 3.603\n",
      "Epoch  45 Batch 2125/2303   train_loss = 3.854\n",
      "Epoch  45 Batch 2145/2303   train_loss = 4.213\n",
      "Epoch  45 Batch 2165/2303   train_loss = 4.808\n",
      "Epoch  45 Batch 2185/2303   train_loss = 4.313\n",
      "Epoch  45 Batch 2205/2303   train_loss = 3.484\n",
      "Epoch  45 Batch 2225/2303   train_loss = 4.895\n",
      "Epoch  45 Batch 2245/2303   train_loss = 3.067\n",
      "Epoch  45 Batch 2265/2303   train_loss = 3.121\n",
      "Epoch  45 Batch 2285/2303   train_loss = 3.033\n",
      "Epoch  46 Batch    2/2303   train_loss = 3.528\n",
      "Epoch  46 Batch   22/2303   train_loss = 4.025\n",
      "Epoch  46 Batch   42/2303   train_loss = 3.691\n",
      "Epoch  46 Batch   62/2303   train_loss = 3.960\n",
      "Epoch  46 Batch   82/2303   train_loss = 3.310\n",
      "Epoch  46 Batch  102/2303   train_loss = 3.037\n",
      "Epoch  46 Batch  122/2303   train_loss = 4.419\n",
      "Epoch  46 Batch  142/2303   train_loss = 3.312\n",
      "Epoch  46 Batch  162/2303   train_loss = 2.622\n",
      "Epoch  46 Batch  182/2303   train_loss = 3.911\n",
      "Epoch  46 Batch  202/2303   train_loss = 2.910\n",
      "Epoch  46 Batch  222/2303   train_loss = 4.116\n",
      "Epoch  46 Batch  242/2303   train_loss = 3.957\n",
      "Epoch  46 Batch  262/2303   train_loss = 3.283\n",
      "Epoch  46 Batch  282/2303   train_loss = 3.198\n",
      "Epoch  46 Batch  302/2303   train_loss = 3.091\n",
      "Epoch  46 Batch  322/2303   train_loss = 3.795\n",
      "Epoch  46 Batch  342/2303   train_loss = 3.912\n",
      "Epoch  46 Batch  362/2303   train_loss = 3.600\n",
      "Epoch  46 Batch  382/2303   train_loss = 3.889\n",
      "Epoch  46 Batch  402/2303   train_loss = 4.276\n",
      "Epoch  46 Batch  422/2303   train_loss = 4.976\n",
      "Epoch  46 Batch  442/2303   train_loss = 3.044\n",
      "Epoch  46 Batch  462/2303   train_loss = 3.910\n",
      "Epoch  46 Batch  482/2303   train_loss = 4.050\n",
      "Epoch  46 Batch  502/2303   train_loss = 4.890\n",
      "Epoch  46 Batch  522/2303   train_loss = 3.738\n",
      "Epoch  46 Batch  542/2303   train_loss = 4.125\n",
      "Epoch  46 Batch  562/2303   train_loss = 4.283\n",
      "Epoch  46 Batch  582/2303   train_loss = 3.665\n",
      "Epoch  46 Batch  602/2303   train_loss = 4.219\n",
      "Epoch  46 Batch  622/2303   train_loss = 3.786\n",
      "Epoch  46 Batch  642/2303   train_loss = 3.588\n",
      "Epoch  46 Batch  662/2303   train_loss = 4.565\n",
      "Epoch  46 Batch  682/2303   train_loss = 3.870\n",
      "Epoch  46 Batch  702/2303   train_loss = 3.864\n",
      "Epoch  46 Batch  722/2303   train_loss = 3.854\n",
      "Epoch  46 Batch  742/2303   train_loss = 3.278\n",
      "Epoch  46 Batch  762/2303   train_loss = 3.861\n",
      "Epoch  46 Batch  782/2303   train_loss = 3.104\n",
      "Epoch  46 Batch  802/2303   train_loss = 3.801\n",
      "Epoch  46 Batch  822/2303   train_loss = 3.071\n",
      "Epoch  46 Batch  842/2303   train_loss = 3.728\n",
      "Epoch  46 Batch  862/2303   train_loss = 3.295\n",
      "Epoch  46 Batch  882/2303   train_loss = 3.848\n",
      "Epoch  46 Batch  902/2303   train_loss = 3.614\n",
      "Epoch  46 Batch  922/2303   train_loss = 3.800\n",
      "Epoch  46 Batch  942/2303   train_loss = 5.038\n",
      "Epoch  46 Batch  962/2303   train_loss = 5.139\n",
      "Epoch  46 Batch  982/2303   train_loss = 3.778\n",
      "Epoch  46 Batch 1002/2303   train_loss = 4.166\n",
      "Epoch  46 Batch 1022/2303   train_loss = 3.676\n",
      "Epoch  46 Batch 1042/2303   train_loss = 3.731\n",
      "Epoch  46 Batch 1062/2303   train_loss = 2.981\n",
      "Epoch  46 Batch 1082/2303   train_loss = 5.202\n",
      "Epoch  46 Batch 1102/2303   train_loss = 4.608\n",
      "Epoch  46 Batch 1122/2303   train_loss = 3.083\n",
      "Epoch  46 Batch 1142/2303   train_loss = 2.887\n",
      "Epoch  46 Batch 1162/2303   train_loss = 3.918\n",
      "Epoch  46 Batch 1182/2303   train_loss = 3.571\n",
      "Epoch  46 Batch 1202/2303   train_loss = 4.097\n",
      "Epoch  46 Batch 1222/2303   train_loss = 3.210\n",
      "Epoch  46 Batch 1242/2303   train_loss = 4.180\n",
      "Epoch  46 Batch 1262/2303   train_loss = 3.947\n",
      "Epoch  46 Batch 1282/2303   train_loss = 3.692\n",
      "Epoch  46 Batch 1302/2303   train_loss = 3.900\n",
      "Epoch  46 Batch 1322/2303   train_loss = 4.484\n",
      "Epoch  46 Batch 1342/2303   train_loss = 4.093\n",
      "Epoch  46 Batch 1362/2303   train_loss = 3.625\n",
      "Epoch  46 Batch 1382/2303   train_loss = 3.133\n",
      "Epoch  46 Batch 1402/2303   train_loss = 3.737\n",
      "Epoch  46 Batch 1422/2303   train_loss = 4.286\n",
      "Epoch  46 Batch 1442/2303   train_loss = 3.268\n",
      "Epoch  46 Batch 1462/2303   train_loss = 4.228\n",
      "Epoch  46 Batch 1482/2303   train_loss = 3.633\n",
      "Epoch  46 Batch 1502/2303   train_loss = 3.974\n",
      "Epoch  46 Batch 1522/2303   train_loss = 3.531\n",
      "Epoch  46 Batch 1542/2303   train_loss = 4.166\n",
      "Epoch  46 Batch 1562/2303   train_loss = 3.686\n",
      "Epoch  46 Batch 1582/2303   train_loss = 3.982\n",
      "Epoch  46 Batch 1602/2303   train_loss = 3.450\n",
      "Epoch  46 Batch 1622/2303   train_loss = 4.037\n",
      "Epoch  46 Batch 1642/2303   train_loss = 3.211\n",
      "Epoch  46 Batch 1662/2303   train_loss = 3.892\n",
      "Epoch  46 Batch 1682/2303   train_loss = 3.686\n",
      "Epoch  46 Batch 1702/2303   train_loss = 3.650\n",
      "Epoch  46 Batch 1722/2303   train_loss = 3.889\n",
      "Epoch  46 Batch 1742/2303   train_loss = 3.328\n",
      "Epoch  46 Batch 1762/2303   train_loss = 4.224\n",
      "Epoch  46 Batch 1782/2303   train_loss = 4.161\n",
      "Epoch  46 Batch 1802/2303   train_loss = 4.410\n",
      "Epoch  46 Batch 1822/2303   train_loss = 4.106\n",
      "Epoch  46 Batch 1842/2303   train_loss = 2.736\n",
      "Epoch  46 Batch 1862/2303   train_loss = 3.308\n",
      "Epoch  46 Batch 1882/2303   train_loss = 3.395\n",
      "Epoch  46 Batch 1902/2303   train_loss = 3.241\n",
      "Epoch  46 Batch 1922/2303   train_loss = 4.013\n",
      "Epoch  46 Batch 1942/2303   train_loss = 4.737\n",
      "Epoch  46 Batch 1962/2303   train_loss = 3.719\n",
      "Epoch  46 Batch 1982/2303   train_loss = 3.835\n",
      "Epoch  46 Batch 2002/2303   train_loss = 3.978\n",
      "Epoch  46 Batch 2022/2303   train_loss = 4.108\n",
      "Epoch  46 Batch 2042/2303   train_loss = 3.179\n",
      "Epoch  46 Batch 2062/2303   train_loss = 3.684\n",
      "Epoch  46 Batch 2082/2303   train_loss = 3.333\n",
      "Epoch  46 Batch 2102/2303   train_loss = 4.058\n",
      "Epoch  46 Batch 2122/2303   train_loss = 4.095\n",
      "Epoch  46 Batch 2142/2303   train_loss = 4.390\n",
      "Epoch  46 Batch 2162/2303   train_loss = 3.513\n",
      "Epoch  46 Batch 2182/2303   train_loss = 3.137\n",
      "Epoch  46 Batch 2202/2303   train_loss = 4.331\n",
      "Epoch  46 Batch 2222/2303   train_loss = 3.547\n",
      "Epoch  46 Batch 2242/2303   train_loss = 3.526\n",
      "Epoch  46 Batch 2262/2303   train_loss = 3.368\n",
      "Epoch  46 Batch 2282/2303   train_loss = 3.377\n",
      "Epoch  46 Batch 2302/2303   train_loss = 3.469\n",
      "Epoch  47 Batch   19/2303   train_loss = 3.064\n",
      "Epoch  47 Batch   39/2303   train_loss = 3.514\n",
      "Epoch  47 Batch   59/2303   train_loss = 3.875\n",
      "Epoch  47 Batch   79/2303   train_loss = 3.797\n",
      "Epoch  47 Batch   99/2303   train_loss = 3.412\n",
      "Epoch  47 Batch  119/2303   train_loss = 3.194\n",
      "Epoch  47 Batch  139/2303   train_loss = 3.102\n",
      "Epoch  47 Batch  159/2303   train_loss = 2.656\n",
      "Epoch  47 Batch  179/2303   train_loss = 3.381\n",
      "Epoch  47 Batch  199/2303   train_loss = 3.947\n",
      "Epoch  47 Batch  219/2303   train_loss = 3.875\n",
      "Epoch  47 Batch  239/2303   train_loss = 3.909\n",
      "Epoch  47 Batch  259/2303   train_loss = 4.126\n",
      "Epoch  47 Batch  279/2303   train_loss = 3.550\n",
      "Epoch  47 Batch  299/2303   train_loss = 4.676\n",
      "Epoch  47 Batch  319/2303   train_loss = 4.160\n",
      "Epoch  47 Batch  339/2303   train_loss = 3.641\n",
      "Epoch  47 Batch  359/2303   train_loss = 3.716\n",
      "Epoch  47 Batch  379/2303   train_loss = 3.295\n",
      "Epoch  47 Batch  399/2303   train_loss = 2.866\n",
      "Epoch  47 Batch  419/2303   train_loss = 3.994\n",
      "Epoch  47 Batch  439/2303   train_loss = 3.489\n",
      "Epoch  47 Batch  459/2303   train_loss = 4.070\n",
      "Epoch  47 Batch  479/2303   train_loss = 4.278\n",
      "Epoch  47 Batch  499/2303   train_loss = 3.345\n",
      "Epoch  47 Batch  519/2303   train_loss = 3.651\n",
      "Epoch  47 Batch  539/2303   train_loss = 3.921\n",
      "Epoch  47 Batch  559/2303   train_loss = 3.469\n",
      "Epoch  47 Batch  579/2303   train_loss = 4.545\n",
      "Epoch  47 Batch  599/2303   train_loss = 4.097\n",
      "Epoch  47 Batch  619/2303   train_loss = 3.730\n",
      "Epoch  47 Batch  639/2303   train_loss = 3.847\n",
      "Epoch  47 Batch  659/2303   train_loss = 4.071\n",
      "Epoch  47 Batch  679/2303   train_loss = 4.094\n",
      "Epoch  47 Batch  699/2303   train_loss = 3.346\n",
      "Epoch  47 Batch  719/2303   train_loss = 3.392\n",
      "Epoch  47 Batch  739/2303   train_loss = 3.036\n",
      "Epoch  47 Batch  759/2303   train_loss = 4.011\n",
      "Epoch  47 Batch  779/2303   train_loss = 3.451\n",
      "Epoch  47 Batch  799/2303   train_loss = 3.262\n",
      "Epoch  47 Batch  819/2303   train_loss = 3.103\n",
      "Epoch  47 Batch  839/2303   train_loss = 4.353\n",
      "Epoch  47 Batch  859/2303   train_loss = 3.538\n",
      "Epoch  47 Batch  879/2303   train_loss = 2.837\n",
      "Epoch  47 Batch  899/2303   train_loss = 3.983\n",
      "Epoch  47 Batch  919/2303   train_loss = 3.038\n",
      "Epoch  47 Batch  939/2303   train_loss = 4.125\n",
      "Epoch  47 Batch  959/2303   train_loss = 3.757\n",
      "Epoch  47 Batch  979/2303   train_loss = 3.600\n",
      "Epoch  47 Batch  999/2303   train_loss = 3.480\n",
      "Epoch  47 Batch 1019/2303   train_loss = 3.329\n",
      "Epoch  47 Batch 1039/2303   train_loss = 2.972\n",
      "Epoch  47 Batch 1059/2303   train_loss = 2.482\n",
      "Epoch  47 Batch 1079/2303   train_loss = 3.358\n",
      "Epoch  47 Batch 1099/2303   train_loss = 3.865\n",
      "Epoch  47 Batch 1119/2303   train_loss = 4.040\n",
      "Epoch  47 Batch 1139/2303   train_loss = 4.195\n",
      "Epoch  47 Batch 1159/2303   train_loss = 4.937\n",
      "Epoch  47 Batch 1179/2303   train_loss = 3.969\n",
      "Epoch  47 Batch 1199/2303   train_loss = 3.690\n",
      "Epoch  47 Batch 1219/2303   train_loss = 3.526\n",
      "Epoch  47 Batch 1239/2303   train_loss = 4.413\n",
      "Epoch  47 Batch 1259/2303   train_loss = 5.235\n",
      "Epoch  47 Batch 1279/2303   train_loss = 4.378\n",
      "Epoch  47 Batch 1299/2303   train_loss = 3.902\n",
      "Epoch  47 Batch 1319/2303   train_loss = 4.202\n",
      "Epoch  47 Batch 1339/2303   train_loss = 4.321\n",
      "Epoch  47 Batch 1359/2303   train_loss = 4.363\n",
      "Epoch  47 Batch 1379/2303   train_loss = 3.569\n",
      "Epoch  47 Batch 1399/2303   train_loss = 3.577\n",
      "Epoch  47 Batch 1419/2303   train_loss = 3.859\n",
      "Epoch  47 Batch 1439/2303   train_loss = 4.273\n",
      "Epoch  47 Batch 1459/2303   train_loss = 3.451\n",
      "Epoch  47 Batch 1479/2303   train_loss = 4.157\n",
      "Epoch  47 Batch 1499/2303   train_loss = 3.429\n",
      "Epoch  47 Batch 1519/2303   train_loss = 3.151\n",
      "Epoch  47 Batch 1539/2303   train_loss = 2.909\n",
      "Epoch  47 Batch 1559/2303   train_loss = 3.943\n",
      "Epoch  47 Batch 1579/2303   train_loss = 3.773\n",
      "Epoch  47 Batch 1599/2303   train_loss = 4.801\n",
      "Epoch  47 Batch 1619/2303   train_loss = 4.172\n",
      "Epoch  47 Batch 1639/2303   train_loss = 3.816\n",
      "Epoch  47 Batch 1659/2303   train_loss = 3.951\n",
      "Epoch  47 Batch 1679/2303   train_loss = 2.973\n",
      "Epoch  47 Batch 1699/2303   train_loss = 3.758\n",
      "Epoch  47 Batch 1719/2303   train_loss = 4.230\n",
      "Epoch  47 Batch 1739/2303   train_loss = 4.633\n",
      "Epoch  47 Batch 1759/2303   train_loss = 4.696\n",
      "Epoch  47 Batch 1779/2303   train_loss = 3.360\n",
      "Epoch  47 Batch 1799/2303   train_loss = 3.499\n",
      "Epoch  47 Batch 1819/2303   train_loss = 4.328\n",
      "Epoch  47 Batch 1839/2303   train_loss = 3.397\n",
      "Epoch  47 Batch 1859/2303   train_loss = 4.213\n",
      "Epoch  47 Batch 1879/2303   train_loss = 3.860\n",
      "Epoch  47 Batch 1899/2303   train_loss = 4.021\n",
      "Epoch  47 Batch 1919/2303   train_loss = 2.793\n",
      "Epoch  47 Batch 1939/2303   train_loss = 3.184\n",
      "Epoch  47 Batch 1959/2303   train_loss = 3.633\n",
      "Epoch  47 Batch 1979/2303   train_loss = 3.839\n",
      "Epoch  47 Batch 1999/2303   train_loss = 3.257\n",
      "Epoch  47 Batch 2019/2303   train_loss = 3.085\n",
      "Epoch  47 Batch 2039/2303   train_loss = 3.440\n",
      "Epoch  47 Batch 2059/2303   train_loss = 4.125\n",
      "Epoch  47 Batch 2079/2303   train_loss = 4.225\n",
      "Epoch  47 Batch 2099/2303   train_loss = 2.633\n",
      "Epoch  47 Batch 2119/2303   train_loss = 3.619\n",
      "Epoch  47 Batch 2139/2303   train_loss = 3.539\n",
      "Epoch  47 Batch 2159/2303   train_loss = 3.189\n",
      "Epoch  47 Batch 2179/2303   train_loss = 3.555\n",
      "Epoch  47 Batch 2199/2303   train_loss = 3.327\n",
      "Epoch  47 Batch 2219/2303   train_loss = 3.242\n",
      "Epoch  47 Batch 2239/2303   train_loss = 3.954\n",
      "Epoch  47 Batch 2259/2303   train_loss = 3.810\n",
      "Epoch  47 Batch 2279/2303   train_loss = 4.364\n",
      "Epoch  47 Batch 2299/2303   train_loss = 4.999\n",
      "Epoch  48 Batch   16/2303   train_loss = 4.021\n",
      "Epoch  48 Batch   36/2303   train_loss = 3.457\n",
      "Epoch  48 Batch   56/2303   train_loss = 3.914\n",
      "Epoch  48 Batch   76/2303   train_loss = 3.682\n",
      "Epoch  48 Batch   96/2303   train_loss = 4.142\n",
      "Epoch  48 Batch  116/2303   train_loss = 3.663\n",
      "Epoch  48 Batch  136/2303   train_loss = 4.109\n",
      "Epoch  48 Batch  156/2303   train_loss = 3.063\n",
      "Epoch  48 Batch  176/2303   train_loss = 3.907\n",
      "Epoch  48 Batch  196/2303   train_loss = 4.337\n",
      "Epoch  48 Batch  216/2303   train_loss = 3.695\n",
      "Epoch  48 Batch  236/2303   train_loss = 3.657\n",
      "Epoch  48 Batch  256/2303   train_loss = 4.362\n",
      "Epoch  48 Batch  276/2303   train_loss = 4.372\n",
      "Epoch  48 Batch  296/2303   train_loss = 3.515\n",
      "Epoch  48 Batch  316/2303   train_loss = 3.121\n",
      "Epoch  48 Batch  336/2303   train_loss = 3.750\n",
      "Epoch  48 Batch  356/2303   train_loss = 3.975\n",
      "Epoch  48 Batch  376/2303   train_loss = 3.608\n",
      "Epoch  48 Batch  396/2303   train_loss = 3.375\n",
      "Epoch  48 Batch  416/2303   train_loss = 3.744\n",
      "Epoch  48 Batch  436/2303   train_loss = 3.156\n",
      "Epoch  48 Batch  456/2303   train_loss = 4.456\n",
      "Epoch  48 Batch  476/2303   train_loss = 4.904\n",
      "Epoch  48 Batch  496/2303   train_loss = 4.064\n",
      "Epoch  48 Batch  516/2303   train_loss = 4.004\n",
      "Epoch  48 Batch  536/2303   train_loss = 4.791\n",
      "Epoch  48 Batch  556/2303   train_loss = 3.801\n",
      "Epoch  48 Batch  576/2303   train_loss = 3.305\n",
      "Epoch  48 Batch  596/2303   train_loss = 3.852\n",
      "Epoch  48 Batch  616/2303   train_loss = 2.669\n",
      "Epoch  48 Batch  636/2303   train_loss = 4.576\n",
      "Epoch  48 Batch  656/2303   train_loss = 3.599\n",
      "Epoch  48 Batch  676/2303   train_loss = 3.574\n",
      "Epoch  48 Batch  696/2303   train_loss = 4.002\n",
      "Epoch  48 Batch  716/2303   train_loss = 3.743\n",
      "Epoch  48 Batch  736/2303   train_loss = 3.114\n",
      "Epoch  48 Batch  756/2303   train_loss = 3.309\n",
      "Epoch  48 Batch  776/2303   train_loss = 2.930\n",
      "Epoch  48 Batch  796/2303   train_loss = 3.999\n",
      "Epoch  48 Batch  816/2303   train_loss = 3.696\n",
      "Epoch  48 Batch  836/2303   train_loss = 3.973\n",
      "Epoch  48 Batch  856/2303   train_loss = 4.173\n",
      "Epoch  48 Batch  876/2303   train_loss = 3.451\n",
      "Epoch  48 Batch  896/2303   train_loss = 3.796\n",
      "Epoch  48 Batch  916/2303   train_loss = 3.236\n",
      "Epoch  48 Batch  936/2303   train_loss = 4.528\n",
      "Epoch  48 Batch  956/2303   train_loss = 3.949\n",
      "Epoch  48 Batch  976/2303   train_loss = 3.365\n",
      "Epoch  48 Batch  996/2303   train_loss = 3.628\n",
      "Epoch  48 Batch 1016/2303   train_loss = 2.954\n",
      "Epoch  48 Batch 1036/2303   train_loss = 3.564\n",
      "Epoch  48 Batch 1056/2303   train_loss = 3.446\n",
      "Epoch  48 Batch 1076/2303   train_loss = 3.855\n",
      "Epoch  48 Batch 1096/2303   train_loss = 3.848\n",
      "Epoch  48 Batch 1116/2303   train_loss = 3.204\n",
      "Epoch  48 Batch 1136/2303   train_loss = 3.641\n",
      "Epoch  48 Batch 1156/2303   train_loss = 3.751\n",
      "Epoch  48 Batch 1176/2303   train_loss = 4.085\n",
      "Epoch  48 Batch 1196/2303   train_loss = 3.708\n",
      "Epoch  48 Batch 1216/2303   train_loss = 3.610\n",
      "Epoch  48 Batch 1236/2303   train_loss = 3.544\n",
      "Epoch  48 Batch 1256/2303   train_loss = 3.721\n",
      "Epoch  48 Batch 1276/2303   train_loss = 3.664\n",
      "Epoch  48 Batch 1296/2303   train_loss = 3.519\n",
      "Epoch  48 Batch 1316/2303   train_loss = 4.191\n",
      "Epoch  48 Batch 1336/2303   train_loss = 3.410\n",
      "Epoch  48 Batch 1356/2303   train_loss = 3.502\n",
      "Epoch  48 Batch 1376/2303   train_loss = 4.666\n",
      "Epoch  48 Batch 1396/2303   train_loss = 3.188\n",
      "Epoch  48 Batch 1416/2303   train_loss = 2.878\n",
      "Epoch  48 Batch 1436/2303   train_loss = 3.663\n",
      "Epoch  48 Batch 1456/2303   train_loss = 4.278\n",
      "Epoch  48 Batch 1476/2303   train_loss = 3.476\n",
      "Epoch  48 Batch 1496/2303   train_loss = 3.140\n",
      "Epoch  48 Batch 1516/2303   train_loss = 4.442\n",
      "Epoch  48 Batch 1536/2303   train_loss = 3.532\n",
      "Epoch  48 Batch 1556/2303   train_loss = 4.271\n",
      "Epoch  48 Batch 1576/2303   train_loss = 3.719\n",
      "Epoch  48 Batch 1596/2303   train_loss = 4.304\n",
      "Epoch  48 Batch 1616/2303   train_loss = 3.836\n",
      "Epoch  48 Batch 1636/2303   train_loss = 4.957\n",
      "Epoch  48 Batch 1656/2303   train_loss = 3.643\n",
      "Epoch  48 Batch 1676/2303   train_loss = 4.405\n",
      "Epoch  48 Batch 1696/2303   train_loss = 4.789\n",
      "Epoch  48 Batch 1716/2303   train_loss = 3.650\n",
      "Epoch  48 Batch 1736/2303   train_loss = 3.649\n",
      "Epoch  48 Batch 1756/2303   train_loss = 3.411\n",
      "Epoch  48 Batch 1776/2303   train_loss = 4.154\n",
      "Epoch  48 Batch 1796/2303   train_loss = 3.580\n",
      "Epoch  48 Batch 1816/2303   train_loss = 4.625\n",
      "Epoch  48 Batch 1836/2303   train_loss = 3.268\n",
      "Epoch  48 Batch 1856/2303   train_loss = 3.934\n",
      "Epoch  48 Batch 1876/2303   train_loss = 4.061\n",
      "Epoch  48 Batch 1896/2303   train_loss = 4.112\n",
      "Epoch  48 Batch 1916/2303   train_loss = 3.725\n",
      "Epoch  48 Batch 1936/2303   train_loss = 3.386\n",
      "Epoch  48 Batch 1956/2303   train_loss = 4.192\n",
      "Epoch  48 Batch 1976/2303   train_loss = 3.634\n",
      "Epoch  48 Batch 1996/2303   train_loss = 3.680\n",
      "Epoch  48 Batch 2016/2303   train_loss = 2.979\n",
      "Epoch  48 Batch 2036/2303   train_loss = 3.696\n",
      "Epoch  48 Batch 2056/2303   train_loss = 3.332\n",
      "Epoch  48 Batch 2076/2303   train_loss = 3.556\n",
      "Epoch  48 Batch 2096/2303   train_loss = 2.533\n",
      "Epoch  48 Batch 2116/2303   train_loss = 3.466\n",
      "Epoch  48 Batch 2136/2303   train_loss = 2.952\n",
      "Epoch  48 Batch 2156/2303   train_loss = 4.093\n",
      "Epoch  48 Batch 2176/2303   train_loss = 3.755\n",
      "Epoch  48 Batch 2196/2303   train_loss = 4.738\n",
      "Epoch  48 Batch 2216/2303   train_loss = 3.642\n",
      "Epoch  48 Batch 2236/2303   train_loss = 3.984\n",
      "Epoch  48 Batch 2256/2303   train_loss = 3.353\n",
      "Epoch  48 Batch 2276/2303   train_loss = 4.234\n",
      "Epoch  48 Batch 2296/2303   train_loss = 3.625\n",
      "Epoch  49 Batch   13/2303   train_loss = 2.809\n",
      "Epoch  49 Batch   33/2303   train_loss = 5.058\n",
      "Epoch  49 Batch   53/2303   train_loss = 3.850\n",
      "Epoch  49 Batch   73/2303   train_loss = 4.805\n",
      "Epoch  49 Batch   93/2303   train_loss = 3.952\n",
      "Epoch  49 Batch  113/2303   train_loss = 3.375\n",
      "Epoch  49 Batch  133/2303   train_loss = 4.381\n",
      "Epoch  49 Batch  153/2303   train_loss = 3.737\n",
      "Epoch  49 Batch  173/2303   train_loss = 3.157\n",
      "Epoch  49 Batch  193/2303   train_loss = 2.905\n",
      "Epoch  49 Batch  213/2303   train_loss = 4.076\n",
      "Epoch  49 Batch  233/2303   train_loss = 3.774\n",
      "Epoch  49 Batch  253/2303   train_loss = 3.535\n",
      "Epoch  49 Batch  273/2303   train_loss = 3.691\n",
      "Epoch  49 Batch  293/2303   train_loss = 4.169\n",
      "Epoch  49 Batch  313/2303   train_loss = 3.828\n",
      "Epoch  49 Batch  333/2303   train_loss = 3.639\n",
      "Epoch  49 Batch  353/2303   train_loss = 3.149\n",
      "Epoch  49 Batch  373/2303   train_loss = 3.803\n",
      "Epoch  49 Batch  393/2303   train_loss = 4.050\n",
      "Epoch  49 Batch  413/2303   train_loss = 3.891\n",
      "Epoch  49 Batch  433/2303   train_loss = 4.315\n",
      "Epoch  49 Batch  453/2303   train_loss = 3.996\n",
      "Epoch  49 Batch  473/2303   train_loss = 3.162\n",
      "Epoch  49 Batch  493/2303   train_loss = 4.089\n",
      "Epoch  49 Batch  513/2303   train_loss = 3.705\n",
      "Epoch  49 Batch  533/2303   train_loss = 3.206\n",
      "Epoch  49 Batch  553/2303   train_loss = 3.439\n",
      "Epoch  49 Batch  573/2303   train_loss = 3.739\n",
      "Epoch  49 Batch  593/2303   train_loss = 3.220\n",
      "Epoch  49 Batch  613/2303   train_loss = 3.289\n",
      "Epoch  49 Batch  633/2303   train_loss = 4.193\n",
      "Epoch  49 Batch  653/2303   train_loss = 3.559\n",
      "Epoch  49 Batch  673/2303   train_loss = 4.146\n",
      "Epoch  49 Batch  693/2303   train_loss = 4.491\n",
      "Epoch  49 Batch  713/2303   train_loss = 3.948\n",
      "Epoch  49 Batch  733/2303   train_loss = 2.948\n",
      "Epoch  49 Batch  753/2303   train_loss = 3.951\n",
      "Epoch  49 Batch  773/2303   train_loss = 4.192\n",
      "Epoch  49 Batch  793/2303   train_loss = 3.809\n",
      "Epoch  49 Batch  813/2303   train_loss = 3.731\n",
      "Epoch  49 Batch  833/2303   train_loss = 3.642\n",
      "Epoch  49 Batch  853/2303   train_loss = 5.328\n",
      "Epoch  49 Batch  873/2303   train_loss = 3.860\n",
      "Epoch  49 Batch  893/2303   train_loss = 4.141\n",
      "Epoch  49 Batch  913/2303   train_loss = 3.782\n",
      "Epoch  49 Batch  933/2303   train_loss = 4.073\n",
      "Epoch  49 Batch  953/2303   train_loss = 3.337\n",
      "Epoch  49 Batch  973/2303   train_loss = 3.678\n",
      "Epoch  49 Batch  993/2303   train_loss = 2.899\n",
      "Epoch  49 Batch 1013/2303   train_loss = 3.592\n",
      "Epoch  49 Batch 1033/2303   train_loss = 3.842\n",
      "Epoch  49 Batch 1053/2303   train_loss = 4.088\n",
      "Epoch  49 Batch 1073/2303   train_loss = 4.053\n",
      "Epoch  49 Batch 1093/2303   train_loss = 3.952\n",
      "Epoch  49 Batch 1113/2303   train_loss = 3.829\n",
      "Epoch  49 Batch 1133/2303   train_loss = 3.743\n",
      "Epoch  49 Batch 1153/2303   train_loss = 3.451\n",
      "Epoch  49 Batch 1173/2303   train_loss = 3.889\n",
      "Epoch  49 Batch 1193/2303   train_loss = 4.231\n",
      "Epoch  49 Batch 1213/2303   train_loss = 3.470\n",
      "Epoch  49 Batch 1233/2303   train_loss = 3.428\n",
      "Epoch  49 Batch 1253/2303   train_loss = 3.928\n",
      "Epoch  49 Batch 1273/2303   train_loss = 3.789\n",
      "Epoch  49 Batch 1293/2303   train_loss = 3.944\n",
      "Epoch  49 Batch 1313/2303   train_loss = 3.443\n",
      "Epoch  49 Batch 1333/2303   train_loss = 4.079\n",
      "Epoch  49 Batch 1353/2303   train_loss = 3.312\n",
      "Epoch  49 Batch 1373/2303   train_loss = 3.403\n",
      "Epoch  49 Batch 1393/2303   train_loss = 3.377\n",
      "Epoch  49 Batch 1413/2303   train_loss = 4.388\n",
      "Epoch  49 Batch 1433/2303   train_loss = 4.416\n",
      "Epoch  49 Batch 1453/2303   train_loss = 3.689\n",
      "Epoch  49 Batch 1473/2303   train_loss = 4.750\n",
      "Epoch  49 Batch 1493/2303   train_loss = 4.047\n",
      "Epoch  49 Batch 1513/2303   train_loss = 3.307\n",
      "Epoch  49 Batch 1533/2303   train_loss = 4.416\n",
      "Epoch  49 Batch 1553/2303   train_loss = 3.463\n",
      "Epoch  49 Batch 1573/2303   train_loss = 4.208\n",
      "Epoch  49 Batch 1593/2303   train_loss = 3.747\n",
      "Epoch  49 Batch 1613/2303   train_loss = 3.384\n",
      "Epoch  49 Batch 1633/2303   train_loss = 3.451\n",
      "Epoch  49 Batch 1653/2303   train_loss = 2.642\n",
      "Epoch  49 Batch 1673/2303   train_loss = 3.639\n",
      "Epoch  49 Batch 1693/2303   train_loss = 3.166\n",
      "Epoch  49 Batch 1713/2303   train_loss = 3.569\n",
      "Epoch  49 Batch 1733/2303   train_loss = 4.085\n",
      "Epoch  49 Batch 1753/2303   train_loss = 3.597\n",
      "Epoch  49 Batch 1773/2303   train_loss = 3.171\n",
      "Epoch  49 Batch 1793/2303   train_loss = 3.412\n",
      "Epoch  49 Batch 1813/2303   train_loss = 3.460\n",
      "Epoch  49 Batch 1833/2303   train_loss = 3.928\n",
      "Epoch  49 Batch 1853/2303   train_loss = 3.348\n",
      "Epoch  49 Batch 1873/2303   train_loss = 3.654\n",
      "Epoch  49 Batch 1893/2303   train_loss = 3.900\n",
      "Epoch  49 Batch 1913/2303   train_loss = 4.469\n",
      "Epoch  49 Batch 1933/2303   train_loss = 3.897\n",
      "Epoch  49 Batch 1953/2303   train_loss = 5.175\n",
      "Epoch  49 Batch 1973/2303   train_loss = 3.879\n",
      "Epoch  49 Batch 1993/2303   train_loss = 3.922\n",
      "Epoch  49 Batch 2013/2303   train_loss = 4.200\n",
      "Epoch  49 Batch 2033/2303   train_loss = 4.763\n",
      "Epoch  49 Batch 2053/2303   train_loss = 4.444\n",
      "Epoch  49 Batch 2073/2303   train_loss = 3.873\n",
      "Epoch  49 Batch 2093/2303   train_loss = 3.778\n",
      "Epoch  49 Batch 2113/2303   train_loss = 4.108\n",
      "Epoch  49 Batch 2133/2303   train_loss = 3.440\n",
      "Epoch  49 Batch 2153/2303   train_loss = 3.859\n",
      "Epoch  49 Batch 2173/2303   train_loss = 3.823\n",
      "Epoch  49 Batch 2193/2303   train_loss = 3.571\n",
      "Epoch  49 Batch 2213/2303   train_loss = 4.124\n",
      "Epoch  49 Batch 2233/2303   train_loss = 3.426\n",
      "Epoch  49 Batch 2253/2303   train_loss = 3.654\n",
      "Epoch  49 Batch 2273/2303   train_loss = 4.320\n",
      "Epoch  49 Batch 2293/2303   train_loss = 3.115\n",
      "Epoch  50 Batch   10/2303   train_loss = 3.683\n",
      "Epoch  50 Batch   30/2303   train_loss = 3.840\n",
      "Epoch  50 Batch   50/2303   train_loss = 4.206\n",
      "Epoch  50 Batch   70/2303   train_loss = 4.176\n",
      "Epoch  50 Batch   90/2303   train_loss = 4.399\n",
      "Epoch  50 Batch  110/2303   train_loss = 4.133\n",
      "Epoch  50 Batch  130/2303   train_loss = 3.901\n",
      "Epoch  50 Batch  150/2303   train_loss = 3.278\n",
      "Epoch  50 Batch  170/2303   train_loss = 4.216\n",
      "Epoch  50 Batch  190/2303   train_loss = 4.561\n",
      "Epoch  50 Batch  210/2303   train_loss = 3.279\n",
      "Epoch  50 Batch  230/2303   train_loss = 3.435\n",
      "Epoch  50 Batch  250/2303   train_loss = 2.946\n",
      "Epoch  50 Batch  270/2303   train_loss = 4.089\n",
      "Epoch  50 Batch  290/2303   train_loss = 3.659\n",
      "Epoch  50 Batch  310/2303   train_loss = 4.743\n",
      "Epoch  50 Batch  330/2303   train_loss = 4.461\n",
      "Epoch  50 Batch  350/2303   train_loss = 4.520\n",
      "Epoch  50 Batch  370/2303   train_loss = 3.915\n",
      "Epoch  50 Batch  390/2303   train_loss = 3.730\n",
      "Epoch  50 Batch  410/2303   train_loss = 4.138\n",
      "Epoch  50 Batch  430/2303   train_loss = 3.225\n",
      "Epoch  50 Batch  450/2303   train_loss = 3.828\n",
      "Epoch  50 Batch  470/2303   train_loss = 3.798\n",
      "Epoch  50 Batch  490/2303   train_loss = 4.109\n",
      "Epoch  50 Batch  510/2303   train_loss = 3.383\n",
      "Epoch  50 Batch  530/2303   train_loss = 3.826\n",
      "Epoch  50 Batch  550/2303   train_loss = 3.754\n",
      "Epoch  50 Batch  570/2303   train_loss = 3.565\n",
      "Epoch  50 Batch  590/2303   train_loss = 3.471\n",
      "Epoch  50 Batch  610/2303   train_loss = 3.464\n",
      "Epoch  50 Batch  630/2303   train_loss = 4.002\n",
      "Epoch  50 Batch  650/2303   train_loss = 2.805\n",
      "Epoch  50 Batch  670/2303   train_loss = 3.859\n",
      "Epoch  50 Batch  690/2303   train_loss = 4.225\n",
      "Epoch  50 Batch  710/2303   train_loss = 3.833\n",
      "Epoch  50 Batch  730/2303   train_loss = 3.692\n",
      "Epoch  50 Batch  750/2303   train_loss = 4.052\n",
      "Epoch  50 Batch  770/2303   train_loss = 3.635\n",
      "Epoch  50 Batch  790/2303   train_loss = 3.179\n",
      "Epoch  50 Batch  810/2303   train_loss = 4.099\n",
      "Epoch  50 Batch  830/2303   train_loss = 3.440\n",
      "Epoch  50 Batch  850/2303   train_loss = 4.178\n",
      "Epoch  50 Batch  870/2303   train_loss = 2.775\n",
      "Epoch  50 Batch  890/2303   train_loss = 3.877\n",
      "Epoch  50 Batch  910/2303   train_loss = 3.797\n",
      "Epoch  50 Batch  930/2303   train_loss = 4.400\n",
      "Epoch  50 Batch  950/2303   train_loss = 3.431\n",
      "Epoch  50 Batch  970/2303   train_loss = 3.338\n",
      "Epoch  50 Batch  990/2303   train_loss = 3.492\n",
      "Epoch  50 Batch 1010/2303   train_loss = 3.142\n",
      "Epoch  50 Batch 1030/2303   train_loss = 2.937\n",
      "Epoch  50 Batch 1050/2303   train_loss = 3.917\n",
      "Epoch  50 Batch 1070/2303   train_loss = 3.930\n",
      "Epoch  50 Batch 1090/2303   train_loss = 4.169\n",
      "Epoch  50 Batch 1110/2303   train_loss = 3.150\n",
      "Epoch  50 Batch 1130/2303   train_loss = 3.893\n",
      "Epoch  50 Batch 1150/2303   train_loss = 3.882\n",
      "Epoch  50 Batch 1170/2303   train_loss = 4.482\n",
      "Epoch  50 Batch 1190/2303   train_loss = 3.200\n",
      "Epoch  50 Batch 1210/2303   train_loss = 3.381\n",
      "Epoch  50 Batch 1230/2303   train_loss = 3.182\n",
      "Epoch  50 Batch 1250/2303   train_loss = 3.553\n",
      "Epoch  50 Batch 1270/2303   train_loss = 3.839\n",
      "Epoch  50 Batch 1290/2303   train_loss = 3.368\n",
      "Epoch  50 Batch 1310/2303   train_loss = 4.159\n",
      "Epoch  50 Batch 1330/2303   train_loss = 4.469\n",
      "Epoch  50 Batch 1350/2303   train_loss = 3.258\n",
      "Epoch  50 Batch 1370/2303   train_loss = 4.194\n",
      "Epoch  50 Batch 1390/2303   train_loss = 3.483\n",
      "Epoch  50 Batch 1410/2303   train_loss = 4.389\n",
      "Epoch  50 Batch 1430/2303   train_loss = 4.275\n",
      "Epoch  50 Batch 1450/2303   train_loss = 3.968\n",
      "Epoch  50 Batch 1470/2303   train_loss = 3.234\n",
      "Epoch  50 Batch 1490/2303   train_loss = 4.321\n",
      "Epoch  50 Batch 1510/2303   train_loss = 3.860\n",
      "Epoch  50 Batch 1530/2303   train_loss = 4.236\n",
      "Epoch  50 Batch 1550/2303   train_loss = 4.296\n",
      "Epoch  50 Batch 1570/2303   train_loss = 4.012\n",
      "Epoch  50 Batch 1590/2303   train_loss = 3.869\n",
      "Epoch  50 Batch 1610/2303   train_loss = 2.530\n",
      "Epoch  50 Batch 1630/2303   train_loss = 3.577\n",
      "Epoch  50 Batch 1650/2303   train_loss = 4.315\n",
      "Epoch  50 Batch 1670/2303   train_loss = 3.750\n",
      "Epoch  50 Batch 1690/2303   train_loss = 4.370\n",
      "Epoch  50 Batch 1710/2303   train_loss = 3.333\n",
      "Epoch  50 Batch 1730/2303   train_loss = 3.976\n",
      "Epoch  50 Batch 1750/2303   train_loss = 3.204\n",
      "Epoch  50 Batch 1770/2303   train_loss = 4.238\n",
      "Epoch  50 Batch 1790/2303   train_loss = 4.279\n",
      "Epoch  50 Batch 1810/2303   train_loss = 3.852\n",
      "Epoch  50 Batch 1830/2303   train_loss = 3.797\n",
      "Epoch  50 Batch 1850/2303   train_loss = 4.095\n",
      "Epoch  50 Batch 1870/2303   train_loss = 3.180\n",
      "Epoch  50 Batch 1890/2303   train_loss = 3.865\n",
      "Epoch  50 Batch 1910/2303   train_loss = 4.114\n",
      "Epoch  50 Batch 1930/2303   train_loss = 2.963\n",
      "Epoch  50 Batch 1950/2303   train_loss = 4.121\n",
      "Epoch  50 Batch 1970/2303   train_loss = 4.014\n",
      "Epoch  50 Batch 1990/2303   train_loss = 3.668\n",
      "Epoch  50 Batch 2010/2303   train_loss = 3.569\n",
      "Epoch  50 Batch 2030/2303   train_loss = 3.723\n",
      "Epoch  50 Batch 2050/2303   train_loss = 4.072\n",
      "Epoch  50 Batch 2070/2303   train_loss = 3.876\n",
      "Epoch  50 Batch 2090/2303   train_loss = 3.268\n",
      "Epoch  50 Batch 2110/2303   train_loss = 4.361\n",
      "Epoch  50 Batch 2130/2303   train_loss = 3.834\n",
      "Epoch  50 Batch 2150/2303   train_loss = 3.649\n",
      "Epoch  50 Batch 2170/2303   train_loss = 3.782\n",
      "Epoch  50 Batch 2190/2303   train_loss = 4.066\n",
      "Epoch  50 Batch 2210/2303   train_loss = 3.249\n",
      "Epoch  50 Batch 2230/2303   train_loss = 4.154\n",
      "Epoch  50 Batch 2250/2303   train_loss = 2.903\n",
      "Epoch  50 Batch 2270/2303   train_loss = 3.535\n",
      "Epoch  50 Batch 2290/2303   train_loss = 2.992\n",
      "Epoch  51 Batch    7/2303   train_loss = 3.627\n",
      "Epoch  51 Batch   27/2303   train_loss = 3.921\n",
      "Epoch  51 Batch   47/2303   train_loss = 4.304\n",
      "Epoch  51 Batch   67/2303   train_loss = 3.490\n",
      "Epoch  51 Batch   87/2303   train_loss = 4.178\n",
      "Epoch  51 Batch  107/2303   train_loss = 4.839\n",
      "Epoch  51 Batch  127/2303   train_loss = 3.852\n",
      "Epoch  51 Batch  147/2303   train_loss = 3.157\n",
      "Epoch  51 Batch  167/2303   train_loss = 3.536\n",
      "Epoch  51 Batch  187/2303   train_loss = 2.986\n",
      "Epoch  51 Batch  207/2303   train_loss = 4.032\n",
      "Epoch  51 Batch  227/2303   train_loss = 3.482\n",
      "Epoch  51 Batch  247/2303   train_loss = 4.198\n",
      "Epoch  51 Batch  267/2303   train_loss = 3.438\n",
      "Epoch  51 Batch  287/2303   train_loss = 3.468\n",
      "Epoch  51 Batch  307/2303   train_loss = 3.165\n",
      "Epoch  51 Batch  327/2303   train_loss = 3.518\n",
      "Epoch  51 Batch  347/2303   train_loss = 4.609\n",
      "Epoch  51 Batch  367/2303   train_loss = 3.322\n",
      "Epoch  51 Batch  387/2303   train_loss = 3.410\n",
      "Epoch  51 Batch  407/2303   train_loss = 3.923\n",
      "Epoch  51 Batch  427/2303   train_loss = 2.758\n",
      "Epoch  51 Batch  447/2303   train_loss = 3.430\n",
      "Epoch  51 Batch  467/2303   train_loss = 3.105\n",
      "Epoch  51 Batch  487/2303   train_loss = 3.827\n",
      "Epoch  51 Batch  507/2303   train_loss = 3.747\n",
      "Epoch  51 Batch  527/2303   train_loss = 3.251\n",
      "Epoch  51 Batch  547/2303   train_loss = 3.641\n",
      "Epoch  51 Batch  567/2303   train_loss = 2.998\n",
      "Epoch  51 Batch  587/2303   train_loss = 3.674\n",
      "Epoch  51 Batch  607/2303   train_loss = 2.835\n",
      "Epoch  51 Batch  627/2303   train_loss = 3.238\n",
      "Epoch  51 Batch  647/2303   train_loss = 3.664\n",
      "Epoch  51 Batch  667/2303   train_loss = 3.979\n",
      "Epoch  51 Batch  687/2303   train_loss = 3.417\n",
      "Epoch  51 Batch  707/2303   train_loss = 3.589\n",
      "Epoch  51 Batch  727/2303   train_loss = 3.571\n",
      "Epoch  51 Batch  747/2303   train_loss = 3.132\n",
      "Epoch  51 Batch  767/2303   train_loss = 4.074\n",
      "Epoch  51 Batch  787/2303   train_loss = 3.957\n",
      "Epoch  51 Batch  807/2303   train_loss = 3.475\n",
      "Epoch  51 Batch  827/2303   train_loss = 3.975\n",
      "Epoch  51 Batch  847/2303   train_loss = 3.794\n",
      "Epoch  51 Batch  867/2303   train_loss = 3.729\n",
      "Epoch  51 Batch  887/2303   train_loss = 3.431\n",
      "Epoch  51 Batch  907/2303   train_loss = 3.692\n",
      "Epoch  51 Batch  927/2303   train_loss = 3.644\n",
      "Epoch  51 Batch  947/2303   train_loss = 4.061\n",
      "Epoch  51 Batch  967/2303   train_loss = 3.298\n",
      "Epoch  51 Batch  987/2303   train_loss = 4.268\n",
      "Epoch  51 Batch 1007/2303   train_loss = 3.321\n",
      "Epoch  51 Batch 1027/2303   train_loss = 3.102\n",
      "Epoch  51 Batch 1047/2303   train_loss = 3.703\n",
      "Epoch  51 Batch 1067/2303   train_loss = 3.040\n",
      "Epoch  51 Batch 1087/2303   train_loss = 3.125\n",
      "Epoch  51 Batch 1107/2303   train_loss = 4.295\n",
      "Epoch  51 Batch 1127/2303   train_loss = 3.304\n",
      "Epoch  51 Batch 1147/2303   train_loss = 4.200\n",
      "Epoch  51 Batch 1167/2303   train_loss = 3.806\n",
      "Epoch  51 Batch 1187/2303   train_loss = 3.457\n",
      "Epoch  51 Batch 1207/2303   train_loss = 4.132\n",
      "Epoch  51 Batch 1227/2303   train_loss = 4.421\n",
      "Epoch  51 Batch 1247/2303   train_loss = 4.018\n",
      "Epoch  51 Batch 1267/2303   train_loss = 2.996\n",
      "Epoch  51 Batch 1287/2303   train_loss = 3.842\n",
      "Epoch  51 Batch 1307/2303   train_loss = 3.935\n",
      "Epoch  51 Batch 1327/2303   train_loss = 4.089\n",
      "Epoch  51 Batch 1347/2303   train_loss = 3.883\n",
      "Epoch  51 Batch 1367/2303   train_loss = 5.210\n",
      "Epoch  51 Batch 1387/2303   train_loss = 3.782\n",
      "Epoch  51 Batch 1407/2303   train_loss = 3.541\n",
      "Epoch  51 Batch 1427/2303   train_loss = 3.979\n",
      "Epoch  51 Batch 1447/2303   train_loss = 4.052\n",
      "Epoch  51 Batch 1467/2303   train_loss = 3.614\n",
      "Epoch  51 Batch 1487/2303   train_loss = 4.203\n",
      "Epoch  51 Batch 1507/2303   train_loss = 3.442\n",
      "Epoch  51 Batch 1527/2303   train_loss = 4.048\n",
      "Epoch  51 Batch 1547/2303   train_loss = 3.082\n",
      "Epoch  51 Batch 1567/2303   train_loss = 3.996\n",
      "Epoch  51 Batch 1587/2303   train_loss = 2.993\n",
      "Epoch  51 Batch 1607/2303   train_loss = 3.598\n",
      "Epoch  51 Batch 1627/2303   train_loss = 3.433\n",
      "Epoch  51 Batch 1647/2303   train_loss = 4.047\n",
      "Epoch  51 Batch 1667/2303   train_loss = 4.000\n",
      "Epoch  51 Batch 1687/2303   train_loss = 4.889\n",
      "Epoch  51 Batch 1707/2303   train_loss = 4.429\n",
      "Epoch  51 Batch 1727/2303   train_loss = 3.915\n",
      "Epoch  51 Batch 1747/2303   train_loss = 3.361\n",
      "Epoch  51 Batch 1767/2303   train_loss = 2.613\n",
      "Epoch  51 Batch 1787/2303   train_loss = 3.828\n",
      "Epoch  51 Batch 1807/2303   train_loss = 3.883\n",
      "Epoch  51 Batch 1827/2303   train_loss = 3.910\n",
      "Epoch  51 Batch 1847/2303   train_loss = 3.403\n",
      "Epoch  51 Batch 1867/2303   train_loss = 3.718\n",
      "Epoch  51 Batch 1887/2303   train_loss = 3.418\n",
      "Epoch  51 Batch 1907/2303   train_loss = 3.120\n",
      "Epoch  51 Batch 1927/2303   train_loss = 4.091\n",
      "Epoch  51 Batch 1947/2303   train_loss = 4.127\n",
      "Epoch  51 Batch 1967/2303   train_loss = 3.695\n",
      "Epoch  51 Batch 1987/2303   train_loss = 3.636\n",
      "Epoch  51 Batch 2007/2303   train_loss = 3.789\n",
      "Epoch  51 Batch 2027/2303   train_loss = 3.859\n",
      "Epoch  51 Batch 2047/2303   train_loss = 4.051\n",
      "Epoch  51 Batch 2067/2303   train_loss = 3.789\n",
      "Epoch  51 Batch 2087/2303   train_loss = 3.758\n",
      "Epoch  51 Batch 2107/2303   train_loss = 3.631\n",
      "Epoch  51 Batch 2127/2303   train_loss = 3.702\n",
      "Epoch  51 Batch 2147/2303   train_loss = 3.067\n",
      "Epoch  51 Batch 2167/2303   train_loss = 3.886\n",
      "Epoch  51 Batch 2187/2303   train_loss = 3.820\n",
      "Epoch  51 Batch 2207/2303   train_loss = 3.840\n",
      "Epoch  51 Batch 2227/2303   train_loss = 4.167\n",
      "Epoch  51 Batch 2247/2303   train_loss = 3.362\n",
      "Epoch  51 Batch 2267/2303   train_loss = 4.821\n",
      "Epoch  51 Batch 2287/2303   train_loss = 4.182\n",
      "Epoch  52 Batch    4/2303   train_loss = 3.832\n",
      "Epoch  52 Batch   24/2303   train_loss = 3.164\n",
      "Epoch  52 Batch   44/2303   train_loss = 3.857\n",
      "Epoch  52 Batch   64/2303   train_loss = 4.167\n",
      "Epoch  52 Batch   84/2303   train_loss = 3.769\n",
      "Epoch  52 Batch  104/2303   train_loss = 4.021\n",
      "Epoch  52 Batch  124/2303   train_loss = 4.478\n",
      "Epoch  52 Batch  144/2303   train_loss = 3.789\n",
      "Epoch  52 Batch  164/2303   train_loss = 4.077\n",
      "Epoch  52 Batch  184/2303   train_loss = 3.723\n",
      "Epoch  52 Batch  204/2303   train_loss = 3.506\n",
      "Epoch  52 Batch  224/2303   train_loss = 2.572\n",
      "Epoch  52 Batch  244/2303   train_loss = 4.649\n",
      "Epoch  52 Batch  264/2303   train_loss = 4.468\n",
      "Epoch  52 Batch  284/2303   train_loss = 4.037\n",
      "Epoch  52 Batch  304/2303   train_loss = 4.210\n",
      "Epoch  52 Batch  324/2303   train_loss = 3.538\n",
      "Epoch  52 Batch  344/2303   train_loss = 3.135\n",
      "Epoch  52 Batch  364/2303   train_loss = 4.063\n",
      "Epoch  52 Batch  384/2303   train_loss = 3.798\n",
      "Epoch  52 Batch  404/2303   train_loss = 3.782\n",
      "Epoch  52 Batch  424/2303   train_loss = 4.103\n",
      "Epoch  52 Batch  444/2303   train_loss = 4.028\n",
      "Epoch  52 Batch  464/2303   train_loss = 3.723\n",
      "Epoch  52 Batch  484/2303   train_loss = 3.954\n",
      "Epoch  52 Batch  504/2303   train_loss = 2.951\n",
      "Epoch  52 Batch  524/2303   train_loss = 3.719\n",
      "Epoch  52 Batch  544/2303   train_loss = 4.727\n",
      "Epoch  52 Batch  564/2303   train_loss = 4.119\n",
      "Epoch  52 Batch  584/2303   train_loss = 3.877\n",
      "Epoch  52 Batch  604/2303   train_loss = 3.465\n",
      "Epoch  52 Batch  624/2303   train_loss = 3.232\n",
      "Epoch  52 Batch  644/2303   train_loss = 3.994\n",
      "Epoch  52 Batch  664/2303   train_loss = 3.262\n",
      "Epoch  52 Batch  684/2303   train_loss = 3.442\n",
      "Epoch  52 Batch  704/2303   train_loss = 3.381\n",
      "Epoch  52 Batch  724/2303   train_loss = 4.136\n",
      "Epoch  52 Batch  744/2303   train_loss = 4.502\n",
      "Epoch  52 Batch  764/2303   train_loss = 4.905\n",
      "Epoch  52 Batch  784/2303   train_loss = 3.112\n",
      "Epoch  52 Batch  804/2303   train_loss = 4.536\n",
      "Epoch  52 Batch  824/2303   train_loss = 3.139\n",
      "Epoch  52 Batch  844/2303   train_loss = 3.352\n",
      "Epoch  52 Batch  864/2303   train_loss = 3.429\n",
      "Epoch  52 Batch  884/2303   train_loss = 3.878\n",
      "Epoch  52 Batch  904/2303   train_loss = 4.018\n",
      "Epoch  52 Batch  924/2303   train_loss = 3.630\n",
      "Epoch  52 Batch  944/2303   train_loss = 3.105\n",
      "Epoch  52 Batch  964/2303   train_loss = 3.391\n",
      "Epoch  52 Batch  984/2303   train_loss = 4.091\n",
      "Epoch  52 Batch 1004/2303   train_loss = 3.856\n",
      "Epoch  52 Batch 1024/2303   train_loss = 3.128\n",
      "Epoch  52 Batch 1044/2303   train_loss = 3.547\n",
      "Epoch  52 Batch 1064/2303   train_loss = 3.160\n",
      "Epoch  52 Batch 1084/2303   train_loss = 3.694\n",
      "Epoch  52 Batch 1104/2303   train_loss = 5.137\n",
      "Epoch  52 Batch 1124/2303   train_loss = 3.626\n",
      "Epoch  52 Batch 1144/2303   train_loss = 3.304\n",
      "Epoch  52 Batch 1164/2303   train_loss = 3.478\n",
      "Epoch  52 Batch 1184/2303   train_loss = 3.588\n",
      "Epoch  52 Batch 1204/2303   train_loss = 3.584\n",
      "Epoch  52 Batch 1224/2303   train_loss = 3.682\n",
      "Epoch  52 Batch 1244/2303   train_loss = 3.690\n",
      "Epoch  52 Batch 1264/2303   train_loss = 3.843\n",
      "Epoch  52 Batch 1284/2303   train_loss = 3.697\n",
      "Epoch  52 Batch 1304/2303   train_loss = 3.457\n",
      "Epoch  52 Batch 1324/2303   train_loss = 3.923\n",
      "Epoch  52 Batch 1344/2303   train_loss = 3.946\n",
      "Epoch  52 Batch 1364/2303   train_loss = 3.393\n",
      "Epoch  52 Batch 1384/2303   train_loss = 4.125\n",
      "Epoch  52 Batch 1404/2303   train_loss = 4.427\n",
      "Epoch  52 Batch 1424/2303   train_loss = 4.138\n",
      "Epoch  52 Batch 1444/2303   train_loss = 4.145\n",
      "Epoch  52 Batch 1464/2303   train_loss = 3.915\n",
      "Epoch  52 Batch 1484/2303   train_loss = 3.715\n",
      "Epoch  52 Batch 1504/2303   train_loss = 4.052\n",
      "Epoch  52 Batch 1524/2303   train_loss = 2.617\n",
      "Epoch  52 Batch 1544/2303   train_loss = 3.846\n",
      "Epoch  52 Batch 1564/2303   train_loss = 3.070\n",
      "Epoch  52 Batch 1584/2303   train_loss = 3.854\n",
      "Epoch  52 Batch 1604/2303   train_loss = 4.358\n",
      "Epoch  52 Batch 1624/2303   train_loss = 3.980\n",
      "Epoch  52 Batch 1644/2303   train_loss = 4.606\n",
      "Epoch  52 Batch 1664/2303   train_loss = 3.520\n",
      "Epoch  52 Batch 1684/2303   train_loss = 4.003\n",
      "Epoch  52 Batch 1704/2303   train_loss = 4.562\n",
      "Epoch  52 Batch 1724/2303   train_loss = 3.503\n",
      "Epoch  52 Batch 1744/2303   train_loss = 4.068\n",
      "Epoch  52 Batch 1764/2303   train_loss = 3.783\n",
      "Epoch  52 Batch 1784/2303   train_loss = 3.399\n",
      "Epoch  52 Batch 1804/2303   train_loss = 3.972\n",
      "Epoch  52 Batch 1824/2303   train_loss = 3.611\n",
      "Epoch  52 Batch 1844/2303   train_loss = 3.003\n",
      "Epoch  52 Batch 1864/2303   train_loss = 3.622\n",
      "Epoch  52 Batch 1884/2303   train_loss = 3.571\n",
      "Epoch  52 Batch 1904/2303   train_loss = 3.907\n",
      "Epoch  52 Batch 1924/2303   train_loss = 4.053\n",
      "Epoch  52 Batch 1944/2303   train_loss = 3.734\n",
      "Epoch  52 Batch 1964/2303   train_loss = 2.763\n",
      "Epoch  52 Batch 1984/2303   train_loss = 3.350\n",
      "Epoch  52 Batch 2004/2303   train_loss = 4.408\n",
      "Epoch  52 Batch 2024/2303   train_loss = 4.200\n",
      "Epoch  52 Batch 2044/2303   train_loss = 4.523\n",
      "Epoch  52 Batch 2064/2303   train_loss = 3.020\n",
      "Epoch  52 Batch 2084/2303   train_loss = 3.717\n",
      "Epoch  52 Batch 2104/2303   train_loss = 4.014\n",
      "Epoch  52 Batch 2124/2303   train_loss = 4.103\n",
      "Epoch  52 Batch 2144/2303   train_loss = 3.600\n",
      "Epoch  52 Batch 2164/2303   train_loss = 3.831\n",
      "Epoch  52 Batch 2184/2303   train_loss = 3.333\n",
      "Epoch  52 Batch 2204/2303   train_loss = 3.848\n",
      "Epoch  52 Batch 2224/2303   train_loss = 3.659\n",
      "Epoch  52 Batch 2244/2303   train_loss = 4.063\n",
      "Epoch  52 Batch 2264/2303   train_loss = 2.906\n",
      "Epoch  52 Batch 2284/2303   train_loss = 3.413\n",
      "Epoch  53 Batch    1/2303   train_loss = 3.163\n",
      "Epoch  53 Batch   21/2303   train_loss = 3.192\n",
      "Epoch  53 Batch   41/2303   train_loss = 3.840\n",
      "Epoch  53 Batch   61/2303   train_loss = 3.409\n",
      "Epoch  53 Batch   81/2303   train_loss = 3.258\n",
      "Epoch  53 Batch  101/2303   train_loss = 4.544\n",
      "Epoch  53 Batch  121/2303   train_loss = 4.573\n",
      "Epoch  53 Batch  141/2303   train_loss = 2.625\n",
      "Epoch  53 Batch  161/2303   train_loss = 3.144\n",
      "Epoch  53 Batch  181/2303   train_loss = 3.185\n",
      "Epoch  53 Batch  201/2303   train_loss = 4.165\n",
      "Epoch  53 Batch  221/2303   train_loss = 3.311\n",
      "Epoch  53 Batch  241/2303   train_loss = 3.950\n",
      "Epoch  53 Batch  261/2303   train_loss = 3.654\n",
      "Epoch  53 Batch  281/2303   train_loss = 3.092\n",
      "Epoch  53 Batch  301/2303   train_loss = 3.412\n",
      "Epoch  53 Batch  321/2303   train_loss = 4.457\n",
      "Epoch  53 Batch  341/2303   train_loss = 4.707\n",
      "Epoch  53 Batch  361/2303   train_loss = 3.523\n",
      "Epoch  53 Batch  381/2303   train_loss = 3.927\n",
      "Epoch  53 Batch  401/2303   train_loss = 3.695\n",
      "Epoch  53 Batch  421/2303   train_loss = 3.798\n",
      "Epoch  53 Batch  441/2303   train_loss = 2.566\n",
      "Epoch  53 Batch  461/2303   train_loss = 3.769\n",
      "Epoch  53 Batch  481/2303   train_loss = 3.820\n",
      "Epoch  53 Batch  501/2303   train_loss = 3.787\n",
      "Epoch  53 Batch  521/2303   train_loss = 4.321\n",
      "Epoch  53 Batch  541/2303   train_loss = 3.206\n",
      "Epoch  53 Batch  561/2303   train_loss = 3.811\n",
      "Epoch  53 Batch  581/2303   train_loss = 3.297\n",
      "Epoch  53 Batch  601/2303   train_loss = 4.126\n",
      "Epoch  53 Batch  621/2303   train_loss = 3.776\n",
      "Epoch  53 Batch  641/2303   train_loss = 3.658\n",
      "Epoch  53 Batch  661/2303   train_loss = 3.442\n",
      "Epoch  53 Batch  681/2303   train_loss = 3.864\n",
      "Epoch  53 Batch  701/2303   train_loss = 3.805\n",
      "Epoch  53 Batch  721/2303   train_loss = 3.633\n",
      "Epoch  53 Batch  741/2303   train_loss = 3.692\n",
      "Epoch  53 Batch  761/2303   train_loss = 3.672\n",
      "Epoch  53 Batch  781/2303   train_loss = 3.424\n",
      "Epoch  53 Batch  801/2303   train_loss = 3.741\n",
      "Epoch  53 Batch  821/2303   train_loss = 3.535\n",
      "Epoch  53 Batch  841/2303   train_loss = 3.904\n",
      "Epoch  53 Batch  861/2303   train_loss = 3.812\n",
      "Epoch  53 Batch  881/2303   train_loss = 3.607\n",
      "Epoch  53 Batch  901/2303   train_loss = 3.538\n",
      "Epoch  53 Batch  921/2303   train_loss = 3.579\n",
      "Epoch  53 Batch  941/2303   train_loss = 3.157\n",
      "Epoch  53 Batch  961/2303   train_loss = 3.616\n",
      "Epoch  53 Batch  981/2303   train_loss = 3.782\n",
      "Epoch  53 Batch 1001/2303   train_loss = 4.021\n",
      "Epoch  53 Batch 1021/2303   train_loss = 3.951\n",
      "Epoch  53 Batch 1041/2303   train_loss = 3.729\n",
      "Epoch  53 Batch 1061/2303   train_loss = 3.757\n",
      "Epoch  53 Batch 1081/2303   train_loss = 3.335\n",
      "Epoch  53 Batch 1101/2303   train_loss = 2.962\n",
      "Epoch  53 Batch 1121/2303   train_loss = 3.655\n",
      "Epoch  53 Batch 1141/2303   train_loss = 3.802\n",
      "Epoch  53 Batch 1161/2303   train_loss = 3.572\n",
      "Epoch  53 Batch 1181/2303   train_loss = 3.594\n",
      "Epoch  53 Batch 1201/2303   train_loss = 3.882\n",
      "Epoch  53 Batch 1221/2303   train_loss = 3.665\n",
      "Epoch  53 Batch 1241/2303   train_loss = 4.420\n",
      "Epoch  53 Batch 1261/2303   train_loss = 3.832\n",
      "Epoch  53 Batch 1281/2303   train_loss = 3.654\n",
      "Epoch  53 Batch 1301/2303   train_loss = 3.708\n",
      "Epoch  53 Batch 1321/2303   train_loss = 3.658\n",
      "Epoch  53 Batch 1341/2303   train_loss = 3.758\n",
      "Epoch  53 Batch 1361/2303   train_loss = 3.174\n",
      "Epoch  53 Batch 1381/2303   train_loss = 4.410\n",
      "Epoch  53 Batch 1401/2303   train_loss = 3.149\n",
      "Epoch  53 Batch 1421/2303   train_loss = 3.507\n",
      "Epoch  53 Batch 1441/2303   train_loss = 3.305\n",
      "Epoch  53 Batch 1461/2303   train_loss = 3.194\n",
      "Epoch  53 Batch 1481/2303   train_loss = 3.189\n",
      "Epoch  53 Batch 1501/2303   train_loss = 3.695\n",
      "Epoch  53 Batch 1521/2303   train_loss = 3.875\n",
      "Epoch  53 Batch 1541/2303   train_loss = 4.190\n",
      "Epoch  53 Batch 1561/2303   train_loss = 3.344\n",
      "Epoch  53 Batch 1581/2303   train_loss = 3.566\n",
      "Epoch  53 Batch 1601/2303   train_loss = 3.818\n",
      "Epoch  53 Batch 1621/2303   train_loss = 4.403\n",
      "Epoch  53 Batch 1641/2303   train_loss = 3.415\n",
      "Epoch  53 Batch 1661/2303   train_loss = 4.262\n",
      "Epoch  53 Batch 1681/2303   train_loss = 3.411\n",
      "Epoch  53 Batch 1701/2303   train_loss = 4.152\n",
      "Epoch  53 Batch 1721/2303   train_loss = 3.891\n",
      "Epoch  53 Batch 1741/2303   train_loss = 3.195\n",
      "Epoch  53 Batch 1761/2303   train_loss = 4.008\n",
      "Epoch  53 Batch 1781/2303   train_loss = 2.737\n",
      "Epoch  53 Batch 1801/2303   train_loss = 4.270\n",
      "Epoch  53 Batch 1821/2303   train_loss = 2.843\n",
      "Epoch  53 Batch 1841/2303   train_loss = 3.656\n",
      "Epoch  53 Batch 1861/2303   train_loss = 3.288\n",
      "Epoch  53 Batch 1881/2303   train_loss = 4.376\n",
      "Epoch  53 Batch 1901/2303   train_loss = 3.283\n",
      "Epoch  53 Batch 1921/2303   train_loss = 3.390\n",
      "Epoch  53 Batch 1941/2303   train_loss = 3.905\n",
      "Epoch  53 Batch 1961/2303   train_loss = 3.574\n",
      "Epoch  53 Batch 1981/2303   train_loss = 3.119\n",
      "Epoch  53 Batch 2001/2303   train_loss = 4.844\n",
      "Epoch  53 Batch 2021/2303   train_loss = 3.391\n",
      "Epoch  53 Batch 2041/2303   train_loss = 3.521\n",
      "Epoch  53 Batch 2061/2303   train_loss = 3.426\n",
      "Epoch  53 Batch 2081/2303   train_loss = 3.484\n",
      "Epoch  53 Batch 2101/2303   train_loss = 4.024\n",
      "Epoch  53 Batch 2121/2303   train_loss = 4.056\n",
      "Epoch  53 Batch 2141/2303   train_loss = 4.179\n",
      "Epoch  53 Batch 2161/2303   train_loss = 3.429\n",
      "Epoch  53 Batch 2181/2303   train_loss = 4.140\n",
      "Epoch  53 Batch 2201/2303   train_loss = 3.991\n",
      "Epoch  53 Batch 2221/2303   train_loss = 2.716\n",
      "Epoch  53 Batch 2241/2303   train_loss = 2.806\n",
      "Epoch  53 Batch 2261/2303   train_loss = 2.391\n",
      "Epoch  53 Batch 2281/2303   train_loss = 4.087\n",
      "Epoch  53 Batch 2301/2303   train_loss = 3.692\n",
      "Epoch  54 Batch   18/2303   train_loss = 3.762\n",
      "Epoch  54 Batch   38/2303   train_loss = 3.664\n",
      "Epoch  54 Batch   58/2303   train_loss = 3.951\n",
      "Epoch  54 Batch   78/2303   train_loss = 3.784\n",
      "Epoch  54 Batch   98/2303   train_loss = 3.586\n",
      "Epoch  54 Batch  118/2303   train_loss = 3.274\n",
      "Epoch  54 Batch  138/2303   train_loss = 3.733\n",
      "Epoch  54 Batch  158/2303   train_loss = 3.068\n",
      "Epoch  54 Batch  178/2303   train_loss = 3.424\n",
      "Epoch  54 Batch  198/2303   train_loss = 3.654\n",
      "Epoch  54 Batch  218/2303   train_loss = 3.201\n",
      "Epoch  54 Batch  238/2303   train_loss = 3.683\n",
      "Epoch  54 Batch  258/2303   train_loss = 2.998\n",
      "Epoch  54 Batch  278/2303   train_loss = 3.830\n",
      "Epoch  54 Batch  298/2303   train_loss = 3.292\n",
      "Epoch  54 Batch  318/2303   train_loss = 4.672\n",
      "Epoch  54 Batch  338/2303   train_loss = 3.822\n",
      "Epoch  54 Batch  358/2303   train_loss = 3.707\n",
      "Epoch  54 Batch  378/2303   train_loss = 3.698\n",
      "Epoch  54 Batch  398/2303   train_loss = 3.879\n",
      "Epoch  54 Batch  418/2303   train_loss = 4.215\n",
      "Epoch  54 Batch  438/2303   train_loss = 3.767\n",
      "Epoch  54 Batch  458/2303   train_loss = 3.789\n",
      "Epoch  54 Batch  478/2303   train_loss = 3.983\n",
      "Epoch  54 Batch  498/2303   train_loss = 3.730\n",
      "Epoch  54 Batch  518/2303   train_loss = 4.350\n",
      "Epoch  54 Batch  538/2303   train_loss = 3.321\n",
      "Epoch  54 Batch  558/2303   train_loss = 4.160\n",
      "Epoch  54 Batch  578/2303   train_loss = 3.440\n",
      "Epoch  54 Batch  598/2303   train_loss = 4.111\n",
      "Epoch  54 Batch  618/2303   train_loss = 3.739\n",
      "Epoch  54 Batch  638/2303   train_loss = 2.190\n",
      "Epoch  54 Batch  658/2303   train_loss = 4.072\n",
      "Epoch  54 Batch  678/2303   train_loss = 3.605\n",
      "Epoch  54 Batch  698/2303   train_loss = 3.751\n",
      "Epoch  54 Batch  718/2303   train_loss = 3.391\n",
      "Epoch  54 Batch  738/2303   train_loss = 3.845\n",
      "Epoch  54 Batch  758/2303   train_loss = 3.538\n",
      "Epoch  54 Batch  778/2303   train_loss = 4.938\n",
      "Epoch  54 Batch  798/2303   train_loss = 3.020\n",
      "Epoch  54 Batch  818/2303   train_loss = 3.976\n",
      "Epoch  54 Batch  838/2303   train_loss = 3.392\n",
      "Epoch  54 Batch  858/2303   train_loss = 3.424\n",
      "Epoch  54 Batch  878/2303   train_loss = 2.936\n",
      "Epoch  54 Batch  898/2303   train_loss = 2.859\n",
      "Epoch  54 Batch  918/2303   train_loss = 3.528\n",
      "Epoch  54 Batch  938/2303   train_loss = 3.883\n",
      "Epoch  54 Batch  958/2303   train_loss = 4.210\n",
      "Epoch  54 Batch  978/2303   train_loss = 4.276\n",
      "Epoch  54 Batch  998/2303   train_loss = 4.287\n",
      "Epoch  54 Batch 1018/2303   train_loss = 4.293\n",
      "Epoch  54 Batch 1038/2303   train_loss = 3.106\n",
      "Epoch  54 Batch 1058/2303   train_loss = 4.051\n",
      "Epoch  54 Batch 1078/2303   train_loss = 4.171\n",
      "Epoch  54 Batch 1098/2303   train_loss = 3.175\n",
      "Epoch  54 Batch 1118/2303   train_loss = 3.214\n",
      "Epoch  54 Batch 1138/2303   train_loss = 4.201\n",
      "Epoch  54 Batch 1158/2303   train_loss = 3.587\n",
      "Epoch  54 Batch 1178/2303   train_loss = 3.726\n",
      "Epoch  54 Batch 1198/2303   train_loss = 3.203\n",
      "Epoch  54 Batch 1218/2303   train_loss = 3.543\n",
      "Epoch  54 Batch 1238/2303   train_loss = 3.954\n",
      "Epoch  54 Batch 1258/2303   train_loss = 3.916\n",
      "Epoch  54 Batch 1278/2303   train_loss = 3.222\n",
      "Epoch  54 Batch 1298/2303   train_loss = 4.504\n",
      "Epoch  54 Batch 1318/2303   train_loss = 4.101\n",
      "Epoch  54 Batch 1338/2303   train_loss = 3.362\n",
      "Epoch  54 Batch 1358/2303   train_loss = 4.529\n",
      "Epoch  54 Batch 1378/2303   train_loss = 4.030\n",
      "Epoch  54 Batch 1398/2303   train_loss = 3.451\n",
      "Epoch  54 Batch 1418/2303   train_loss = 3.722\n",
      "Epoch  54 Batch 1438/2303   train_loss = 3.963\n",
      "Epoch  54 Batch 1458/2303   train_loss = 3.698\n",
      "Epoch  54 Batch 1478/2303   train_loss = 4.363\n",
      "Epoch  54 Batch 1498/2303   train_loss = 3.141\n",
      "Epoch  54 Batch 1518/2303   train_loss = 3.709\n",
      "Epoch  54 Batch 1538/2303   train_loss = 3.871\n",
      "Epoch  54 Batch 1558/2303   train_loss = 3.796\n",
      "Epoch  54 Batch 1578/2303   train_loss = 4.033\n",
      "Epoch  54 Batch 1598/2303   train_loss = 3.353\n",
      "Epoch  54 Batch 1618/2303   train_loss = 3.314\n",
      "Epoch  54 Batch 1638/2303   train_loss = 3.614\n",
      "Epoch  54 Batch 1658/2303   train_loss = 3.156\n",
      "Epoch  54 Batch 1678/2303   train_loss = 4.450\n",
      "Epoch  54 Batch 1698/2303   train_loss = 3.706\n",
      "Epoch  54 Batch 1718/2303   train_loss = 3.883\n",
      "Epoch  54 Batch 1738/2303   train_loss = 3.233\n",
      "Epoch  54 Batch 1758/2303   train_loss = 3.638\n",
      "Epoch  54 Batch 1778/2303   train_loss = 3.895\n",
      "Epoch  54 Batch 1798/2303   train_loss = 3.932\n",
      "Epoch  54 Batch 1818/2303   train_loss = 4.270\n",
      "Epoch  54 Batch 1838/2303   train_loss = 3.146\n",
      "Epoch  54 Batch 1858/2303   train_loss = 4.256\n",
      "Epoch  54 Batch 1878/2303   train_loss = 3.537\n",
      "Epoch  54 Batch 1898/2303   train_loss = 4.106\n",
      "Epoch  54 Batch 1918/2303   train_loss = 3.263\n",
      "Epoch  54 Batch 1938/2303   train_loss = 4.493\n",
      "Epoch  54 Batch 1958/2303   train_loss = 4.510\n",
      "Epoch  54 Batch 1978/2303   train_loss = 3.775\n",
      "Epoch  54 Batch 1998/2303   train_loss = 4.149\n",
      "Epoch  54 Batch 2018/2303   train_loss = 3.457\n",
      "Epoch  54 Batch 2038/2303   train_loss = 4.413\n",
      "Epoch  54 Batch 2058/2303   train_loss = 3.894\n",
      "Epoch  54 Batch 2078/2303   train_loss = 3.544\n",
      "Epoch  54 Batch 2098/2303   train_loss = 2.998\n",
      "Epoch  54 Batch 2118/2303   train_loss = 5.035\n",
      "Epoch  54 Batch 2138/2303   train_loss = 2.838\n",
      "Epoch  54 Batch 2158/2303   train_loss = 4.027\n",
      "Epoch  54 Batch 2178/2303   train_loss = 4.050\n",
      "Epoch  54 Batch 2198/2303   train_loss = 3.752\n",
      "Epoch  54 Batch 2218/2303   train_loss = 3.713\n",
      "Epoch  54 Batch 2238/2303   train_loss = 3.422\n",
      "Epoch  54 Batch 2258/2303   train_loss = 3.348\n",
      "Epoch  54 Batch 2278/2303   train_loss = 3.730\n",
      "Epoch  54 Batch 2298/2303   train_loss = 3.123\n",
      "Epoch  55 Batch   15/2303   train_loss = 3.337\n",
      "Epoch  55 Batch   35/2303   train_loss = 2.983\n",
      "Epoch  55 Batch   55/2303   train_loss = 3.899\n",
      "Epoch  55 Batch   75/2303   train_loss = 4.260\n",
      "Epoch  55 Batch   95/2303   train_loss = 2.955\n",
      "Epoch  55 Batch  115/2303   train_loss = 3.138\n",
      "Epoch  55 Batch  135/2303   train_loss = 3.756\n",
      "Epoch  55 Batch  155/2303   train_loss = 3.501\n",
      "Epoch  55 Batch  175/2303   train_loss = 3.888\n",
      "Epoch  55 Batch  195/2303   train_loss = 3.519\n",
      "Epoch  55 Batch  215/2303   train_loss = 2.936\n",
      "Epoch  55 Batch  235/2303   train_loss = 3.000\n",
      "Epoch  55 Batch  255/2303   train_loss = 3.989\n",
      "Epoch  55 Batch  275/2303   train_loss = 3.845\n",
      "Epoch  55 Batch  295/2303   train_loss = 4.423\n",
      "Epoch  55 Batch  315/2303   train_loss = 4.330\n",
      "Epoch  55 Batch  335/2303   train_loss = 4.882\n",
      "Epoch  55 Batch  355/2303   train_loss = 3.642\n",
      "Epoch  55 Batch  375/2303   train_loss = 5.578\n",
      "Epoch  55 Batch  395/2303   train_loss = 2.981\n",
      "Epoch  55 Batch  415/2303   train_loss = 3.342\n",
      "Epoch  55 Batch  435/2303   train_loss = 4.093\n",
      "Epoch  55 Batch  455/2303   train_loss = 4.354\n",
      "Epoch  55 Batch  475/2303   train_loss = 3.675\n",
      "Epoch  55 Batch  495/2303   train_loss = 2.978\n",
      "Epoch  55 Batch  515/2303   train_loss = 3.977\n",
      "Epoch  55 Batch  535/2303   train_loss = 4.224\n",
      "Epoch  55 Batch  555/2303   train_loss = 4.223\n",
      "Epoch  55 Batch  575/2303   train_loss = 3.759\n",
      "Epoch  55 Batch  595/2303   train_loss = 3.314\n",
      "Epoch  55 Batch  615/2303   train_loss = 3.119\n",
      "Epoch  55 Batch  635/2303   train_loss = 3.968\n",
      "Epoch  55 Batch  655/2303   train_loss = 3.082\n",
      "Epoch  55 Batch  675/2303   train_loss = 3.420\n",
      "Epoch  55 Batch  695/2303   train_loss = 3.906\n",
      "Epoch  55 Batch  715/2303   train_loss = 3.483\n",
      "Epoch  55 Batch  735/2303   train_loss = 3.324\n",
      "Epoch  55 Batch  755/2303   train_loss = 3.556\n",
      "Epoch  55 Batch  775/2303   train_loss = 3.788\n",
      "Epoch  55 Batch  795/2303   train_loss = 4.135\n",
      "Epoch  55 Batch  815/2303   train_loss = 3.841\n",
      "Epoch  55 Batch  835/2303   train_loss = 3.935\n",
      "Epoch  55 Batch  855/2303   train_loss = 3.415\n",
      "Epoch  55 Batch  875/2303   train_loss = 4.445\n",
      "Epoch  55 Batch  895/2303   train_loss = 4.364\n",
      "Epoch  55 Batch  915/2303   train_loss = 2.566\n",
      "Epoch  55 Batch  935/2303   train_loss = 4.025\n",
      "Epoch  55 Batch  955/2303   train_loss = 3.764\n",
      "Epoch  55 Batch  975/2303   train_loss = 4.018\n",
      "Epoch  55 Batch  995/2303   train_loss = 3.433\n",
      "Epoch  55 Batch 1015/2303   train_loss = 3.629\n",
      "Epoch  55 Batch 1035/2303   train_loss = 4.074\n",
      "Epoch  55 Batch 1055/2303   train_loss = 4.769\n",
      "Epoch  55 Batch 1075/2303   train_loss = 3.646\n",
      "Epoch  55 Batch 1095/2303   train_loss = 3.207\n",
      "Epoch  55 Batch 1115/2303   train_loss = 4.478\n",
      "Epoch  55 Batch 1135/2303   train_loss = 3.380\n",
      "Epoch  55 Batch 1155/2303   train_loss = 4.669\n",
      "Epoch  55 Batch 1175/2303   train_loss = 4.122\n",
      "Epoch  55 Batch 1195/2303   train_loss = 3.707\n",
      "Epoch  55 Batch 1215/2303   train_loss = 4.503\n",
      "Epoch  55 Batch 1235/2303   train_loss = 3.644\n",
      "Epoch  55 Batch 1255/2303   train_loss = 3.298\n",
      "Epoch  55 Batch 1275/2303   train_loss = 2.956\n",
      "Epoch  55 Batch 1295/2303   train_loss = 4.185\n",
      "Epoch  55 Batch 1315/2303   train_loss = 3.348\n",
      "Epoch  55 Batch 1335/2303   train_loss = 4.428\n",
      "Epoch  55 Batch 1355/2303   train_loss = 3.873\n",
      "Epoch  55 Batch 1375/2303   train_loss = 4.398\n",
      "Epoch  55 Batch 1395/2303   train_loss = 2.747\n",
      "Epoch  55 Batch 1415/2303   train_loss = 3.117\n",
      "Epoch  55 Batch 1435/2303   train_loss = 3.451\n",
      "Epoch  55 Batch 1455/2303   train_loss = 3.341\n",
      "Epoch  55 Batch 1475/2303   train_loss = 3.953\n",
      "Epoch  55 Batch 1495/2303   train_loss = 3.525\n",
      "Epoch  55 Batch 1515/2303   train_loss = 3.667\n",
      "Epoch  55 Batch 1535/2303   train_loss = 3.893\n",
      "Epoch  55 Batch 1555/2303   train_loss = 3.932\n",
      "Epoch  55 Batch 1575/2303   train_loss = 4.004\n",
      "Epoch  55 Batch 1595/2303   train_loss = 4.095\n",
      "Epoch  55 Batch 1615/2303   train_loss = 3.837\n",
      "Epoch  55 Batch 1635/2303   train_loss = 3.713\n",
      "Epoch  55 Batch 1655/2303   train_loss = 3.498\n",
      "Epoch  55 Batch 1675/2303   train_loss = 3.444\n",
      "Epoch  55 Batch 1695/2303   train_loss = 3.576\n",
      "Epoch  55 Batch 1715/2303   train_loss = 4.021\n",
      "Epoch  55 Batch 1735/2303   train_loss = 4.809\n",
      "Epoch  55 Batch 1755/2303   train_loss = 4.246\n",
      "Epoch  55 Batch 1775/2303   train_loss = 3.902\n",
      "Epoch  55 Batch 1795/2303   train_loss = 3.665\n",
      "Epoch  55 Batch 1815/2303   train_loss = 3.901\n",
      "Epoch  55 Batch 1835/2303   train_loss = 3.460\n",
      "Epoch  55 Batch 1855/2303   train_loss = 3.694\n",
      "Epoch  55 Batch 1875/2303   train_loss = 3.907\n",
      "Epoch  55 Batch 1895/2303   train_loss = 3.738\n",
      "Epoch  55 Batch 1915/2303   train_loss = 3.805\n",
      "Epoch  55 Batch 1935/2303   train_loss = 3.681\n",
      "Epoch  55 Batch 1955/2303   train_loss = 3.381\n",
      "Epoch  55 Batch 1975/2303   train_loss = 4.134\n",
      "Epoch  55 Batch 1995/2303   train_loss = 3.178\n",
      "Epoch  55 Batch 2015/2303   train_loss = 3.945\n",
      "Epoch  55 Batch 2035/2303   train_loss = 3.787\n",
      "Epoch  55 Batch 2055/2303   train_loss = 3.827\n",
      "Epoch  55 Batch 2075/2303   train_loss = 4.018\n",
      "Epoch  55 Batch 2095/2303   train_loss = 3.150\n",
      "Epoch  55 Batch 2115/2303   train_loss = 3.724\n",
      "Epoch  55 Batch 2135/2303   train_loss = 3.366\n",
      "Epoch  55 Batch 2155/2303   train_loss = 3.614\n",
      "Epoch  55 Batch 2175/2303   train_loss = 2.802\n",
      "Epoch  55 Batch 2195/2303   train_loss = 4.398\n",
      "Epoch  55 Batch 2215/2303   train_loss = 4.680\n",
      "Epoch  55 Batch 2235/2303   train_loss = 3.248\n",
      "Epoch  55 Batch 2255/2303   train_loss = 3.780\n",
      "Epoch  55 Batch 2275/2303   train_loss = 4.576\n",
      "Epoch  55 Batch 2295/2303   train_loss = 3.729\n",
      "Epoch  56 Batch   12/2303   train_loss = 4.119\n",
      "Epoch  56 Batch   32/2303   train_loss = 2.583\n",
      "Epoch  56 Batch   52/2303   train_loss = 3.958\n",
      "Epoch  56 Batch   72/2303   train_loss = 3.054\n",
      "Epoch  56 Batch   92/2303   train_loss = 3.766\n",
      "Epoch  56 Batch  112/2303   train_loss = 2.779\n",
      "Epoch  56 Batch  132/2303   train_loss = 3.766\n",
      "Epoch  56 Batch  152/2303   train_loss = 3.270\n",
      "Epoch  56 Batch  172/2303   train_loss = 3.011\n",
      "Epoch  56 Batch  192/2303   train_loss = 3.148\n",
      "Epoch  56 Batch  212/2303   train_loss = 1.910\n",
      "Epoch  56 Batch  232/2303   train_loss = 4.106\n",
      "Epoch  56 Batch  252/2303   train_loss = 3.948\n",
      "Epoch  56 Batch  272/2303   train_loss = 3.014\n",
      "Epoch  56 Batch  292/2303   train_loss = 4.195\n",
      "Epoch  56 Batch  312/2303   train_loss = 3.244\n",
      "Epoch  56 Batch  332/2303   train_loss = 4.640\n",
      "Epoch  56 Batch  352/2303   train_loss = 4.309\n",
      "Epoch  56 Batch  372/2303   train_loss = 3.860\n",
      "Epoch  56 Batch  392/2303   train_loss = 3.337\n",
      "Epoch  56 Batch  412/2303   train_loss = 4.122\n",
      "Epoch  56 Batch  432/2303   train_loss = 3.872\n",
      "Epoch  56 Batch  452/2303   train_loss = 4.586\n",
      "Epoch  56 Batch  472/2303   train_loss = 4.779\n",
      "Epoch  56 Batch  492/2303   train_loss = 3.981\n",
      "Epoch  56 Batch  512/2303   train_loss = 3.899\n",
      "Epoch  56 Batch  532/2303   train_loss = 3.551\n",
      "Epoch  56 Batch  552/2303   train_loss = 4.357\n",
      "Epoch  56 Batch  572/2303   train_loss = 3.584\n",
      "Epoch  56 Batch  592/2303   train_loss = 3.579\n",
      "Epoch  56 Batch  612/2303   train_loss = 3.163\n",
      "Epoch  56 Batch  632/2303   train_loss = 4.583\n",
      "Epoch  56 Batch  652/2303   train_loss = 3.910\n",
      "Epoch  56 Batch  672/2303   train_loss = 3.870\n",
      "Epoch  56 Batch  692/2303   train_loss = 4.574\n",
      "Epoch  56 Batch  712/2303   train_loss = 4.560\n",
      "Epoch  56 Batch  732/2303   train_loss = 3.403\n",
      "Epoch  56 Batch  752/2303   train_loss = 4.222\n",
      "Epoch  56 Batch  772/2303   train_loss = 3.076\n",
      "Epoch  56 Batch  792/2303   train_loss = 4.236\n",
      "Epoch  56 Batch  812/2303   train_loss = 4.409\n",
      "Epoch  56 Batch  832/2303   train_loss = 3.727\n",
      "Epoch  56 Batch  852/2303   train_loss = 3.532\n",
      "Epoch  56 Batch  872/2303   train_loss = 3.911\n",
      "Epoch  56 Batch  892/2303   train_loss = 4.676\n",
      "Epoch  56 Batch  912/2303   train_loss = 4.464\n",
      "Epoch  56 Batch  932/2303   train_loss = 4.430\n",
      "Epoch  56 Batch  952/2303   train_loss = 4.165\n",
      "Epoch  56 Batch  972/2303   train_loss = 3.305\n",
      "Epoch  56 Batch  992/2303   train_loss = 4.463\n",
      "Epoch  56 Batch 1012/2303   train_loss = 3.205\n",
      "Epoch  56 Batch 1032/2303   train_loss = 3.594\n",
      "Epoch  56 Batch 1052/2303   train_loss = 3.619\n",
      "Epoch  56 Batch 1072/2303   train_loss = 3.759\n",
      "Epoch  56 Batch 1092/2303   train_loss = 3.647\n",
      "Epoch  56 Batch 1112/2303   train_loss = 3.639\n",
      "Epoch  56 Batch 1132/2303   train_loss = 3.918\n",
      "Epoch  56 Batch 1152/2303   train_loss = 3.669\n",
      "Epoch  56 Batch 1172/2303   train_loss = 4.734\n",
      "Epoch  56 Batch 1192/2303   train_loss = 3.877\n",
      "Epoch  56 Batch 1212/2303   train_loss = 3.972\n",
      "Epoch  56 Batch 1232/2303   train_loss = 3.960\n",
      "Epoch  56 Batch 1252/2303   train_loss = 4.184\n",
      "Epoch  56 Batch 1272/2303   train_loss = 3.084\n",
      "Epoch  56 Batch 1292/2303   train_loss = 4.629\n",
      "Epoch  56 Batch 1312/2303   train_loss = 3.072\n",
      "Epoch  56 Batch 1332/2303   train_loss = 3.615\n",
      "Epoch  56 Batch 1352/2303   train_loss = 3.496\n",
      "Epoch  56 Batch 1372/2303   train_loss = 4.009\n",
      "Epoch  56 Batch 1392/2303   train_loss = 4.117\n",
      "Epoch  56 Batch 1412/2303   train_loss = 3.939\n",
      "Epoch  56 Batch 1432/2303   train_loss = 4.299\n",
      "Epoch  56 Batch 1452/2303   train_loss = 3.495\n",
      "Epoch  56 Batch 1472/2303   train_loss = 4.694\n",
      "Epoch  56 Batch 1492/2303   train_loss = 3.578\n",
      "Epoch  56 Batch 1512/2303   train_loss = 3.690\n",
      "Epoch  56 Batch 1532/2303   train_loss = 3.424\n",
      "Epoch  56 Batch 1552/2303   train_loss = 4.203\n",
      "Epoch  56 Batch 1572/2303   train_loss = 3.724\n",
      "Epoch  56 Batch 1592/2303   train_loss = 3.589\n",
      "Epoch  56 Batch 1612/2303   train_loss = 3.882\n",
      "Epoch  56 Batch 1632/2303   train_loss = 3.552\n",
      "Epoch  56 Batch 1652/2303   train_loss = 2.452\n",
      "Epoch  56 Batch 1672/2303   train_loss = 3.790\n",
      "Epoch  56 Batch 1692/2303   train_loss = 4.624\n",
      "Epoch  56 Batch 1712/2303   train_loss = 4.170\n",
      "Epoch  56 Batch 1732/2303   train_loss = 4.464\n",
      "Epoch  56 Batch 1752/2303   train_loss = 4.082\n",
      "Epoch  56 Batch 1772/2303   train_loss = 3.154\n",
      "Epoch  56 Batch 1792/2303   train_loss = 4.751\n",
      "Epoch  56 Batch 1812/2303   train_loss = 4.282\n",
      "Epoch  56 Batch 1832/2303   train_loss = 3.958\n",
      "Epoch  56 Batch 1852/2303   train_loss = 4.162\n",
      "Epoch  56 Batch 1872/2303   train_loss = 3.825\n",
      "Epoch  56 Batch 1892/2303   train_loss = 4.222\n",
      "Epoch  56 Batch 1912/2303   train_loss = 4.024\n",
      "Epoch  56 Batch 1932/2303   train_loss = 4.149\n",
      "Epoch  56 Batch 1952/2303   train_loss = 4.101\n",
      "Epoch  56 Batch 1972/2303   train_loss = 3.720\n",
      "Epoch  56 Batch 1992/2303   train_loss = 3.542\n",
      "Epoch  56 Batch 2012/2303   train_loss = 4.032\n",
      "Epoch  56 Batch 2032/2303   train_loss = 3.678\n",
      "Epoch  56 Batch 2052/2303   train_loss = 3.459\n",
      "Epoch  56 Batch 2072/2303   train_loss = 3.900\n",
      "Epoch  56 Batch 2092/2303   train_loss = 3.622\n",
      "Epoch  56 Batch 2112/2303   train_loss = 3.207\n",
      "Epoch  56 Batch 2132/2303   train_loss = 3.677\n",
      "Epoch  56 Batch 2152/2303   train_loss = 4.042\n",
      "Epoch  56 Batch 2172/2303   train_loss = 4.185\n",
      "Epoch  56 Batch 2192/2303   train_loss = 3.882\n",
      "Epoch  56 Batch 2212/2303   train_loss = 3.979\n",
      "Epoch  56 Batch 2232/2303   train_loss = 3.781\n",
      "Epoch  56 Batch 2252/2303   train_loss = 3.187\n",
      "Epoch  56 Batch 2272/2303   train_loss = 3.791\n",
      "Epoch  56 Batch 2292/2303   train_loss = 3.710\n",
      "Epoch  57 Batch    9/2303   train_loss = 3.958\n",
      "Epoch  57 Batch   29/2303   train_loss = 3.518\n",
      "Epoch  57 Batch   49/2303   train_loss = 3.330\n",
      "Epoch  57 Batch   69/2303   train_loss = 2.726\n",
      "Epoch  57 Batch   89/2303   train_loss = 4.008\n",
      "Epoch  57 Batch  109/2303   train_loss = 3.005\n",
      "Epoch  57 Batch  129/2303   train_loss = 4.484\n",
      "Epoch  57 Batch  149/2303   train_loss = 3.141\n",
      "Epoch  57 Batch  169/2303   train_loss = 2.848\n",
      "Epoch  57 Batch  189/2303   train_loss = 3.617\n",
      "Epoch  57 Batch  209/2303   train_loss = 3.784\n",
      "Epoch  57 Batch  229/2303   train_loss = 4.792\n",
      "Epoch  57 Batch  249/2303   train_loss = 3.635\n",
      "Epoch  57 Batch  269/2303   train_loss = 3.962\n",
      "Epoch  57 Batch  289/2303   train_loss = 4.158\n",
      "Epoch  57 Batch  309/2303   train_loss = 4.320\n",
      "Epoch  57 Batch  329/2303   train_loss = 3.393\n",
      "Epoch  57 Batch  349/2303   train_loss = 4.194\n",
      "Epoch  57 Batch  369/2303   train_loss = 4.525\n",
      "Epoch  57 Batch  389/2303   train_loss = 3.727\n",
      "Epoch  57 Batch  409/2303   train_loss = 3.403\n",
      "Epoch  57 Batch  429/2303   train_loss = 3.574\n",
      "Epoch  57 Batch  449/2303   train_loss = 3.759\n",
      "Epoch  57 Batch  469/2303   train_loss = 4.917\n",
      "Epoch  57 Batch  489/2303   train_loss = 3.872\n",
      "Epoch  57 Batch  509/2303   train_loss = 4.002\n",
      "Epoch  57 Batch  529/2303   train_loss = 3.689\n",
      "Epoch  57 Batch  549/2303   train_loss = 4.414\n",
      "Epoch  57 Batch  569/2303   train_loss = 3.133\n",
      "Epoch  57 Batch  589/2303   train_loss = 3.975\n",
      "Epoch  57 Batch  609/2303   train_loss = 4.100\n",
      "Epoch  57 Batch  629/2303   train_loss = 2.662\n",
      "Epoch  57 Batch  649/2303   train_loss = 4.479\n",
      "Epoch  57 Batch  669/2303   train_loss = 4.152\n",
      "Epoch  57 Batch  689/2303   train_loss = 3.601\n",
      "Epoch  57 Batch  709/2303   train_loss = 3.264\n",
      "Epoch  57 Batch  729/2303   train_loss = 2.528\n",
      "Epoch  57 Batch  749/2303   train_loss = 3.902\n",
      "Epoch  57 Batch  769/2303   train_loss = 4.342\n",
      "Epoch  57 Batch  789/2303   train_loss = 3.353\n",
      "Epoch  57 Batch  809/2303   train_loss = 3.822\n",
      "Epoch  57 Batch  829/2303   train_loss = 3.925\n",
      "Epoch  57 Batch  849/2303   train_loss = 3.657\n",
      "Epoch  57 Batch  869/2303   train_loss = 3.378\n",
      "Epoch  57 Batch  889/2303   train_loss = 3.669\n",
      "Epoch  57 Batch  909/2303   train_loss = 4.449\n",
      "Epoch  57 Batch  929/2303   train_loss = 3.705\n",
      "Epoch  57 Batch  949/2303   train_loss = 3.459\n",
      "Epoch  57 Batch  969/2303   train_loss = 4.166\n",
      "Epoch  57 Batch  989/2303   train_loss = 4.517\n",
      "Epoch  57 Batch 1009/2303   train_loss = 3.534\n",
      "Epoch  57 Batch 1029/2303   train_loss = 2.909\n",
      "Epoch  57 Batch 1049/2303   train_loss = 3.303\n",
      "Epoch  57 Batch 1069/2303   train_loss = 3.806\n",
      "Epoch  57 Batch 1089/2303   train_loss = 3.931\n",
      "Epoch  57 Batch 1109/2303   train_loss = 1.970\n",
      "Epoch  57 Batch 1129/2303   train_loss = 4.001\n",
      "Epoch  57 Batch 1149/2303   train_loss = 3.454\n",
      "Epoch  57 Batch 1169/2303   train_loss = 3.899\n",
      "Epoch  57 Batch 1189/2303   train_loss = 3.907\n",
      "Epoch  57 Batch 1209/2303   train_loss = 3.659\n",
      "Epoch  57 Batch 1229/2303   train_loss = 3.472\n",
      "Epoch  57 Batch 1249/2303   train_loss = 3.608\n",
      "Epoch  57 Batch 1269/2303   train_loss = 2.924\n",
      "Epoch  57 Batch 1289/2303   train_loss = 3.883\n",
      "Epoch  57 Batch 1309/2303   train_loss = 3.579\n",
      "Epoch  57 Batch 1329/2303   train_loss = 4.783\n",
      "Epoch  57 Batch 1349/2303   train_loss = 3.457\n",
      "Epoch  57 Batch 1369/2303   train_loss = 3.254\n",
      "Epoch  57 Batch 1389/2303   train_loss = 2.962\n",
      "Epoch  57 Batch 1409/2303   train_loss = 3.947\n",
      "Epoch  57 Batch 1429/2303   train_loss = 3.957\n",
      "Epoch  57 Batch 1449/2303   train_loss = 3.662\n",
      "Epoch  57 Batch 1469/2303   train_loss = 3.413\n",
      "Epoch  57 Batch 1489/2303   train_loss = 3.994\n",
      "Epoch  57 Batch 1509/2303   train_loss = 4.366\n",
      "Epoch  57 Batch 1529/2303   train_loss = 3.143\n",
      "Epoch  57 Batch 1549/2303   train_loss = 4.646\n",
      "Epoch  57 Batch 1569/2303   train_loss = 3.549\n",
      "Epoch  57 Batch 1589/2303   train_loss = 3.360\n",
      "Epoch  57 Batch 1609/2303   train_loss = 3.913\n",
      "Epoch  57 Batch 1629/2303   train_loss = 3.816\n",
      "Epoch  57 Batch 1649/2303   train_loss = 4.359\n",
      "Epoch  57 Batch 1669/2303   train_loss = 3.558\n",
      "Epoch  57 Batch 1689/2303   train_loss = 3.766\n",
      "Epoch  57 Batch 1709/2303   train_loss = 3.001\n",
      "Epoch  57 Batch 1729/2303   train_loss = 4.410\n",
      "Epoch  57 Batch 1749/2303   train_loss = 4.154\n",
      "Epoch  57 Batch 1769/2303   train_loss = 4.130\n",
      "Epoch  57 Batch 1789/2303   train_loss = 4.136\n",
      "Epoch  57 Batch 1809/2303   train_loss = 3.630\n",
      "Epoch  57 Batch 1829/2303   train_loss = 3.046\n",
      "Epoch  57 Batch 1849/2303   train_loss = 3.808\n",
      "Epoch  57 Batch 1869/2303   train_loss = 3.378\n",
      "Epoch  57 Batch 1889/2303   train_loss = 4.316\n",
      "Epoch  57 Batch 1909/2303   train_loss = 3.809\n",
      "Epoch  57 Batch 1929/2303   train_loss = 3.465\n",
      "Epoch  57 Batch 1949/2303   train_loss = 3.960\n",
      "Epoch  57 Batch 1969/2303   train_loss = 3.861\n",
      "Epoch  57 Batch 1989/2303   train_loss = 3.064\n",
      "Epoch  57 Batch 2009/2303   train_loss = 4.009\n",
      "Epoch  57 Batch 2029/2303   train_loss = 3.788\n",
      "Epoch  57 Batch 2049/2303   train_loss = 2.939\n",
      "Epoch  57 Batch 2069/2303   train_loss = 3.083\n",
      "Epoch  57 Batch 2089/2303   train_loss = 3.426\n",
      "Epoch  57 Batch 2109/2303   train_loss = 4.156\n",
      "Epoch  57 Batch 2129/2303   train_loss = 3.788\n",
      "Epoch  57 Batch 2149/2303   train_loss = 3.573\n",
      "Epoch  57 Batch 2169/2303   train_loss = 4.524\n",
      "Epoch  57 Batch 2189/2303   train_loss = 4.744\n",
      "Epoch  57 Batch 2209/2303   train_loss = 3.956\n",
      "Epoch  57 Batch 2229/2303   train_loss = 3.713\n",
      "Epoch  57 Batch 2249/2303   train_loss = 3.707\n",
      "Epoch  57 Batch 2269/2303   train_loss = 3.330\n",
      "Epoch  57 Batch 2289/2303   train_loss = 3.497\n",
      "Epoch  58 Batch    6/2303   train_loss = 3.796\n",
      "Epoch  58 Batch   26/2303   train_loss = 3.580\n",
      "Epoch  58 Batch   46/2303   train_loss = 2.995\n",
      "Epoch  58 Batch   66/2303   train_loss = 3.448\n",
      "Epoch  58 Batch   86/2303   train_loss = 3.970\n",
      "Epoch  58 Batch  106/2303   train_loss = 3.139\n",
      "Epoch  58 Batch  126/2303   train_loss = 4.080\n",
      "Epoch  58 Batch  146/2303   train_loss = 4.409\n",
      "Epoch  58 Batch  166/2303   train_loss = 5.310\n",
      "Epoch  58 Batch  186/2303   train_loss = 3.611\n",
      "Epoch  58 Batch  206/2303   train_loss = 3.814\n",
      "Epoch  58 Batch  226/2303   train_loss = 3.710\n",
      "Epoch  58 Batch  246/2303   train_loss = 4.723\n",
      "Epoch  58 Batch  266/2303   train_loss = 3.554\n",
      "Epoch  58 Batch  286/2303   train_loss = 3.911\n",
      "Epoch  58 Batch  306/2303   train_loss = 3.912\n",
      "Epoch  58 Batch  326/2303   train_loss = 3.332\n",
      "Epoch  58 Batch  346/2303   train_loss = 4.762\n",
      "Epoch  58 Batch  366/2303   train_loss = 3.794\n",
      "Epoch  58 Batch  386/2303   train_loss = 4.331\n",
      "Epoch  58 Batch  406/2303   train_loss = 3.113\n",
      "Epoch  58 Batch  426/2303   train_loss = 3.261\n",
      "Epoch  58 Batch  446/2303   train_loss = 4.047\n",
      "Epoch  58 Batch  466/2303   train_loss = 4.218\n",
      "Epoch  58 Batch  486/2303   train_loss = 3.627\n",
      "Epoch  58 Batch  506/2303   train_loss = 3.507\n",
      "Epoch  58 Batch  526/2303   train_loss = 1.935\n",
      "Epoch  58 Batch  546/2303   train_loss = 4.287\n",
      "Epoch  58 Batch  566/2303   train_loss = 3.374\n",
      "Epoch  58 Batch  586/2303   train_loss = 3.798\n",
      "Epoch  58 Batch  606/2303   train_loss = 4.173\n",
      "Epoch  58 Batch  626/2303   train_loss = 3.447\n",
      "Epoch  58 Batch  646/2303   train_loss = 3.611\n",
      "Epoch  58 Batch  666/2303   train_loss = 3.888\n",
      "Epoch  58 Batch  686/2303   train_loss = 3.958\n",
      "Epoch  58 Batch  706/2303   train_loss = 4.005\n",
      "Epoch  58 Batch  726/2303   train_loss = 3.784\n",
      "Epoch  58 Batch  746/2303   train_loss = 3.487\n",
      "Epoch  58 Batch  766/2303   train_loss = 3.721\n",
      "Epoch  58 Batch  786/2303   train_loss = 3.788\n",
      "Epoch  58 Batch  806/2303   train_loss = 3.801\n",
      "Epoch  58 Batch  826/2303   train_loss = 3.071\n",
      "Epoch  58 Batch  846/2303   train_loss = 4.405\n",
      "Epoch  58 Batch  866/2303   train_loss = 3.655\n",
      "Epoch  58 Batch  886/2303   train_loss = 3.398\n",
      "Epoch  58 Batch  906/2303   train_loss = 3.882\n",
      "Epoch  58 Batch  926/2303   train_loss = 3.357\n",
      "Epoch  58 Batch  946/2303   train_loss = 3.723\n",
      "Epoch  58 Batch  966/2303   train_loss = 3.799\n",
      "Epoch  58 Batch  986/2303   train_loss = 3.198\n",
      "Epoch  58 Batch 1006/2303   train_loss = 2.955\n",
      "Epoch  58 Batch 1026/2303   train_loss = 2.845\n",
      "Epoch  58 Batch 1046/2303   train_loss = 5.099\n",
      "Epoch  58 Batch 1066/2303   train_loss = 3.282\n",
      "Epoch  58 Batch 1086/2303   train_loss = 3.248\n",
      "Epoch  58 Batch 1106/2303   train_loss = 4.093\n",
      "Epoch  58 Batch 1126/2303   train_loss = 4.175\n",
      "Epoch  58 Batch 1146/2303   train_loss = 3.852\n",
      "Epoch  58 Batch 1166/2303   train_loss = 3.838\n",
      "Epoch  58 Batch 1186/2303   train_loss = 4.156\n",
      "Epoch  58 Batch 1206/2303   train_loss = 3.823\n",
      "Epoch  58 Batch 1226/2303   train_loss = 3.431\n",
      "Epoch  58 Batch 1246/2303   train_loss = 4.061\n",
      "Epoch  58 Batch 1266/2303   train_loss = 3.496\n",
      "Epoch  58 Batch 1286/2303   train_loss = 3.341\n",
      "Epoch  58 Batch 1306/2303   train_loss = 3.278\n",
      "Epoch  58 Batch 1326/2303   train_loss = 3.702\n",
      "Epoch  58 Batch 1346/2303   train_loss = 3.356\n",
      "Epoch  58 Batch 1366/2303   train_loss = 3.062\n",
      "Epoch  58 Batch 1386/2303   train_loss = 4.697\n",
      "Epoch  58 Batch 1406/2303   train_loss = 3.664\n",
      "Epoch  58 Batch 1426/2303   train_loss = 4.348\n",
      "Epoch  58 Batch 1446/2303   train_loss = 4.171\n",
      "Epoch  58 Batch 1466/2303   train_loss = 3.700\n",
      "Epoch  58 Batch 1486/2303   train_loss = 3.633\n",
      "Epoch  58 Batch 1506/2303   train_loss = 4.411\n",
      "Epoch  58 Batch 1526/2303   train_loss = 3.685\n",
      "Epoch  58 Batch 1546/2303   train_loss = 3.021\n",
      "Epoch  58 Batch 1566/2303   train_loss = 3.772\n",
      "Epoch  58 Batch 1586/2303   train_loss = 1.985\n",
      "Epoch  58 Batch 1606/2303   train_loss = 3.647\n",
      "Epoch  58 Batch 1626/2303   train_loss = 3.959\n",
      "Epoch  58 Batch 1646/2303   train_loss = 3.983\n",
      "Epoch  58 Batch 1666/2303   train_loss = 3.561\n",
      "Epoch  58 Batch 1686/2303   train_loss = 3.926\n",
      "Epoch  58 Batch 1706/2303   train_loss = 3.273\n",
      "Epoch  58 Batch 1726/2303   train_loss = 3.358\n",
      "Epoch  58 Batch 1746/2303   train_loss = 4.399\n",
      "Epoch  58 Batch 1766/2303   train_loss = 3.790\n",
      "Epoch  58 Batch 1786/2303   train_loss = 3.305\n",
      "Epoch  58 Batch 1806/2303   train_loss = 4.200\n",
      "Epoch  58 Batch 1826/2303   train_loss = 4.185\n",
      "Epoch  58 Batch 1846/2303   train_loss = 3.040\n",
      "Epoch  58 Batch 1866/2303   train_loss = 4.154\n",
      "Epoch  58 Batch 1886/2303   train_loss = 3.638\n",
      "Epoch  58 Batch 1906/2303   train_loss = 4.680\n",
      "Epoch  58 Batch 1926/2303   train_loss = 4.349\n",
      "Epoch  58 Batch 1946/2303   train_loss = 3.946\n",
      "Epoch  58 Batch 1966/2303   train_loss = 4.315\n",
      "Epoch  58 Batch 1986/2303   train_loss = 3.800\n",
      "Epoch  58 Batch 2006/2303   train_loss = 3.764\n",
      "Epoch  58 Batch 2026/2303   train_loss = 4.129\n",
      "Epoch  58 Batch 2046/2303   train_loss = 3.718\n",
      "Epoch  58 Batch 2066/2303   train_loss = 4.764\n",
      "Epoch  58 Batch 2086/2303   train_loss = 3.663\n",
      "Epoch  58 Batch 2106/2303   train_loss = 2.873\n",
      "Epoch  58 Batch 2126/2303   train_loss = 3.349\n",
      "Epoch  58 Batch 2146/2303   train_loss = 3.840\n",
      "Epoch  58 Batch 2166/2303   train_loss = 3.984\n",
      "Epoch  58 Batch 2186/2303   train_loss = 4.303\n",
      "Epoch  58 Batch 2206/2303   train_loss = 3.856\n",
      "Epoch  58 Batch 2226/2303   train_loss = 3.082\n",
      "Epoch  58 Batch 2246/2303   train_loss = 4.596\n",
      "Epoch  58 Batch 2266/2303   train_loss = 3.096\n",
      "Epoch  58 Batch 2286/2303   train_loss = 3.516\n",
      "Epoch  59 Batch    3/2303   train_loss = 3.680\n",
      "Epoch  59 Batch   23/2303   train_loss = 3.581\n",
      "Epoch  59 Batch   43/2303   train_loss = 3.177\n",
      "Epoch  59 Batch   63/2303   train_loss = 3.412\n",
      "Epoch  59 Batch   83/2303   train_loss = 3.075\n",
      "Epoch  59 Batch  103/2303   train_loss = 2.959\n",
      "Epoch  59 Batch  123/2303   train_loss = 4.242\n",
      "Epoch  59 Batch  143/2303   train_loss = 3.058\n",
      "Epoch  59 Batch  163/2303   train_loss = 4.279\n",
      "Epoch  59 Batch  183/2303   train_loss = 3.963\n",
      "Epoch  59 Batch  203/2303   train_loss = 3.897\n",
      "Epoch  59 Batch  223/2303   train_loss = 4.145\n",
      "Epoch  59 Batch  243/2303   train_loss = 3.636\n",
      "Epoch  59 Batch  263/2303   train_loss = 3.023\n",
      "Epoch  59 Batch  283/2303   train_loss = 3.863\n",
      "Epoch  59 Batch  303/2303   train_loss = 3.705\n",
      "Epoch  59 Batch  323/2303   train_loss = 3.743\n",
      "Epoch  59 Batch  343/2303   train_loss = 4.418\n",
      "Epoch  59 Batch  363/2303   train_loss = 3.817\n",
      "Epoch  59 Batch  383/2303   train_loss = 3.780\n",
      "Epoch  59 Batch  403/2303   train_loss = 3.584\n",
      "Epoch  59 Batch  423/2303   train_loss = 3.077\n",
      "Epoch  59 Batch  443/2303   train_loss = 3.116\n",
      "Epoch  59 Batch  463/2303   train_loss = 2.642\n",
      "Epoch  59 Batch  483/2303   train_loss = 4.298\n",
      "Epoch  59 Batch  503/2303   train_loss = 3.610\n",
      "Epoch  59 Batch  523/2303   train_loss = 4.160\n",
      "Epoch  59 Batch  543/2303   train_loss = 3.438\n",
      "Epoch  59 Batch  563/2303   train_loss = 3.781\n",
      "Epoch  59 Batch  583/2303   train_loss = 3.697\n",
      "Epoch  59 Batch  603/2303   train_loss = 3.774\n",
      "Epoch  59 Batch  623/2303   train_loss = 3.855\n",
      "Epoch  59 Batch  643/2303   train_loss = 2.892\n",
      "Epoch  59 Batch  663/2303   train_loss = 3.989\n",
      "Epoch  59 Batch  683/2303   train_loss = 3.221\n",
      "Epoch  59 Batch  703/2303   train_loss = 3.558\n",
      "Epoch  59 Batch  723/2303   train_loss = 3.626\n",
      "Epoch  59 Batch  743/2303   train_loss = 3.256\n",
      "Epoch  59 Batch  763/2303   train_loss = 3.196\n",
      "Epoch  59 Batch  783/2303   train_loss = 4.010\n",
      "Epoch  59 Batch  803/2303   train_loss = 2.837\n",
      "Epoch  59 Batch  823/2303   train_loss = 4.158\n",
      "Epoch  59 Batch  843/2303   train_loss = 3.973\n",
      "Epoch  59 Batch  863/2303   train_loss = 3.610\n",
      "Epoch  59 Batch  883/2303   train_loss = 3.209\n",
      "Epoch  59 Batch  903/2303   train_loss = 3.175\n",
      "Epoch  59 Batch  923/2303   train_loss = 2.919\n",
      "Epoch  59 Batch  943/2303   train_loss = 3.582\n",
      "Epoch  59 Batch  963/2303   train_loss = 2.845\n",
      "Epoch  59 Batch  983/2303   train_loss = 3.785\n",
      "Epoch  59 Batch 1003/2303   train_loss = 3.761\n",
      "Epoch  59 Batch 1023/2303   train_loss = 3.796\n",
      "Epoch  59 Batch 1043/2303   train_loss = 3.321\n",
      "Epoch  59 Batch 1063/2303   train_loss = 3.336\n",
      "Epoch  59 Batch 1083/2303   train_loss = 4.087\n",
      "Epoch  59 Batch 1103/2303   train_loss = 3.928\n",
      "Epoch  59 Batch 1123/2303   train_loss = 4.116\n",
      "Epoch  59 Batch 1143/2303   train_loss = 4.092\n",
      "Epoch  59 Batch 1163/2303   train_loss = 3.128\n",
      "Epoch  59 Batch 1183/2303   train_loss = 3.989\n",
      "Epoch  59 Batch 1203/2303   train_loss = 3.080\n",
      "Epoch  59 Batch 1223/2303   train_loss = 3.436\n",
      "Epoch  59 Batch 1243/2303   train_loss = 4.272\n",
      "Epoch  59 Batch 1263/2303   train_loss = 4.009\n",
      "Epoch  59 Batch 1283/2303   train_loss = 3.464\n",
      "Epoch  59 Batch 1303/2303   train_loss = 3.052\n",
      "Epoch  59 Batch 1323/2303   train_loss = 3.944\n",
      "Epoch  59 Batch 1343/2303   train_loss = 3.146\n",
      "Epoch  59 Batch 1363/2303   train_loss = 3.557\n",
      "Epoch  59 Batch 1383/2303   train_loss = 3.696\n",
      "Epoch  59 Batch 1403/2303   train_loss = 3.677\n",
      "Epoch  59 Batch 1423/2303   train_loss = 3.982\n",
      "Epoch  59 Batch 1443/2303   train_loss = 3.239\n",
      "Epoch  59 Batch 1463/2303   train_loss = 3.746\n",
      "Epoch  59 Batch 1483/2303   train_loss = 3.533\n",
      "Epoch  59 Batch 1503/2303   train_loss = 3.750\n",
      "Epoch  59 Batch 1523/2303   train_loss = 2.981\n",
      "Epoch  59 Batch 1543/2303   train_loss = 4.035\n",
      "Epoch  59 Batch 1563/2303   train_loss = 4.229\n",
      "Epoch  59 Batch 1583/2303   train_loss = 3.612\n",
      "Epoch  59 Batch 1603/2303   train_loss = 3.752\n",
      "Epoch  59 Batch 1623/2303   train_loss = 4.077\n",
      "Epoch  59 Batch 1643/2303   train_loss = 3.985\n",
      "Epoch  59 Batch 1663/2303   train_loss = 2.827\n",
      "Epoch  59 Batch 1683/2303   train_loss = 3.617\n",
      "Epoch  59 Batch 1703/2303   train_loss = 3.510\n",
      "Epoch  59 Batch 1723/2303   train_loss = 4.168\n",
      "Epoch  59 Batch 1743/2303   train_loss = 3.868\n",
      "Epoch  59 Batch 1763/2303   train_loss = 4.649\n",
      "Epoch  59 Batch 1783/2303   train_loss = 3.635\n",
      "Epoch  59 Batch 1803/2303   train_loss = 2.740\n",
      "Epoch  59 Batch 1823/2303   train_loss = 3.730\n",
      "Epoch  59 Batch 1843/2303   train_loss = 3.793\n",
      "Epoch  59 Batch 1863/2303   train_loss = 4.582\n",
      "Epoch  59 Batch 1883/2303   train_loss = 4.214\n",
      "Epoch  59 Batch 1903/2303   train_loss = 3.980\n",
      "Epoch  59 Batch 1923/2303   train_loss = 3.579\n",
      "Epoch  59 Batch 1943/2303   train_loss = 3.845\n",
      "Epoch  59 Batch 1963/2303   train_loss = 3.747\n",
      "Epoch  59 Batch 1983/2303   train_loss = 4.360\n",
      "Epoch  59 Batch 2003/2303   train_loss = 3.676\n",
      "Epoch  59 Batch 2023/2303   train_loss = 3.794\n",
      "Epoch  59 Batch 2043/2303   train_loss = 3.957\n",
      "Epoch  59 Batch 2063/2303   train_loss = 3.448\n",
      "Epoch  59 Batch 2083/2303   train_loss = 3.750\n",
      "Epoch  59 Batch 2103/2303   train_loss = 4.452\n",
      "Epoch  59 Batch 2123/2303   train_loss = 3.395\n",
      "Epoch  59 Batch 2143/2303   train_loss = 2.092\n",
      "Epoch  59 Batch 2163/2303   train_loss = 3.597\n",
      "Epoch  59 Batch 2183/2303   train_loss = 3.947\n",
      "Epoch  59 Batch 2203/2303   train_loss = 3.842\n",
      "Epoch  59 Batch 2223/2303   train_loss = 3.624\n",
      "Epoch  59 Batch 2243/2303   train_loss = 3.716\n",
      "Epoch  59 Batch 2263/2303   train_loss = 3.074\n",
      "Epoch  59 Batch 2283/2303   train_loss = 2.880\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print (\"shape\",probabilities.shape)\n",
    "    #print (probabilities)\n",
    "    idx = np.argmax(probabilities)\n",
    "    return int_to_vocab[idx]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak:(into phone) i don't know.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna do you.\n",
      "homer_simpson:(screams) oh, i don't wanna\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
